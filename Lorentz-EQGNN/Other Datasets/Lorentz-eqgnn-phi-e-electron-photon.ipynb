{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lorentz-Equivariant Quantum Graph Neural Network (Lorentz-EQGNN)","metadata":{"id":"rB_xvk_TXLpz"}},{"cell_type":"code","source":"# For Colab\n!pip install torch_geometric\n# !pip install torch_sparse\n# !pip install torch_scatter","metadata":{"id":"1qx2qWQoXLp2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"99fa5c5a-82da-4114-fa5a-cbaa829d1d60","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T14:09:47.374753Z","iopub.execute_input":"2024-10-25T14:09:47.375329Z","iopub.status.idle":"2024-10-25T14:10:01.829875Z","shell.execute_reply.started":"2024-10-25T14:09:47.375273Z","shell.execute_reply":"2024-10-25T14:10:01.828209Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pennylane qiskit pennylane-qiskit pylatexenc","metadata":{"id":"_CF_l60hp0xJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e919b92a-85ea-493a-b7ad-fa3ec5e21e55","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T14:10:01.832863Z","iopub.execute_input":"2024-10-25T14:10:01.833440Z","iopub.status.idle":"2024-10-25T14:10:27.829283Z","shell.execute_reply.started":"2024-10-25T14:10:01.833379Z","shell.execute_reply":"2024-10-25T14:10:27.827996Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.38.0-py3-none-any.whl.metadata (9.3 kB)\nCollecting qiskit\n  Downloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.38.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.14.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.3)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting autograd (from pennylane)\n  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nCollecting pennylane-lightning>=0.38 (from pennylane)\n  Downloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane) (21.3)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.12)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (0.3.8)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from qiskit) (2.9.0.post0)\nCollecting stevedore>=3.0.0 (from qiskit)\n  Downloading stevedore-5.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting symengine<0.14,>=0.11 (from qiskit)\n  Downloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting qiskit-aer (from pennylane-qiskit)\n  Downloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nCollecting qiskit-ibm-runtime<=0.29 (from pennylane-qiskit)\n  Downloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl.metadata (19 kB)\nCollecting qiskit-ibm-provider (from pennylane-qiskit)\n  Downloading qiskit_ibm_provider-0.11.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\nCollecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.26.18)\nRequirement already satisfied: websocket-client>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.8.0)\nCollecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_platform_services-0.58.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.9.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.8.30)\nCollecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane) (3.1.2)\nRequirement already satisfied: psutil>=5 in /opt/conda/lib/python3.10/site-packages (from qiskit-aer->pennylane-qiskit) (5.9.3)\nRequirement already satisfied: websockets>=10.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (12.0)\nCollecting ibm-cloud-sdk-core<4.0.0,>=3.22.0 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.23.4)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (42.0.8)\nCollecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading pyspnego-0.11.1-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.16.0)\nCollecting urllib3>=1.21.1 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.22.0->ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.8.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.22)\nDownloading PennyLane-0.38.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading PennyLane_qiskit-0.38.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading stevedore-5.3.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_provider-0.11.0-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ibm_platform_services-0.58.0-py3-none-any.whl (340 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.7/340.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\nDownloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspnego-0.11.1-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pylatexenc\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=08055648993ccae33351d8bed429bbfe6d5eb47ca90f5e5389446d7501707b04\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\nSuccessfully built pylatexenc\nInstalling collected packages: pylatexenc, urllib3, symengine, rustworkx, pbr, autoray, autograd, stevedore, qiskit, pyspnego, ibm-cloud-sdk-core, requests-ntlm, qiskit-aer, ibm-platform-services, qiskit-ibm-runtime, qiskit-ibm-provider, pennylane-lightning, pennylane, pennylane-qiskit\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autograd-1.7.0 autoray-0.7.0 ibm-cloud-sdk-core-3.22.0 ibm-platform-services-0.58.0 pbr-6.1.0 pennylane-0.38.0 pennylane-lightning-0.38.0 pennylane-qiskit-0.38.1 pylatexenc-2.10 pyspnego-0.11.1 qiskit-1.2.4 qiskit-aer-0.15.1 qiskit-ibm-provider-0.11.0 qiskit-ibm-runtime-0.29.0 requests-ntlm-1.3.0 rustworkx-0.15.1 stevedore-5.3.0 symengine-0.13.0 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pennylane as qml\nimport qiskit\nprint(qml.__version__)\nprint(qiskit.__version__)\nimport pennylane_qiskit\nprint(pennylane_qiskit.__version__)\nimport pennylane as qml\nfrom pennylane import numpy as np\n# from pennylane_qiskit import AerDevice","metadata":{"id":"wITHoRhbp1XM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"481cf9ec-ff54-4999-cdcf-5f6f2d3509ce","execution":{"iopub.status.busy":"2024-10-25T14:10:27.830935Z","iopub.execute_input":"2024-10-25T14:10:27.831321Z","iopub.status.idle":"2024-10-25T14:10:31.775925Z","shell.execute_reply.started":"2024-10-25T14:10:27.831281Z","shell.execute_reply":"2024-10-25T14:10:31.774751Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0.38.0\n1.2.4\n0.38.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"Okf-o8BkYVJF"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport h5py\nfrom matplotlib.colors import LogNorm\nimport matplotlib.colors as mcolors\nfrom sklearn.model_selection import train_test_split\nimport urllib\n\n# Downloading Dataset\ndef download_file(url, filename):\n    urllib.request.urlretrieve(url, filename)\n\nphoton_url = 'https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\nelectron_url = 'https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n\ndownload_file(photon_url, 'photon.hdf5')\ndownload_file(electron_url, 'electron.hdf5')\n\ndef electron_photon(sample=100000):\n    file_path_electron = \"/kaggle/working/electron.hdf5\"\n    with h5py.File(file_path_electron, \"r\") as file:\n        X_e = np.array(file[\"X\"])\n        y_e = np.array(file[\"y\"])\n\n    file_path_photon = \"/kaggle/working/photon.hdf5\"\n    with h5py.File(file_path_photon, \"r\") as file:\n        X_p = np.array(file[\"X\"])\n        y_p = np.array(file[\"y\"])\n\n    X = np.concatenate((X_e[:sample], X_p[:sample]), axis=0)\n    y = np.concatenate((y_e[:sample], y_p[:sample]), axis=0)\n\n    return X, y","metadata":{"id":"fh1f599G-YAA","execution":{"iopub.status.busy":"2024-10-25T14:12:04.724184Z","iopub.execute_input":"2024-10-25T14:12:04.724620Z","iopub.status.idle":"2024-10-25T14:12:12.871105Z","shell.execute_reply.started":"2024-10-25T14:12:04.724580Z","shell.execute_reply":"2024-10-25T14:12:12.869841Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X, y = electron_photon(sample=100000)\nx_red = X\ny_red = y\nx_red.shape, y_red\njets_photon = x_red[y_red == 1][:10]\njets_photon.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6zUOie7-niR","outputId":"efd3586f-3a88-4dc2-9c7e-371a0a388659","execution":{"iopub.status.busy":"2024-10-25T14:12:12.872940Z","iopub.execute_input":"2024-10-25T14:12:12.873301Z","iopub.status.idle":"2024-10-25T14:12:28.698105Z","shell.execute_reply.started":"2024-10-25T14:12:12.873266Z","shell.execute_reply":"2024-10-25T14:12:28.697046Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(10, 32, 32, 2)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nimport h5py\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import coo_matrix\n\ndef save_electron_photon_tensors(num_data_per_class=500, max_nodes=139, save_dir=\"electron_photon/data\"):\n    \"\"\"\n    Generate and save tensor data files for Electron-Photon dataset in a graph-like format.\n\n    Args:\n        num_data_per_class (int): Number of samples to use per class\n        max_nodes (int): Maximum number of nodes per graph\n        save_dir (str): Directory to save the processed data\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load electron and photon data\n    with h5py.File(\"electron.hdf5\", \"r\") as file:\n        X_e = np.array(file[\"X\"])[:num_data_per_class]\n        y_e = np.ones(num_data_per_class)\n\n    with h5py.File(\"photon.hdf5\", \"r\") as file:\n        X_p = np.array(file[\"X\"])[:num_data_per_class]\n        y_p = np.zeros(num_data_per_class)\n\n    # Combine data\n    X = np.concatenate((X_e, X_p), axis=0)\n    labels = np.concatenate((y_e, y_p), axis=0)\n\n    # Shuffle the data\n    shuffle_idx = np.random.permutation(len(X))\n    X = X[shuffle_idx]\n    labels = labels[shuffle_idx]\n\n    def image_to_nodes(image_data, max_nodes=139, threshold=1e-4):\n        \"\"\"Convert image data to graph nodes with position and feature information.\"\"\"\n        batch_size, height, width, channels = image_data.shape\n        nodes = np.zeros((batch_size, max_nodes, 8))  # 8-dimensional node features\n        p4s = np.zeros((batch_size, max_nodes, 4))    # Position and momentum information\n        atom_masks = np.zeros((batch_size, max_nodes), dtype=bool)\n\n        for b in range(batch_size):\n            # Combine track and ECAL information\n            combined = np.sum(image_data[b, :, :, :2], axis=-1)\n\n            # Find significant points and their values\n            significant_points = np.where(combined > threshold)\n            values = combined[significant_points]\n\n            # Sort points by their values and take exactly max_nodes\n            sorted_indices = np.argsort(-values)\n            n_points = min(len(sorted_indices), max_nodes)\n            selected_indices = sorted_indices[:n_points]\n\n            for idx_pos, idx in enumerate(selected_indices):\n                h, w = significant_points[0][idx], significant_points[1][idx]\n\n                # Enhanced node features (8-dimensional)\n                nodes[b, idx_pos] = [\n                    image_data[b, h, w, 0],  # track energy\n                    image_data[b, h, w, 1],  # ECAL energy\n                    h / height,              # normalized height position\n                    w / width,               # normalized width position\n                    np.sqrt(h**2 + w**2) / np.sqrt(height**2 + width**2),  # normalized radius\n                    np.arctan2(h - height/2, w - width/2) / np.pi,         # angular position\n                    values[idx] / np.max(values),                          # normalized energy\n                    1.0 if image_data[b, h, w, 0] > image_data[b, h, w, 1] else 0.0  # track vs ECAL dominance\n                ]\n\n                # Create p4s (x, y, E, 0) - normalized coordinates\n                x = (w - width/2) / (width/2)   # Center and normalize to [-1, 1]\n                y = (h - height/2) / (height/2) # Center and normalize to [-1, 1]\n                E = values[idx]                 # Energy value\n                p4s[b, idx_pos] = [x, y, E, 0]\n                atom_masks[b, idx_pos] = True\n\n        return p4s, nodes, atom_masks\n\n    # Convert image data to graph format\n    p4s, nodes, atom_mask = image_to_nodes(X, max_nodes=max_nodes)\n\n    # Convert to torch tensors\n    p4s = torch.from_numpy(p4s).float()\n    nodes = torch.from_numpy(nodes).float()\n    labels = torch.from_numpy(labels).float()\n    atom_mask = torch.from_numpy(atom_mask)\n\n    # Create edge mask (fully connected graph between valid nodes)\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask = edge_mask * diag_mask\n\n    # Calculate edges\n    batch_size = len(X)\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx * max_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n    edges = np.stack([rows, cols])\n\n    # Save tensors\n    torch.save(p4s, os.path.join(save_dir, \"p4s.pt\"))\n    torch.save(nodes, os.path.join(save_dir, \"nodes.pt\"))\n    torch.save(labels, os.path.join(save_dir, \"labels.pt\"))\n    torch.save(atom_mask, os.path.join(save_dir, \"atom_mask.pt\"))\n    np.save(os.path.join(save_dir, \"edge_mask.npy\"), edge_mask.numpy())\n    np.save(os.path.join(save_dir, \"edges.npy\"), edges)\n\n    print(f\"Saved tensor files to {save_dir}\")\n    print(f\"Shapes:\")\n    print(f\"p4s: {p4s.shape}\")\n    print(f\"nodes: {nodes.shape}\")\n    print(f\"labels: {labels.shape}\")\n    print(f\"atom_mask: {atom_mask.shape}\")\n    print(f\"edge_mask: {edge_mask.shape}\")\n    print(f\"edges: {edges.shape}\")\n\n    # Print label distribution\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    print(\"\\nLabel distribution:\")\n    for label, count in zip(unique_labels, counts):\n        print(f\"Class {label}: {count} samples\")\n\nif __name__ == '__main__':\n    # Generate data for electron (1) vs photon (0)\n    save_electron_photon_tensors(\n        num_data_per_class=500,  # 500 samples per class = 1000 total\n        max_nodes=200,           # Maximum number of nodes per graph\n        save_dir=\"electron_photon/data\"\n    )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1mxTg93YVJF","outputId":"4cc9185a-e379-4885-f1ba-14a61b9cedd2","execution":{"iopub.status.busy":"2024-10-25T14:12:35.611060Z","iopub.execute_input":"2024-10-25T14:12:35.611532Z","iopub.status.idle":"2024-10-25T14:13:04.053541Z","shell.execute_reply.started":"2024-10-25T14:12:35.611481Z","shell.execute_reply":"2024-10-25T14:13:04.052221Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Saved tensor files to electron_photon/data\nShapes:\np4s: torch.Size([1000, 200, 4])\nnodes: torch.Size([1000, 200, 8])\nlabels: torch.Size([1000])\natom_mask: torch.Size([1000, 200])\nedge_mask: torch.Size([1000, 200, 200])\nedges: (2, 420236)\n\nLabel distribution:\nClass 0.0: 500 samples\nClass 1.0: 500 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    edge_mask = data[-2]\n\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edges]","metadata":{"id":"FsxIXz8SW7C9","execution":{"iopub.status.busy":"2024-10-25T14:13:04.055964Z","iopub.execute_input":"2024-10-25T14:13:04.056404Z","iopub.status.idle":"2024-10-25T14:13:04.065240Z","shell.execute_reply.started":"2024-10-25T14:13:04.056347Z","shell.execute_reply":"2024-10-25T14:13:04.064019Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\nimport torch\n\ndef create_stratified_split(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n    \"\"\"\n    Creates stratified train/val/test splits for binary classification.\n    \"\"\"\n    # Extract labels and convert to integer labels for stratification\n    labels = dataset.tensors[0].numpy()\n    labels_int = labels.astype(int)  # Convert to int for stratification\n\n    # Print initial distribution\n    unique_classes, class_counts = np.unique(labels_int, return_counts=True)\n    print(\"\\nInitial class distribution:\")\n    for class_idx, count in zip(unique_classes, class_counts):\n        print(f\"Class {class_idx}: {count} samples ({count/len(labels)*100:.2f}%)\")\n\n    # First split: separate train from temporary val+test\n    first_sss = StratifiedShuffleSplit(n_splits=1,\n                                      train_size=train_ratio,\n                                      random_state=random_state)\n\n    train_idx, temp_idx = next(first_sss.split(np.zeros(len(labels)), labels_int))\n\n    # Second split: separate val and test from temporary set\n    temp_labels = labels_int[temp_idx]\n    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n\n    second_sss = StratifiedShuffleSplit(n_splits=1,\n                                       train_size=val_ratio_adjusted,\n                                       random_state=random_state)\n\n    val_idx_temp, test_idx_temp = next(second_sss.split(np.zeros(len(temp_labels)), temp_labels))\n\n    # Convert temporary indices to original dataset indices\n    val_idx = temp_idx[val_idx_temp]\n    test_idx = temp_idx[test_idx_temp]\n\n    # Create subset datasets\n    train_dataset = Subset(dataset, train_idx)\n    val_dataset = Subset(dataset, val_idx)\n    test_dataset = Subset(dataset, test_idx)\n\n    # Verify class distribution in each split\n    def get_split_distribution(indices):\n        split_labels = labels_int[indices]\n        unique, counts = np.unique(split_labels, return_counts=True)\n        return dict(zip(unique, counts / len(indices)))\n\n    print(\"\\nClass distribution in splits:\")\n    print(\"Train:\", get_split_distribution(train_idx))\n    print(\"Val:\", get_split_distribution(val_idx))\n    print(\"Test:\", get_split_distribution(test_idx))\n\n    return {\n        \"train\": train_dataset,\n        \"val\": val_dataset,\n        \"test\": test_dataset\n    }\n\n# Let's reload the data and ensure we have binary labels\nlabels = torch.load('/kaggle/working/electron_photon/data/labels.pt')\np4s = torch.load('/kaggle/working/electron_photon/data/p4s.pt')\nnodes = torch.load('/kaggle/working/electron_photon/data/nodes.pt')\natom_mask = torch.load('/kaggle/working/electron_photon/data/atom_mask.pt')\nedge_mask = torch.from_numpy(np.load('/kaggle/working/electron_photon/data/edge_mask.npy'))\n\n# Print label statistics before creating dataset\nprint(\"Label Statistics:\")\nprint(\"Shape:\", labels.shape)\nprint(\"Unique values:\", torch.unique(labels))\nprint(\"Class distribution:\", torch.bincount(labels.long()) / len(labels))\n\n# Create the dataset\ndataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n\n# Create stratified splits\ndatasets = create_stratified_split(dataset_all)\n\n# Create dataloaders\ndataloaders = {\n    split: DataLoader(\n        dataset,\n        batch_size=16,\n        pin_memory=False,\n        collate_fn=collate_fn,\n        drop_last=True if (split == 'train') else False,\n        num_workers=0,\n        shuffle=(split == 'train')\n    )\n    for split, dataset in datasets.items()\n}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHGC6RzeW11q","outputId":"bb0f23a4-7be4-48ee-deeb-d18561b53209","execution":{"iopub.status.busy":"2024-10-25T14:14:09.719641Z","iopub.execute_input":"2024-10-25T14:14:09.720407Z","iopub.status.idle":"2024-10-25T14:14:09.822308Z","shell.execute_reply.started":"2024-10-25T14:14:09.720341Z","shell.execute_reply":"2024-10-25T14:14:09.821267Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Label Statistics:\nShape: torch.Size([1000])\nUnique values: tensor([0., 1.])\nClass distribution: tensor([0.5000, 0.5000])\n\nInitial class distribution:\nClass 0: 500 samples (50.00%)\nClass 1: 500 samples (50.00%)\n\nClass distribution in splits:\nTrain: {0: 0.5, 1: 0.5}\nVal: {0: 0.5, 1: 0.5}\nTest: {0: 0.5, 1: 0.5}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_122/2935664756.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  labels = torch.load('/kaggle/working/electron_photon/data/labels.pt')\n/tmp/ipykernel_122/2935664756.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  p4s = torch.load('/kaggle/working/electron_photon/data/p4s.pt')\n/tmp/ipykernel_122/2935664756.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  nodes = torch.load('/kaggle/working/electron_photon/data/nodes.pt')\n/tmp/ipykernel_122/2935664756.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  atom_mask = torch.load('/kaggle/working/electron_photon/data/atom_mask.pt')\n","output_type":"stream"}]},{"cell_type":"code","source":"print(p4s.shape) # p4s\nprint(nodes.shape) # mass\nprint(atom_mask.shape) # torch.ones\nprint(edge_mask.shape) # adj_matrix","metadata":{"id":"d5obJPELXLp7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"718b468b-1754-44f3-871f-5787dedfe8d1","execution":{"iopub.status.busy":"2024-10-25T14:14:16.322293Z","iopub.execute_input":"2024-10-25T14:14:16.322711Z","iopub.status.idle":"2024-10-25T14:14:16.328756Z","shell.execute_reply.started":"2024-10-25T14:14:16.322674Z","shell.execute_reply":"2024-10-25T14:14:16.327700Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([1000, 200, 4])\ntorch.Size([1000, 200, 8])\ntorch.Size([1000, 200])\ntorch.Size([1000, 200, 200])\n","output_type":"stream"}]},{"cell_type":"code","source":"dataloaders","metadata":{"id":"01O7t2mkXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fc2f256-29c2-455a-cde5-a572abb6f25c","execution":{"iopub.status.busy":"2024-10-25T14:14:18.546734Z","iopub.execute_input":"2024-10-25T14:14:18.547172Z","iopub.status.idle":"2024-10-25T14:14:18.554955Z","shell.execute_reply.started":"2024-10-25T14:14:18.547132Z","shell.execute_reply":"2024-10-25T14:14:18.553678Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'train': <torch.utils.data.dataloader.DataLoader at 0x7a529ffa5b10>,\n 'val': <torch.utils.data.dataloader.DataLoader at 0x7a51f1b42e60>,\n 'test': <torch.utils.data.dataloader.DataLoader at 0x7a51f1b42d40>}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport h5py\nimport os\nfrom scipy.sparse import coo_matrix\n\ndef save_physics_tensors(num_data=1000, save_dir=\"random/data\"):\n    \"\"\"\n    Generate and save tensor data files for electron-photon analysis.\n\n    Args:\n        num_data: Number of data points per class\n        save_dir: Directory to save the tensor files\n    \"\"\"\n    # Create save directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load electron-photon data\n    def load_data(sample=1000):\n        with h5py.File(\"/kaggle/working/electron.hdf5\", \"r\") as file:\n            X_e = np.array(file[\"X\"])[:sample]\n            y_e = np.ones(sample)\n\n        with h5py.File(\"/kaggle/working/photon.hdf5\", \"r\") as file:\n            X_p = np.array(file[\"X\"])[:sample]\n            y_p = np.zeros(sample)\n\n        X = np.concatenate((X_e, X_p), axis=0)\n        y = np.concatenate((y_e, y_p), axis=0)\n        return X, y\n\n    # Load raw data\n    X, labels = load_data(sample=num_data)\n    batch_size = len(X)\n    n_nodes = 500  # Fixed number of nodes as in quark-gluon data\n\n    # Process image data into graph format\n    def image_to_nodes(image_data, max_nodes=100, threshold=1e-4):\n        batch_size = len(image_data)\n        nodes = np.zeros((batch_size, max_nodes, 1))  # Match quark-gluon node dimension\n        p4s = np.zeros((batch_size, max_nodes, 4))\n        atom_masks = np.zeros((batch_size, max_nodes), dtype=bool)\n\n        for b in range(batch_size):\n            # Combine track and ECAL information\n            combined = np.sum(image_data[b, :, :, :2], axis=-1)\n            significant_points = np.where(combined > threshold)\n            values = combined[significant_points]\n\n            # Sort points by their values and take top max_nodes\n            sorted_indices = np.argsort(-values)\n            n_points = min(len(sorted_indices), max_nodes)\n            selected_indices = sorted_indices[:n_points]\n\n            for idx_pos, idx in enumerate(selected_indices):\n                h, w = significant_points[0][idx], significant_points[1][idx]\n\n                # Create node features similar to quark-gluon format\n                track_energy = image_data[b, h, w, 0]\n                ecal_energy = image_data[b, h, w, 1]\n                total_energy = track_energy + ecal_energy\n\n                # Node feature (single dimension as in quark-gluon)\n                nodes[b, idx_pos, 0] = np.log(total_energy + 1)\n\n                # Create pseudo-p4s (pt, eta, phi, m) from position and energy\n                pt = total_energy\n                eta = (h - 16) / 16  # Convert pixel position to pseudorapidity-like value\n                phi = (w - 16) / 16 * np.pi  # Convert pixel position to phi-like value\n                mass = 0  # Assume massless particles\n\n                p4s[b, idx_pos] = [pt, eta, phi, mass]\n                atom_masks[b, idx_pos] = True\n\n        return p4s, nodes, atom_masks\n\n    # Convert image data to graph format\n    p4s, nodes, atom_mask = image_to_nodes(X)\n\n    # Convert to torch tensors\n    p4s = torch.from_numpy(p4s).float()\n    nodes = torch.from_numpy(nodes).float()\n    labels = torch.from_numpy(labels).float()\n    atom_mask = torch.from_numpy(atom_mask)\n\n    # Create edge mask\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask = edge_mask * diag_mask\n\n    # Calculate edges\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx * n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n    edges = np.stack([rows, cols])\n\n    # Save tensors\n    torch.save(p4s, os.path.join(save_dir, \"p4s.pt\"))\n    torch.save(nodes, os.path.join(save_dir, \"nodes.pt\"))\n    torch.save(labels, os.path.join(save_dir, \"labels.pt\"))\n    torch.save(atom_mask, os.path.join(save_dir, \"atom_mask.pt\"))\n    np.save(os.path.join(save_dir, \"edge_mask.npy\"), edge_mask.numpy())\n    np.save(os.path.join(save_dir, \"edges.npy\"), edges)\n\n    print(f\"Saved tensor files to {save_dir}\")\n    print(f\"Shapes:\")\n    print(f\"p4s: {p4s.shape}\")\n    print(f\"nodes: {nodes.shape}\")\n    print(f\"labels: {labels.shape}\")\n    print(f\"atom_mask: {atom_mask.shape}\")\n    print(f\"edge_mask: {edge_mask.shape}\")\n    print(f\"edges: {edges.shape}\")\n\n# Generate and save the tensor files\nsave_physics_tensors(num_data=500)  # This will give 1000 total samples (500 each class)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDF86EgyDwkY","outputId":"79812f8a-dffa-4b1c-9f98-b79f81dfe226","execution":{"iopub.status.busy":"2024-10-25T14:15:03.839230Z","iopub.execute_input":"2024-10-25T14:15:03.839662Z","iopub.status.idle":"2024-10-25T14:15:19.049393Z","shell.execute_reply.started":"2024-10-25T14:15:03.839621Z","shell.execute_reply":"2024-10-25T14:15:19.048124Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Saved tensor files to random/data\nShapes:\np4s: torch.Size([1000, 100, 4])\nnodes: torch.Size([1000, 100, 1])\nlabels: torch.Size([1000])\natom_mask: torch.Size([1000, 100])\nedge_mask: torch.Size([1000, 100, 100])\nedges: (2, 420236)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    # edge_mask *= diag_mask\n\n    edge_mask = data[-2]\n\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edges]\n\n\np4s = torch.load('random/data/p4s.pt')\nnodes = torch.load('random/data/nodes.pt')\nlabels = torch.load('random/data/labels.pt')\natom_mask = torch.load('random/data/atom_mask.pt')\nedge_mask = torch.from_numpy(np.load('random/data/edge_mask.npy'))\nedges = torch.from_numpy(np.load('random/data/edges.npy'))\n\n\n# Create a TensorDataset\ndataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n\n# Define the split ratios\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\n# Calculate the lengths for each split\ntotal_size = len(dataset_all)\ntrain_size = int(total_size * train_ratio)\nval_size = int(total_size * val_ratio)\ntest_size = total_size - train_size - val_size  # Ensure all data is used\n\n# Split the dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset_all, [train_size, val_size, test_size])\n\n# Create a dictionary to hold the datasets\ndatasets = {\n    \"train\": train_dataset,\n    \"val\": val_dataset,\n    \"test\": test_dataset\n}\n\ndataloaders = {split: DataLoader(dataset,\n                                 batch_size=16,\n                                 # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n                                 pin_memory=False,\n                                 # persistent_workers=True,\n                                 collate_fn = collate_fn,\n                                 drop_last=True if (split == 'train') else False,\n                                 num_workers=0)\n                    for split, dataset in datasets.items()}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGYOC1AdECNJ","outputId":"6d081d53-2f02-4700-aa65-56d9ccea42de","execution":{"iopub.status.busy":"2024-10-25T14:15:19.051869Z","iopub.execute_input":"2024-10-25T14:15:19.052224Z","iopub.status.idle":"2024-10-25T14:15:19.080649Z","shell.execute_reply.started":"2024-10-25T14:15:19.052188Z","shell.execute_reply":"2024-10-25T14:15:19.079425Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_122/1146867464.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  p4s = torch.load('random/data/p4s.pt')\n/tmp/ipykernel_122/1146867464.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  nodes = torch.load('random/data/nodes.pt')\n/tmp/ipykernel_122/1146867464.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  labels = torch.load('random/data/labels.pt')\n/tmp/ipykernel_122/1146867464.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  atom_mask = torch.load('random/data/atom_mask.pt')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set desired dimensions\nbatch_size = 1\nn_nodes = 3\ndevice = 'cpu'\ndtype = torch.float32\n\n# Print initial shapes\nprint(\"Initial shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Select subset of data\np4s = p4s[:batch_size, :n_nodes, :]\natom_mask = atom_mask[:batch_size, :n_nodes]\nedge_mask = edge_mask[:batch_size, :n_nodes, :n_nodes]\nnodes = nodes[:batch_size, :n_nodes, :]\n\nprint(\"\\nAfter selection shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Reshape tensors\natom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n# Don't reshape edge_mask yet\nnodes = nodes.view(batch_size * n_nodes, -1).to(device, dtype)\n\nprint(\"\\nAfter reshape shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)  # original shape\nprint(\"nodes:\", nodes.shape)\n\n# Recalculate edges for the subset\nfrom scipy.sparse import coo_matrix\nrows, cols = [], []\nfor batch_idx in range(batch_size):\n    nn = batch_idx * n_nodes\n    # Convert edge_mask to numpy and remove any extra dimensions\n    edge_mask_np = edge_mask[batch_idx].cpu().numpy().squeeze()\n    x = coo_matrix(edge_mask_np)\n    rows.append(nn + x.row)\n    cols.append(nn + x.col)\n\nedges = [torch.LongTensor(np.concatenate(rows)).to(device),\n         torch.LongTensor(np.concatenate(cols)).to(device)]\n\n# Now reshape edge_mask after edges are calculated\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nprint(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"TEs9qVoYXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8412d177-63d2-43e6-90db-34ddd82bac58","execution":{"iopub.status.busy":"2024-10-25T14:15:19.082149Z","iopub.execute_input":"2024-10-25T14:15:19.082530Z","iopub.status.idle":"2024-10-25T14:15:19.107183Z","shell.execute_reply.started":"2024-10-25T14:15:19.082493Z","shell.execute_reply":"2024-10-25T14:15:19.105935Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Initial shapes:\np4s: torch.Size([1000, 100, 4])\natom_mask: torch.Size([1000, 100])\nedge_mask: torch.Size([1000, 100, 100])\nnodes: torch.Size([1000, 100, 1])\n\nAfter selection shapes:\np4s: torch.Size([1, 3, 4])\natom_mask: torch.Size([1, 3])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([1, 3, 1])\n\nAfter reshape shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([3, 1])\n\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"cNq2qPiZKqis","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46321d76-d40b-4410-a0f8-d8fa1d6f9f1e","execution":{"iopub.status.busy":"2024-10-25T14:15:27.815517Z","iopub.execute_input":"2024-10-25T14:15:27.815934Z","iopub.status.idle":"2024-10-25T14:15:27.822708Z","shell.execute_reply.started":"2024-10-25T14:15:27.815895Z","shell.execute_reply":"2024-10-25T14:15:27.821439Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. LorentzNet","metadata":{"id":"_3cyDPxrXLp-"}},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\n\n\"\"\"Some auxiliary functions\"\"\"\n\ndef unsorted_segment_sum(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    return result\n\ndef unsorted_segment_mean(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    count = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    count.index_add_(0, segment_ids, torch.ones_like(data))\n    return result / count.clamp(min=1)\n\ndef normsq4(p):\n    r''' Minkowski square norm\n         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n    '''\n    psq = torch.pow(p, 2)\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef dotsq4(p,q):\n    r''' Minkowski inner product\n         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n    '''\n    psq = p*q\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef normA_fn(A):\n    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n\ndef dotA_fn(A):\n    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n\ndef psi(p):\n    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n    '''\n    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n\n\n\"\"\"Lorentz Group-Equivariant Block\"\"\"\n\nclass LGEB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LGEB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2.\n        print('Input size of phi_e: ', n_input)\n\n        self.include_x = include_x\n        self.phi_e = nn.Sequential(\n            nn.Linear(n_input, n_hidden, bias=False), # n_input * 2 + n_edge_attr\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU())\n\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        # Reduce the dimension of 'out' to 4 using a linear layer\n        out = self.dimension_reducer(out)\n        out = self.phi_e(out)\n        # print(\"m_model output: \", out.shape)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.phi_e(out)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m): # norms\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # print(\"m: \", m.shape)\n        # print(\"trans: \", trans.shape)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        # print(\"trans: \", trans.shape)\n        # print(\"x.size: \", x.size(0))\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            # print(\"X: \", x)\n            x = self.x_model(x, edges, x_diff, m)\n            # print(\"phi_x(X) = \", x, '\\n---\\n')\n\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LorentzNet(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LGEB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LorentzNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n\n        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n        return pred.squeeze(1)","metadata":{"id":"49hLoUYRXLp_","execution":{"iopub.status.busy":"2024-10-25T14:15:44.030282Z","iopub.execute_input":"2024-10-25T14:15:44.030907Z","iopub.status.idle":"2024-10-25T14:15:44.064987Z","shell.execute_reply.started":"2024-10-25T14:15:44.030864Z","shell.execute_reply":"2024-10-25T14:15:44.063785Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\\\n                                    n_node_attr=n_scalar, dropout=dropout,\\\n                                    c_weight=c_weight, last_layer=\\(i==n_layers-1), A=A, include_x=include_x)\n                                    \nWe are using n_hidden = 4 and n_layers = 6\n\nn_input=n_hidden, n_output=n_hidden, n_hidden=n_hidden, n_node_attr=n_scalar=8","metadata":{"id":"yKQ1ZOC2U92Y"}},{"cell_type":"code","source":"# @title\nimport torch\nimport os, json, random, string\nimport torch.distributed as dist\n\ndef makedir(path):\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n\ndef args_init(args):\n    r''' Initialize seed and exp_name.\n    '''\n    if args.seed is None: # use random seed if not specified\n        args.seed = np.random.randint(100)\n    if args.exp_name == '': # use random strings if not specified\n        args.exp_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n    if (args.local_rank == 0): # master\n        print(args)\n        makedir(f\"{args.logdir}/{args.exp_name}\")\n        with open(f\"{args.logdir}/{args.exp_name}/args.json\", 'w') as f:\n            json.dump(args.__dict__, f, indent=4)\n\ndef sum_reduce(num, device):\n    r''' Sum the tensor across the devices.\n    '''\n    if not torch.is_tensor(num):\n        rt = torch.tensor(num).to(device)\n    else:\n        rt = num.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    return rt\n\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        warmup_epoch: target learning rate is reached at warmup_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    Reference:\n        https://github.com/ildoonet/pytorch-gradual-warmup-lr\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, warmup_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.warmup_epoch = warmup_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    @property\n    def _warmup_lr(self):\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch + 1) / self.warmup_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * (self.last_epoch + 1) / self.warmup_epoch + 1.) for base_lr in self.base_lrs]\n\n    def get_lr(self):\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        return self._warmup_lr\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        self.last_epoch = self.last_epoch + 1 if epoch==None else epoch\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if not self.finished:\n                warmup_lr = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                    param_group['lr'] = lr\n                self.finished = True\n                return\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.warmup_epoch)\n            return\n\n        for param_group, lr in zip(self.optimizer.param_groups, self._warmup_lr):\n            param_group['lr'] = lr\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.warmup_epoch)\n                self.last_epoch = self.after_scheduler.last_epoch + self.warmup_epoch + 1\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        result = {key: value for key, value in self.__dict__.items() if key != 'optimizer' or key != \"after_scheduler\"}\n        if self.after_scheduler:\n            result.update({\"after_scheduler\": self.after_scheduler.state_dict()})\n        return result\n\n    def load_state_dict(self, state_dict):\n        after_scheduler_state = state_dict.pop(\"after_scheduler\", None)\n        self.__dict__.update(state_dict)\n        if after_scheduler_state:\n            self.after_scheduler.load_state_dict(after_scheduler_state)\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport numpy as np\n\ndef buildROC(labels, score, targetEff=[0.3,0.5]):\n    r''' ROC curve is a plot of the true positive rate (Sensitivity) in the function of the false positive rate\n    (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a\n    sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC\n    curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups.\n    '''\n    if not isinstance(targetEff, list):\n        targetEff = [targetEff]\n    fpr, tpr, threshold = roc_curve(labels, score)\n    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n    eB, eS = fpr[idx], tpr[idx]\n    return fpr, tpr, threshold, eB, eS","metadata":{"id":"KRBzC37VorM9","execution":{"iopub.status.busy":"2024-10-25T14:15:48.618863Z","iopub.execute_input":"2024-10-25T14:15:48.619469Z","iopub.status.idle":"2024-10-25T14:15:48.651388Z","shell.execute_reply.started":"2024-10-25T14:15:48.619347Z","shell.execute_reply":"2024-10-25T14:15:48.650111Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\n\ndef run(model, epoch, loader, partition, N_EPOCHS=None):\n    if partition == 'train':\n        model.train()\n    else:\n        model.eval()\n\n    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n\n    tik = time.time()\n    loader_length = len(loader)\n\n    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n        if partition == 'train':\n            optimizer.zero_grad()\n\n        batch_size, n_nodes, _ = p4s.size()\n        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n        edges = [a.to(device) for a in edges]\n        label = label.to(device, dtype).long()\n\n        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                         edge_mask=edge_mask, n_nodes=n_nodes)\n\n        predict = pred.max(1).indices\n        correct = torch.sum(predict == label).item()\n        # print(pred.shape,label.shape)\n        loss = loss_fn(pred, label)\n\n        if partition == 'train':\n            loss.backward()\n            optimizer.step()\n        elif partition == 'test':\n            # save labels and probilities for ROC / AUC\n            # print(\"Preds \", pred)\n            score = torch.nn.functional.softmax(pred, dim = -1)\n            # print(\"Score test \", score)\n            # raise\n            res['label'].append(label)\n            res['score'].append(score)\n\n        res['time'] = time.time() - tik\n        res['correct'] += correct\n        res['loss'] += loss.item() * batch_size\n        res['counter'] += batch_size\n        res['loss_arr'].append(loss.item())\n        res['correct_arr'].append(correct)\n\n        # if i != 0 and i % args.log_interval == 0:\n\n    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n    avg_time = res['time']/res['counter'] * batch_size\n    tmp_counter = res['counter']\n    tmp_loss = res['loss'] / tmp_counter\n    tmp_acc = res['correct'] / tmp_counter\n\n    if N_EPOCHS:\n        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n    else:\n        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, running_loss, running_acc, tmp_acc, avg_time))\n\n    torch.cuda.empty_cache()\n    # ---------- reduce -----------\n    if partition == 'test':\n        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n        res['score'] = torch.cat(res['score'])\n        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n    res['counter'] = res['counter']\n    res['loss'] = res['loss'] / res['counter']\n    res['acc'] = res['correct'] / res['counter']\n    return res\n\ndef train(model, res, N_EPOCHS, model_path, log_path):\n    ### training and validation\n    os.makedirs(model_path, exist_ok=True)\n    os.makedirs(log_path, exist_ok=True)\n\n    for epoch in range(N_EPOCHS):\n        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n        # if epoch % args.val_interval == 0:\n\n        # if (args.local_rank == 0):\n        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n        with torch.no_grad():\n            val_res = run(model, epoch, dataloaders['val'], partition='val')\n\n        # if (args.local_rank == 0): # only master process save\n        res['lr'].append(optimizer.param_groups[0]['lr'])\n        res['train_time'].append(train_res['time'])\n        res['val_time'].append(val_res['time'])\n        res['train_loss'].append(train_res['loss'])\n        res['train_acc'].append(train_res['acc'])\n        res['val_loss'].append(val_res['loss'])\n        res['val_acc'].append(val_res['acc'])\n        res['epochs'].append(epoch)\n\n        ## save best model\n        if val_res['acc'] > res['best_val']:\n            print(\"New best validation model, saving...\")\n            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n            res['best_val'] = val_res['acc']\n            res['best_epoch'] = epoch\n\n        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n\n        json_object = json.dumps(res, indent=4)\n        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n            outfile.write(json_object)\n\n        ## adjust learning rate\n        if (epoch < 31):\n            lr_scheduler.step(metrics=val_res['acc'])\n        else:\n            for g in optimizer.param_groups:\n                g['lr'] = g['lr']*0.5\n\n\ndef test(model, res, model_path, log_path):\n    ### test on best model\n    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n    model.load_state_dict(best_model)\n    with torch.no_grad():\n        test_res = run(model, 0, dataloaders['test'], partition='test')\n\n    print(\"Final \", test_res['score'])\n    pred = test_res['score'].cpu()\n\n    np.save(os.path.join(log_path, \"score.npy\"), pred)\n    fpr, tpr, thres, eB, eS  = buildROC(pred[...,0], pred[...,2])\n    auc = roc_auc_score(pred[...,0], pred[...,2])\n\n    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n    res.update(metric)\n    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n    json_object = json.dumps(res, indent=4)\n    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n        outfile.write(json_object)\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 55 # 60\n\n    model_path = \"models/LorentzNet/\"\n    log_path = \"logs/LorentzNet/\"\n    # args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cuda\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cpu\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    ### create parallel model\n    model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"scrolled":true,"id":"8azxxVjtXLqI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67fd3503-ae8d-48d7-f72a-045564390b0e","execution":{"iopub.status.busy":"2024-10-25T14:16:51.344869Z","iopub.execute_input":"2024-10-25T14:16:51.345592Z","iopub.status.idle":"2024-10-25T14:17:16.711105Z","shell.execute_reply.started":"2024-10-25T14:16:51.345551Z","shell.execute_reply":"2024-10-25T14:17:16.709839Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input size of phi_e:  4\nModel Size: 199\n train samples: 800\n val samples: 100\n test samples: 100\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.87it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/55 \t Batch 49/50 \t Loss 0.6964 \t Running Acc 0.497 \t Total Acc 0.497 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6964 \t Train acc: 0.4975\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 264.69it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6962 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.0011\nNew best validation model, saving...\nEpoch 0/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6964 \t Train acc: 0.4975\nVal loss: 0.6952 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/55 \t Batch 49/50 \t Loss 0.6955 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6955 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 245.75it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6962 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.0012\nEpoch 1/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6955 \t Train acc: 0.5025\nVal loss: 0.6949 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 107.58it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/55 \t Batch 49/50 \t Loss 0.6968 \t Running Acc 0.472 \t Total Acc 0.472 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6968 \t Train acc: 0.4725\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 222.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6947 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.0014\nEpoch 2/55 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6968 \t Train acc: 0.4725\nVal loss: 0.6939 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 95.21it/s] \n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/55 \t Batch 49/50 \t Loss 0.6931 \t Running Acc 0.516 \t Total Acc 0.516 \t Avg Batch Time 0.0106\nTime: train: 0.53 \t Train loss 0.6931 \t Train acc: 0.5162\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 177.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6942 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.0017\nEpoch 3/55 finished.\nTrain time: 0.53 \t Val time 0.04\nTrain loss 0.6931 \t Train acc: 0.5162\nVal loss: 0.6935 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.97it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/55 \t Batch 49/50 \t Loss 0.6943 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.6943 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 250.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6937 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0012\nNew best validation model, saving...\nEpoch 4/55 finished.\nTrain time: 0.50 \t Val time 0.03\nTrain loss 0.6943 \t Train acc: 0.5000\nVal loss: 0.6932 \t Val acc: 0.5400\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/55 \t Batch 49/50 \t Loss 0.6950 \t Running Acc 0.482 \t Total Acc 0.482 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6950 \t Train acc: 0.4825\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 232.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6933 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.0013\nEpoch 5/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6950 \t Train acc: 0.4825\nVal loss: 0.6929 \t Val acc: 0.5000\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.03it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/55 \t Batch 49/50 \t Loss 0.6936 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6936 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 254.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6933 \t Running Acc 1.607 \t Total Acc 0.450 \t Avg Batch Time 0.0012\nEpoch 6/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6936 \t Train acc: 0.4988\nVal loss: 0.6929 \t Val acc: 0.4500\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 120.81it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/55 \t Batch 49/50 \t Loss 0.6929 \t Running Acc 0.505 \t Total Acc 0.505 \t Avg Batch Time 0.0083\nTime: train: 0.42 \t Train loss 0.6929 \t Train acc: 0.5050\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 239.48it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6933 \t Running Acc 1.607 \t Total Acc 0.450 \t Avg Batch Time 0.0013\nEpoch 7/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6929 \t Train acc: 0.5050\nVal loss: 0.6929 \t Val acc: 0.4500\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.63it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/55 \t Batch 49/50 \t Loss 0.6931 \t Running Acc 0.507 \t Total Acc 0.507 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6931 \t Train acc: 0.5075\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 246.27it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6931 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.0012\nEpoch 8/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6931 \t Train acc: 0.5075\nVal loss: 0.6928 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.50it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/55 \t Batch 49/50 \t Loss 0.6938 \t Running Acc 0.496 \t Total Acc 0.496 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6938 \t Train acc: 0.4963\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 241.61it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6932 \t Running Acc 1.679 \t Total Acc 0.470 \t Avg Batch Time 0.0012\nEpoch 9/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6938 \t Train acc: 0.4963\nVal loss: 0.6929 \t Val acc: 0.4700\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 117.84it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/55 \t Batch 49/50 \t Loss 0.6939 \t Running Acc 0.492 \t Total Acc 0.492 \t Avg Batch Time 0.0085\nTime: train: 0.43 \t Train loss 0.6939 \t Train acc: 0.4925\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 241.31it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.0013\nEpoch 10/55 finished.\nTrain time: 0.43 \t Val time 0.03\nTrain loss 0.6939 \t Train acc: 0.4925\nVal loss: 0.6927 \t Val acc: 0.5000\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 120.21it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/55 \t Batch 49/50 \t Loss 0.6924 \t Running Acc 0.531 \t Total Acc 0.531 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6924 \t Train acc: 0.5312\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 232.53it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 1.679 \t Total Acc 0.470 \t Avg Batch Time 0.0013\nEpoch 11/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6924 \t Train acc: 0.5312\nVal loss: 0.6926 \t Val acc: 0.4700\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.78it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/55 \t Batch 49/50 \t Loss 0.6928 \t Running Acc 0.527 \t Total Acc 0.527 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6928 \t Train acc: 0.5275\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 239.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 1.679 \t Total Acc 0.470 \t Avg Batch Time 0.0013\nEpoch 12/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6928 \t Train acc: 0.5275\nVal loss: 0.6925 \t Val acc: 0.4700\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 118.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/55 \t Batch 49/50 \t Loss 0.6933 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 0.0085\nTime: train: 0.43 \t Train loss 0.6933 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 227.20it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6927 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.0013\nEpoch 13/55 finished.\nTrain time: 0.43 \t Val time 0.03\nTrain loss 0.6933 \t Train acc: 0.4988\nVal loss: 0.6924 \t Val acc: 0.5000\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 116.96it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/55 \t Batch 49/50 \t Loss 0.6930 \t Running Acc 0.515 \t Total Acc 0.515 \t Avg Batch Time 0.0086\nTime: train: 0.43 \t Train loss 0.6930 \t Train acc: 0.5150\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 248.49it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6927 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.0012\nEpoch 14/55 finished.\nTrain time: 0.43 \t Val time 0.03\nTrain loss 0.6930 \t Train acc: 0.5150\nVal loss: 0.6924 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 118.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/55 \t Batch 49/50 \t Loss 0.6907 \t Running Acc 0.547 \t Total Acc 0.547 \t Avg Batch Time 0.0085\nTime: train: 0.42 \t Train loss 0.6907 \t Train acc: 0.5475\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 241.12it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6927 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.0013\nEpoch 15/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6907 \t Train acc: 0.5475\nVal loss: 0.6924 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 119.22it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/55 \t Batch 49/50 \t Loss 0.6910 \t Running Acc 0.544 \t Total Acc 0.544 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6910 \t Train acc: 0.5437\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 249.21it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nNew best validation model, saving...\nEpoch 16/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6910 \t Train acc: 0.5437\nVal loss: 0.6924 \t Val acc: 0.5600\nBest val acc: 0.5600 at epoch 16.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.25it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/55 \t Batch 49/50 \t Loss 0.6923 \t Running Acc 0.519 \t Total Acc 0.519 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6923 \t Train acc: 0.5188\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 240.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 2.036 \t Total Acc 0.570 \t Avg Batch Time 0.0013\nNew best validation model, saving...\nEpoch 17/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6923 \t Train acc: 0.5188\nVal loss: 0.6924 \t Val acc: 0.5700\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 124.06it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/55 \t Batch 49/50 \t Loss 0.6918 \t Running Acc 0.531 \t Total Acc 0.531 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6918 \t Train acc: 0.5312\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 240.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6926 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 18/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6918 \t Train acc: 0.5312\nVal loss: 0.6924 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 120.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/55 \t Batch 49/50 \t Loss 0.6912 \t Running Acc 0.522 \t Total Acc 0.522 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6912 \t Train acc: 0.5225\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 246.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6923 \t Running Acc 1.893 \t Total Acc 0.530 \t Avg Batch Time 0.0012\nEpoch 19/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6912 \t Train acc: 0.5225\nVal loss: 0.6922 \t Val acc: 0.5300\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 112.05it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/55 \t Batch 49/50 \t Loss 0.6919 \t Running Acc 0.521 \t Total Acc 0.521 \t Avg Batch Time 0.0090\nTime: train: 0.45 \t Train loss 0.6919 \t Train acc: 0.5212\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 248.07it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6926 \t Running Acc 1.964 \t Total Acc 0.550 \t Avg Batch Time 0.0012\nEpoch 20/55 finished.\nTrain time: 0.45 \t Val time 0.03\nTrain loss 0.6919 \t Train acc: 0.5212\nVal loss: 0.6922 \t Val acc: 0.5500\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.31it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/55 \t Batch 49/50 \t Loss 0.6903 \t Running Acc 0.549 \t Total Acc 0.549 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6903 \t Train acc: 0.5487\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 260.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6923 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 21/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6903 \t Train acc: 0.5487\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.63it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/55 \t Batch 49/50 \t Loss 0.6897 \t Running Acc 0.549 \t Total Acc 0.549 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6897 \t Train acc: 0.5487\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 262.68it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6924 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0012\nEpoch 22/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6897 \t Train acc: 0.5487\nVal loss: 0.6922 \t Val acc: 0.5400\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/55 \t Batch 49/50 \t Loss 0.6887 \t Running Acc 0.566 \t Total Acc 0.566 \t Avg Batch Time 0.0083\nTime: train: 0.42 \t Train loss 0.6887 \t Train acc: 0.5663\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 231.48it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6925 \t Running Acc 1.857 \t Total Acc 0.520 \t Avg Batch Time 0.0013\nEpoch 23/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6887 \t Train acc: 0.5663\nVal loss: 0.6922 \t Val acc: 0.5200\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.43it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/55 \t Batch 49/50 \t Loss 0.6896 \t Running Acc 0.542 \t Total Acc 0.542 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6896 \t Train acc: 0.5425\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 256.87it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6924 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0012\nEpoch 24/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6896 \t Train acc: 0.5425\nVal loss: 0.6921 \t Val acc: 0.5400\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.96it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 26/55 \t Batch 49/50 \t Loss 0.6894 \t Running Acc 0.549 \t Total Acc 0.549 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6894 \t Train acc: 0.5487\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 264.42it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6926 \t Running Acc 1.893 \t Total Acc 0.530 \t Avg Batch Time 0.0011\nEpoch 25/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6894 \t Train acc: 0.5487\nVal loss: 0.6921 \t Val acc: 0.5300\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 27/55 \t Batch 49/50 \t Loss 0.6878 \t Running Acc 0.568 \t Total Acc 0.568 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6878 \t Train acc: 0.5675\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 252.68it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6926 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0012\nEpoch 26/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6878 \t Train acc: 0.5675\nVal loss: 0.6921 \t Val acc: 0.5400\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.06it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 28/55 \t Batch 49/50 \t Loss 0.6884 \t Running Acc 0.560 \t Total Acc 0.560 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6884 \t Train acc: 0.5600\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 236.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0013\nEpoch 27/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6884 \t Train acc: 0.5600\nVal loss: 0.6921 \t Val acc: 0.5400\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 119.85it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 29/55 \t Batch 49/50 \t Loss 0.6893 \t Running Acc 0.542 \t Total Acc 0.542 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6893 \t Train acc: 0.5425\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 248.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 1.964 \t Total Acc 0.550 \t Avg Batch Time 0.0012\nEpoch 28/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6893 \t Train acc: 0.5425\nVal loss: 0.6921 \t Val acc: 0.5500\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 115.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 30/55 \t Batch 49/50 \t Loss 0.6888 \t Running Acc 0.556 \t Total Acc 0.556 \t Avg Batch Time 0.0087\nTime: train: 0.44 \t Train loss 0.6888 \t Train acc: 0.5563\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 263.39it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 29/55 finished.\nTrain time: 0.44 \t Val time 0.03\nTrain loss 0.6888 \t Train acc: 0.5563\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.71it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 31/55 \t Batch 49/50 \t Loss 0.6895 \t Running Acc 0.552 \t Total Acc 0.552 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6895 \t Train acc: 0.5525\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 261.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 30/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6895 \t Train acc: 0.5525\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.75it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 32/55 \t Batch 49/50 \t Loss 0.6890 \t Running Acc 0.535 \t Total Acc 0.535 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6890 \t Train acc: 0.5350\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 271.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0011\nEpoch 31/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6890 \t Train acc: 0.5350\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.88it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 33/55 \t Batch 49/50 \t Loss 0.6892 \t Running Acc 0.550 \t Total Acc 0.550 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6892 \t Train acc: 0.5500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 254.91it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 32/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6892 \t Train acc: 0.5500\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 124.01it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 34/55 \t Batch 49/50 \t Loss 0.6895 \t Running Acc 0.541 \t Total Acc 0.541 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6895 \t Train acc: 0.5413\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 258.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 33/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6895 \t Train acc: 0.5413\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.78it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 35/55 \t Batch 49/50 \t Loss 0.6886 \t Running Acc 0.551 \t Total Acc 0.551 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6886 \t Train acc: 0.5513\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 217.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0014\nEpoch 34/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6886 \t Train acc: 0.5513\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.00it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 36/55 \t Batch 49/50 \t Loss 0.6902 \t Running Acc 0.526 \t Total Acc 0.526 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6902 \t Train acc: 0.5262\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 252.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 35/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6902 \t Train acc: 0.5262\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.33it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 37/55 \t Batch 49/50 \t Loss 0.6896 \t Running Acc 0.544 \t Total Acc 0.544 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6896 \t Train acc: 0.5437\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 241.22it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 36/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6896 \t Train acc: 0.5437\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 120.61it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 38/55 \t Batch 49/50 \t Loss 0.6890 \t Running Acc 0.542 \t Total Acc 0.542 \t Avg Batch Time 0.0083\nTime: train: 0.42 \t Train loss 0.6890 \t Train acc: 0.5425\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 236.91it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 37/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6890 \t Train acc: 0.5425\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.90it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 39/55 \t Batch 49/50 \t Loss 0.6887 \t Running Acc 0.560 \t Total Acc 0.560 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6887 \t Train acc: 0.5600\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 219.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0014\nEpoch 38/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6887 \t Train acc: 0.5600\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.72it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 40/55 \t Batch 49/50 \t Loss 0.6866 \t Running Acc 0.583 \t Total Acc 0.583 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6866 \t Train acc: 0.5825\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 245.71it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 39/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6866 \t Train acc: 0.5825\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 125.07it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 41/55 \t Batch 49/50 \t Loss 0.6878 \t Running Acc 0.565 \t Total Acc 0.565 \t Avg Batch Time 0.0081\nTime: train: 0.40 \t Train loss 0.6878 \t Train acc: 0.5650\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 260.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 40/55 finished.\nTrain time: 0.40 \t Val time 0.03\nTrain loss 0.6878 \t Train acc: 0.5650\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 42/55 \t Batch 49/50 \t Loss 0.6874 \t Running Acc 0.579 \t Total Acc 0.579 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6874 \t Train acc: 0.5787\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 217.95it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0014\nEpoch 41/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6874 \t Train acc: 0.5787\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 112.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 43/55 \t Batch 49/50 \t Loss 0.6893 \t Running Acc 0.547 \t Total Acc 0.547 \t Avg Batch Time 0.0090\nTime: train: 0.45 \t Train loss 0.6893 \t Train acc: 0.5475\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 249.92it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 42/55 finished.\nTrain time: 0.45 \t Val time 0.03\nTrain loss 0.6893 \t Train acc: 0.5475\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 117.81it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 44/55 \t Batch 49/50 \t Loss 0.6895 \t Running Acc 0.541 \t Total Acc 0.541 \t Avg Batch Time 0.0085\nTime: train: 0.43 \t Train loss 0.6895 \t Train acc: 0.5413\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 254.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 43/55 finished.\nTrain time: 0.43 \t Val time 0.03\nTrain loss 0.6895 \t Train acc: 0.5413\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 118.32it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 45/55 \t Batch 49/50 \t Loss 0.6894 \t Running Acc 0.547 \t Total Acc 0.547 \t Avg Batch Time 0.0085\nTime: train: 0.42 \t Train loss 0.6894 \t Train acc: 0.5475\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 247.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 44/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6894 \t Train acc: 0.5475\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 121.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 46/55 \t Batch 49/50 \t Loss 0.6888 \t Running Acc 0.557 \t Total Acc 0.557 \t Avg Batch Time 0.0083\nTime: train: 0.41 \t Train loss 0.6888 \t Train acc: 0.5575\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 248.56it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 45/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6888 \t Train acc: 0.5575\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 124.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 47/55 \t Batch 49/50 \t Loss 0.6885 \t Running Acc 0.562 \t Total Acc 0.562 \t Avg Batch Time 0.0081\nTime: train: 0.40 \t Train loss 0.6885 \t Train acc: 0.5625\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 265.73it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0011\nEpoch 46/55 finished.\nTrain time: 0.40 \t Val time 0.03\nTrain loss 0.6885 \t Train acc: 0.5625\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 48/55 \t Batch 49/50 \t Loss 0.6891 \t Running Acc 0.545 \t Total Acc 0.545 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6891 \t Train acc: 0.5450\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 237.65it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 47/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6891 \t Train acc: 0.5450\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 49/55 \t Batch 49/50 \t Loss 0.6907 \t Running Acc 0.534 \t Total Acc 0.534 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6907 \t Train acc: 0.5337\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 239.43it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 48/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6907 \t Train acc: 0.5337\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 119.19it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 50/55 \t Batch 49/50 \t Loss 0.6889 \t Running Acc 0.545 \t Total Acc 0.545 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6889 \t Train acc: 0.5450\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 245.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 49/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6889 \t Train acc: 0.5450\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 51/55 \t Batch 49/50 \t Loss 0.6881 \t Running Acc 0.562 \t Total Acc 0.562 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6881 \t Train acc: 0.5625\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 259.93it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 50/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6881 \t Train acc: 0.5625\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.66it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 52/55 \t Batch 49/50 \t Loss 0.6898 \t Running Acc 0.531 \t Total Acc 0.531 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6898 \t Train acc: 0.5312\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 222.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 51/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6898 \t Train acc: 0.5312\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 122.10it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 53/55 \t Batch 49/50 \t Loss 0.6894 \t Running Acc 0.541 \t Total Acc 0.541 \t Avg Batch Time 0.0082\nTime: train: 0.41 \t Train loss 0.6894 \t Train acc: 0.5413\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 259.63it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 52/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6894 \t Train acc: 0.5413\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 120.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 54/55 \t Batch 49/50 \t Loss 0.6891 \t Running Acc 0.554 \t Total Acc 0.554 \t Avg Batch Time 0.0084\nTime: train: 0.42 \t Train loss 0.6891 \t Train acc: 0.5537\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 246.71it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0012\nEpoch 53/55 finished.\nTrain time: 0.42 \t Val time 0.03\nTrain loss 0.6891 \t Train acc: 0.5537\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 123.61it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 55/55 \t Batch 49/50 \t Loss 0.6891 \t Running Acc 0.556 \t Total Acc 0.556 \t Avg Batch Time 0.0081\nTime: train: 0.41 \t Train loss 0.6891 \t Train acc: 0.5563\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 230.45it/s]\n/tmp/ipykernel_122/861624552.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0013\nEpoch 54/55 finished.\nTrain time: 0.41 \t Val time 0.03\nTrain loss 0.6891 \t Train acc: 0.5563\nVal loss: 0.6921 \t Val acc: 0.5600\nBest val acc: 0.5700 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 217.48it/s]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.6906 \t Running Acc 2.179 \t Total Acc 0.610 \t Avg Batch Time 0.0014\nFinal  tensor([[1.0000, 0.4909, 0.5091],\n        [0.0000, 0.5016, 0.4984],\n        [1.0000, 0.4915, 0.5085],\n        [0.0000, 0.5023, 0.4977],\n        [1.0000, 0.5035, 0.4965],\n        [0.0000, 0.4963, 0.5037],\n        [0.0000, 0.5004, 0.4996],\n        [0.0000, 0.5018, 0.4982],\n        [1.0000, 0.5002, 0.4998],\n        [0.0000, 0.5012, 0.4988],\n        [1.0000, 0.5012, 0.4988],\n        [1.0000, 0.4853, 0.5147],\n        [0.0000, 0.4988, 0.5012],\n        [0.0000, 0.4949, 0.5051],\n        [1.0000, 0.4980, 0.5020],\n        [1.0000, 0.4976, 0.5024],\n        [1.0000, 0.4938, 0.5062],\n        [1.0000, 0.4930, 0.5070],\n        [1.0000, 0.4989, 0.5011],\n        [1.0000, 0.5043, 0.4957],\n        [1.0000, 0.4870, 0.5130],\n        [0.0000, 0.5018, 0.4982],\n        [1.0000, 0.4949, 0.5051],\n        [0.0000, 0.5002, 0.4998],\n        [1.0000, 0.5017, 0.4983],\n        [1.0000, 0.4933, 0.5067],\n        [1.0000, 0.5019, 0.4981],\n        [1.0000, 0.5001, 0.4999],\n        [1.0000, 0.4902, 0.5098],\n        [1.0000, 0.5008, 0.4992],\n        [0.0000, 0.5008, 0.4992],\n        [0.0000, 0.4961, 0.5039],\n        [0.0000, 0.5022, 0.4978],\n        [1.0000, 0.5008, 0.4992],\n        [0.0000, 0.4985, 0.5015],\n        [0.0000, 0.5033, 0.4967],\n        [1.0000, 0.5010, 0.4990],\n        [1.0000, 0.4978, 0.5022],\n        [0.0000, 0.4996, 0.5004],\n        [0.0000, 0.5004, 0.4996],\n        [1.0000, 0.4967, 0.5033],\n        [0.0000, 0.5033, 0.4967],\n        [0.0000, 0.5001, 0.4999],\n        [0.0000, 0.4999, 0.5001],\n        [0.0000, 0.4992, 0.5008],\n        [1.0000, 0.4901, 0.5099],\n        [1.0000, 0.4933, 0.5067],\n        [0.0000, 0.5033, 0.4967],\n        [1.0000, 0.5018, 0.4982],\n        [0.0000, 0.5016, 0.4984],\n        [1.0000, 0.4981, 0.5019],\n        [1.0000, 0.5001, 0.4999],\n        [1.0000, 0.4862, 0.5138],\n        [0.0000, 0.4999, 0.5001],\n        [1.0000, 0.4997, 0.5003],\n        [0.0000, 0.5020, 0.4980],\n        [0.0000, 0.5006, 0.4994],\n        [0.0000, 0.5013, 0.4987],\n        [0.0000, 0.4977, 0.5023],\n        [1.0000, 0.4962, 0.5038],\n        [1.0000, 0.5006, 0.4994],\n        [1.0000, 0.5019, 0.4981],\n        [0.0000, 0.5011, 0.4989],\n        [0.0000, 0.5053, 0.4947],\n        [1.0000, 0.5008, 0.4992],\n        [0.0000, 0.4884, 0.5116],\n        [1.0000, 0.5024, 0.4976],\n        [1.0000, 0.5022, 0.4978],\n        [0.0000, 0.4997, 0.5003],\n        [1.0000, 0.4965, 0.5035],\n        [0.0000, 0.5010, 0.4990],\n        [0.0000, 0.5005, 0.4995],\n        [1.0000, 0.4982, 0.5018],\n        [1.0000, 0.4972, 0.5028],\n        [1.0000, 0.5008, 0.4992],\n        [1.0000, 0.4964, 0.5036],\n        [0.0000, 0.5000, 0.5000],\n        [0.0000, 0.4938, 0.5062],\n        [1.0000, 0.4982, 0.5018],\n        [0.0000, 0.4996, 0.5004],\n        [0.0000, 0.5024, 0.4976],\n        [1.0000, 0.4934, 0.5066],\n        [0.0000, 0.5011, 0.4989],\n        [0.0000, 0.4962, 0.5038],\n        [1.0000, 0.4974, 0.5026],\n        [0.0000, 0.5023, 0.4977],\n        [0.0000, 0.4983, 0.5017],\n        [1.0000, 0.5022, 0.4978],\n        [0.0000, 0.5014, 0.4986],\n        [0.0000, 0.5024, 0.4976],\n        [1.0000, 0.4997, 0.5003],\n        [0.0000, 0.5026, 0.4974],\n        [0.0000, 0.4902, 0.5098],\n        [1.0000, 0.4849, 0.5151],\n        [1.0000, 0.4993, 0.5007],\n        [0.0000, 0.5011, 0.4989],\n        [0.0000, 0.4991, 0.5009],\n        [0.0000, 0.4934, 0.5066],\n        [0.0000, 0.5021, 0.4979],\n        [1.0000, 0.4917, 0.5083]])\nTest: Loss 0.6904 \t Acc 0.6100 \t AUC: 0.6764 \t 1/eB 0.3: 12.5000 \t 1/eB 0.5: 5.5556\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Proposed\n","metadata":{"id":"gZwcNRHBXLqI"}},{"cell_type":"code","source":"import torch\nimport pennylane as qml\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch_geometric.utils import to_dense_adj\n\nn_qubits = 4\n\ndev = qml.device('default.qubit', wires=n_qubits)\n# dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n\n\ndef H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\ndef RY_RX_layer(weights):\n    \"\"\"Applies a layer of parametrized RY and RX rotations.\"\"\"\n    for i, w in enumerate(weights):\n        qml.RY(w, wires=i)\n        qml.RX(w, wires=i)\n\ndef full_entangling_layer(n_qubits):\n    \"\"\"Applies CNOT gates between all pairs of qubits.\"\"\"\n    for i in range(n_qubits):\n        for j in range(i+1, n_qubits):\n            qml.CNOT(wires=[i, j])\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(nqubits - 1):\n        qml.CRZ(np.pi / 2, wires=[i, i + 1])\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.SWAP(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.SWAP(wires=[i, i + 1])\n\n\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    # RY_layer(q_input_features)\n    qml.AngleEmbedding(features=q_input_features, wires=range(n_qubits), rotation='Z')\n\n    # Sequence of trainable variational layers\n    # for k in range(q_depth):\n    #     entangling_layer(n_qubits)\n    #     RY_RX_layer(q_weights[k])\n    #     # RY_layer(q_weights[k])\n    for k in range(q_depth):\n        if k % 2 == 0:\n            entangling_layer(n_qubits)\n            RY_layer(q_weights[k])\n        else:\n            full_entangling_layer(n_qubits)\n            RY_RX_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\n\nclass DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n        print('n_qubits: ', n_qubits)\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.q_depth = q_depth\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n\n    def forward(self, input_features):\n        \"\"\"\n        Optimized forward pass to reduce runtime.\n        \"\"\"\n\n        # Quantum Embedding (U(X))\n        q_in = torch.tanh(input_features) * np.pi / 2.0\n\n        # Preallocate output tensor\n        batch_size = q_in.shape[0]\n        q_out = torch.zeros(batch_size, self.n_qubits, device=q_in.device)\n\n        # Vectorized execution\n        for i, elem in enumerate(q_in):\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float()\n            q_out[i] = q_out_elem\n\n        return q_out","metadata":{"id":"3T8yKHYCZbPk","execution":{"iopub.status.busy":"2024-10-25T14:17:52.315624Z","iopub.execute_input":"2024-10-25T14:17:52.316879Z","iopub.status.idle":"2024-10-25T14:17:55.154117Z","shell.execute_reply.started":"2024-10-25T14:17:52.316826Z","shell.execute_reply":"2024-10-25T14:17:55.152972Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pennylane as qml\n\n\"\"\"\n    Lorentz-Equivariant Quantum Block (LEQB).\n\n        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n          that we develop further), once the metric tensor J is found via the equation:\n\n                          L.J + J.(L^T) = 0,\n\n          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group.\n          In the cells below, we can see how the model preserves symmetries (starting with the default Lorentz group),\n          and when we change J to some other metric (Euclidean, for example), Lorentz boosts **break** equivariance, while other\n          transformations preserve it (rotations, for the example shown in the cells below)\n\"\"\"\nclass LEQB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LEQB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        self.dimension_reducer2 = nn.Linear(9, 4) # New linear layer for dimension reduction for phi_h\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2. n_input = n_hidden = 4\n        print('Input size of phi_e: ', n_input)\n        self.include_x = include_x\n\n        \"\"\"\n            phi_e: input size: n_qubits -> output size: n_qubits\n            n_hidden has to be equal to n_input,\n            but this is just considering that this is a simple working example.\n        \"\"\"\n        self.phi_e = DressedQuantumNet(n_input)\n#         self.phi_e = nn.Sequential(\n#             nn.Linear(n_input, n_hidden, bias=False),  # n_input * 2 + n_edge_attr\n#             nn.BatchNorm1d(n_hidden),\n#             nn.ReLU(),\n#             nn.Linear(n_hidden, n_hidden),\n#             nn.ReLU())\n\n        n_hidden = n_input # n_input * 2 + n_edge_attr\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n#         self.phi_h = DressedQuantumNet(n_hidden)\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n#         self.phi_x = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             layer)\n\n#         self.phi_m = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             nn.Linear(n_hidden, 1),\n#             nn.Sigmoid())\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        # self.phi_e = nn.Sequential(\n        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n        #     nn.BatchNorm1d(n_hidden),\n        #     nn.ReLU(),\n        #     nn.Linear(n_hidden, n_hidden),\n        #     nn.ReLU())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        out = self.dimension_reducer(out) # extra\n        # print(\"Before embedding to |psi> : \", out)\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.dimension_reducer(out) # extra\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        #agg = self.dimension_reducer2(agg) # extra for phi_h\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m):\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            x = self.x_model(x, edges, x_diff, m)\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LieEQGNN(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LEQB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LieEQGNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LEQBs = nn.ModuleList([LEQB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LEQBs[i](h, x, edges, node_attr=scalars)\n\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n        return pred.squeeze(1)","metadata":{"id":"9sBE05_9XLqJ","execution":{"iopub.status.busy":"2024-10-25T14:17:55.156386Z","iopub.execute_input":"2024-10-25T14:17:55.157645Z","iopub.status.idle":"2024-10-25T14:17:55.188706Z","shell.execute_reply.started":"2024-10-25T14:17:55.157588Z","shell.execute_reply":"2024-10-25T14:17:55.187440Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 55 # 60\n\n    model_path = \"models/LieEQGNN/\"\n    log_path = \"logs/LieEQGNN/\"\n    # utils_lorentz.args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cpu\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cuda\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    model = LieEQGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"id":"sCLi_VJSZiEE","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee36bda5-9a84-4e4b-bcd0-6cc1efc2ae76","execution":{"iopub.status.busy":"2024-10-25T14:18:40.694218Z","iopub.execute_input":"2024-10-25T14:18:40.695385Z","iopub.status.idle":"2024-10-25T18:41:35.663569Z","shell.execute_reply.started":"2024-10-25T14:18:40.695293Z","shell.execute_reply":"2024-10-25T18:41:35.662199Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input size of phi_e:  4\nn_qubits:  4\nModel Size: 199\n train samples: 800\n val samples: 100\n test samples: 100\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"50it [04:29,  5.40s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/55 \t Batch 49/50 \t Loss 0.7467 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.3992\nTime: train: 269.96 \t Train loss 0.7467 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7514 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8314\nNew best validation model, saving...\nEpoch 0/55 finished.\nTrain time: 269.96 \t Val time 20.79\nTrain loss 0.7467 \t Train acc: 0.5025\nVal loss: 0.7524 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.24s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/55 \t Batch 49/50 \t Loss 0.7334 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.2447\nTime: train: 262.23 \t Train loss 0.7334 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7398 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8246\nEpoch 1/55 finished.\nTrain time: 262.23 \t Val time 20.61\nTrain loss 0.7334 \t Train acc: 0.5025\nVal loss: 0.7408 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.25s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/55 \t Batch 49/50 \t Loss 0.7219 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.2490\nTime: train: 262.45 \t Train loss 0.7219 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7249 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8251\nEpoch 2/55 finished.\nTrain time: 262.45 \t Val time 20.63\nTrain loss 0.7219 \t Train acc: 0.5025\nVal loss: 0.7258 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/55 \t Batch 49/50 \t Loss 0.7088 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.3303\nTime: train: 266.52 \t Train loss 0.7088 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7112 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8331\nEpoch 3/55 finished.\nTrain time: 266.52 \t Val time 20.83\nTrain loss 0.7088 \t Train acc: 0.5025\nVal loss: 0.7121 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/55 \t Batch 49/50 \t Loss 0.7025 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.3118\nTime: train: 265.59 \t Train loss 0.7025 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.02s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7015 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8456\nEpoch 4/55 finished.\nTrain time: 265.59 \t Val time 21.14\nTrain loss 0.7025 \t Train acc: 0.5025\nVal loss: 0.7024 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:28,  5.37s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/55 \t Batch 49/50 \t Loss 0.6934 \t Running Acc 0.509 \t Total Acc 0.509 \t Avg Batch Time 5.3707\nTime: train: 268.53 \t Train loss 0.6934 \t Train acc: 0.5088\n","output_type":"stream"},{"name":"stderr","text":"7it [00:22,  3.19s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6967 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8933\nEpoch 5/55 finished.\nTrain time: 268.53 \t Val time 22.33\nTrain loss 0.6934 \t Train acc: 0.5088\nVal loss: 0.6975 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/55 \t Batch 49/50 \t Loss 0.6941 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 5.3062\nTime: train: 265.31 \t Train loss 0.6941 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  3.00s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6953 \t Running Acc 1.679 \t Total Acc 0.470 \t Avg Batch Time 0.8388\nEpoch 6/55 finished.\nTrain time: 265.31 \t Val time 20.97\nTrain loss 0.6941 \t Train acc: 0.5000\nVal loss: 0.6961 \t Val acc: 0.4700\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.30s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/55 \t Batch 49/50 \t Loss 0.6900 \t Running Acc 0.545 \t Total Acc 0.545 \t Avg Batch Time 5.3010\nTime: train: 265.05 \t Train loss 0.6900 \t Train acc: 0.5450\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6949 \t Running Acc 1.679 \t Total Acc 0.470 \t Avg Batch Time 0.8294\nEpoch 7/55 finished.\nTrain time: 265.05 \t Val time 20.74\nTrain loss 0.6900 \t Train acc: 0.5450\nVal loss: 0.6958 \t Val acc: 0.4700\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/55 \t Batch 49/50 \t Loss 0.6922 \t Running Acc 0.550 \t Total Acc 0.550 \t Avg Batch Time 5.2797\nTime: train: 263.99 \t Train loss 0.6922 \t Train acc: 0.5500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.99s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6934 \t Running Acc 1.714 \t Total Acc 0.480 \t Avg Batch Time 0.8385\nEpoch 8/55 finished.\nTrain time: 263.99 \t Val time 20.96\nTrain loss 0.6922 \t Train acc: 0.5500\nVal loss: 0.6943 \t Val acc: 0.4800\nBest val acc: 0.4800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.27s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/55 \t Batch 49/50 \t Loss 0.6928 \t Running Acc 0.506 \t Total Acc 0.506 \t Avg Batch Time 5.2705\nTime: train: 263.53 \t Train loss 0.6928 \t Train acc: 0.5062\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6929 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.8321\nNew best validation model, saving...\nEpoch 9/55 finished.\nTrain time: 263.53 \t Val time 20.80\nTrain loss 0.6928 \t Train acc: 0.5062\nVal loss: 0.6938 \t Val acc: 0.5000\nBest val acc: 0.5000 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/55 \t Batch 49/50 \t Loss 0.6946 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.2779\nTime: train: 263.90 \t Train loss 0.6946 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6928 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.8317\nEpoch 10/55 finished.\nTrain time: 263.90 \t Val time 20.79\nTrain loss 0.6946 \t Train acc: 0.4988\nVal loss: 0.6938 \t Val acc: 0.5000\nBest val acc: 0.5000 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/55 \t Batch 49/50 \t Loss 0.6915 \t Running Acc 0.512 \t Total Acc 0.512 \t Avg Batch Time 5.3092\nTime: train: 265.46 \t Train loss 0.6915 \t Train acc: 0.5125\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.01s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6924 \t Running Acc 1.786 \t Total Acc 0.500 \t Avg Batch Time 0.8431\nEpoch 11/55 finished.\nTrain time: 265.46 \t Val time 21.08\nTrain loss 0.6915 \t Train acc: 0.5125\nVal loss: 0.6933 \t Val acc: 0.5000\nBest val acc: 0.5000 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.29s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/55 \t Batch 49/50 \t Loss 0.6919 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.2872\nTime: train: 264.36 \t Train loss 0.6919 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6923 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8291\nNew best validation model, saving...\nEpoch 12/55 finished.\nTrain time: 264.36 \t Val time 20.73\nTrain loss 0.6919 \t Train acc: 0.4988\nVal loss: 0.6932 \t Val acc: 0.5100\nBest val acc: 0.5100 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:31,  5.43s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/55 \t Batch 49/50 \t Loss 0.6950 \t Running Acc 0.484 \t Total Acc 0.484 \t Avg Batch Time 5.4253\nTime: train: 271.27 \t Train loss 0.6950 \t Train acc: 0.4838\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.00s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6922 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8409\nEpoch 13/55 finished.\nTrain time: 271.27 \t Val time 21.02\nTrain loss 0.6950 \t Train acc: 0.4838\nVal loss: 0.6932 \t Val acc: 0.5100\nBest val acc: 0.5100 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.29s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/55 \t Batch 49/50 \t Loss 0.6927 \t Running Acc 0.519 \t Total Acc 0.519 \t Avg Batch Time 5.2944\nTime: train: 264.72 \t Train loss 0.6927 \t Train acc: 0.5188\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.05s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6922 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8553\nEpoch 14/55 finished.\nTrain time: 264.72 \t Val time 21.38\nTrain loss 0.6927 \t Train acc: 0.5188\nVal loss: 0.6932 \t Val acc: 0.5100\nBest val acc: 0.5100 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:30,  5.41s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/55 \t Batch 49/50 \t Loss 0.6964 \t Running Acc 0.486 \t Total Acc 0.486 \t Avg Batch Time 5.4051\nTime: train: 270.26 \t Train loss 0.6964 \t Train acc: 0.4863\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6922 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8301\nEpoch 15/55 finished.\nTrain time: 270.26 \t Val time 20.75\nTrain loss 0.6964 \t Train acc: 0.4863\nVal loss: 0.6932 \t Val acc: 0.5100\nBest val acc: 0.5100 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.29s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/55 \t Batch 49/50 \t Loss 0.6951 \t Running Acc 0.487 \t Total Acc 0.487 \t Avg Batch Time 5.2942\nTime: train: 264.71 \t Train loss 0.6951 \t Train acc: 0.4875\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6923 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8301\nEpoch 16/55 finished.\nTrain time: 264.71 \t Val time 20.75\nTrain loss 0.6951 \t Train acc: 0.4875\nVal loss: 0.6933 \t Val acc: 0.5100\nBest val acc: 0.5100 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.29s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/55 \t Batch 49/50 \t Loss 0.6962 \t Running Acc 0.471 \t Total Acc 0.471 \t Avg Batch Time 5.2896\nTime: train: 264.48 \t Train loss 0.6962 \t Train acc: 0.4713\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.01s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6924 \t Running Acc 1.893 \t Total Acc 0.530 \t Avg Batch Time 0.8437\nNew best validation model, saving...\nEpoch 17/55 finished.\nTrain time: 264.48 \t Val time 21.09\nTrain loss 0.6962 \t Train acc: 0.4713\nVal loss: 0.6934 \t Val acc: 0.5300\nBest val acc: 0.5300 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/55 \t Batch 49/50 \t Loss 0.6913 \t Running Acc 0.502 \t Total Acc 0.502 \t Avg Batch Time 5.3071\nTime: train: 265.36 \t Train loss 0.6913 \t Train acc: 0.5025\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6921 \t Running Acc 1.857 \t Total Acc 0.520 \t Avg Batch Time 0.8246\nEpoch 18/55 finished.\nTrain time: 265.36 \t Val time 20.61\nTrain loss 0.6913 \t Train acc: 0.5025\nVal loss: 0.6930 \t Val acc: 0.5200\nBest val acc: 0.5300 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/55 \t Batch 49/50 \t Loss 0.6900 \t Running Acc 0.517 \t Total Acc 0.517 \t Avg Batch Time 5.3279\nTime: train: 266.40 \t Train loss 0.6900 \t Train acc: 0.5175\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  3.00s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6917 \t Running Acc 1.893 \t Total Acc 0.530 \t Avg Batch Time 0.8399\nEpoch 19/55 finished.\nTrain time: 266.40 \t Val time 21.00\nTrain loss 0.6900 \t Train acc: 0.5175\nVal loss: 0.6926 \t Val acc: 0.5300\nBest val acc: 0.5300 at epoch 17.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.25s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/55 \t Batch 49/50 \t Loss 0.6900 \t Running Acc 0.526 \t Total Acc 0.526 \t Avg Batch Time 5.2468\nTime: train: 262.34 \t Train loss 0.6900 \t Train acc: 0.5262\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6916 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.8310\nNew best validation model, saving...\nEpoch 20/55 finished.\nTrain time: 262.34 \t Val time 20.77\nTrain loss 0.6900 \t Train acc: 0.5262\nVal loss: 0.6926 \t Val acc: 0.5400\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.32s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/55 \t Batch 49/50 \t Loss 0.6916 \t Running Acc 0.505 \t Total Acc 0.505 \t Avg Batch Time 5.3206\nTime: train: 266.03 \t Train loss 0.6916 \t Train acc: 0.5050\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6916 \t Running Acc 1.893 \t Total Acc 0.530 \t Avg Batch Time 0.8330\nEpoch 21/55 finished.\nTrain time: 266.03 \t Val time 20.83\nTrain loss 0.6916 \t Train acc: 0.5050\nVal loss: 0.6927 \t Val acc: 0.5300\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/55 \t Batch 49/50 \t Loss 0.6937 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.2827\nTime: train: 264.13 \t Train loss 0.6937 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.04s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6918 \t Running Acc 1.857 \t Total Acc 0.520 \t Avg Batch Time 0.8499\nEpoch 22/55 finished.\nTrain time: 264.13 \t Val time 21.25\nTrain loss 0.6937 \t Train acc: 0.4988\nVal loss: 0.6928 \t Val acc: 0.5200\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:30,  5.42s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/55 \t Batch 49/50 \t Loss 0.6891 \t Running Acc 0.512 \t Total Acc 0.512 \t Avg Batch Time 5.4194\nTime: train: 270.97 \t Train loss 0.6891 \t Train acc: 0.5125\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.01s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6917 \t Running Acc 1.857 \t Total Acc 0.520 \t Avg Batch Time 0.8422\nEpoch 23/55 finished.\nTrain time: 270.97 \t Val time 21.06\nTrain loss 0.6891 \t Train acc: 0.5125\nVal loss: 0.6927 \t Val acc: 0.5200\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:27,  5.36s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/55 \t Batch 49/50 \t Loss 0.6940 \t Running Acc 0.494 \t Total Acc 0.494 \t Avg Batch Time 5.3588\nTime: train: 267.94 \t Train loss 0.6940 \t Train acc: 0.4938\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.09s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6917 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8658\nEpoch 24/55 finished.\nTrain time: 267.94 \t Val time 21.65\nTrain loss 0.6940 \t Train acc: 0.4938\nVal loss: 0.6928 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:31,  5.44s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 26/55 \t Batch 49/50 \t Loss 0.6899 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.4358\nTime: train: 271.79 \t Train loss 0.6899 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.99s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6917 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8360\nEpoch 25/55 finished.\nTrain time: 271.79 \t Val time 20.90\nTrain loss 0.6899 \t Train acc: 0.4988\nVal loss: 0.6928 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 27/55 \t Batch 49/50 \t Loss 0.6892 \t Running Acc 0.522 \t Total Acc 0.522 \t Avg Batch Time 5.3132\nTime: train: 265.66 \t Train loss 0.6892 \t Train acc: 0.5225\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6916 \t Running Acc 1.857 \t Total Acc 0.520 \t Avg Batch Time 0.8325\nEpoch 26/55 finished.\nTrain time: 265.66 \t Val time 20.81\nTrain loss 0.6892 \t Train acc: 0.5225\nVal loss: 0.6927 \t Val acc: 0.5200\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.34s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 28/55 \t Batch 49/50 \t Loss 0.6896 \t Running Acc 0.517 \t Total Acc 0.517 \t Avg Batch Time 5.3383\nTime: train: 266.91 \t Train loss 0.6896 \t Train acc: 0.5175\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8273\nEpoch 27/55 finished.\nTrain time: 266.91 \t Val time 20.68\nTrain loss 0.6896 \t Train acc: 0.5175\nVal loss: 0.6926 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.27s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 29/55 \t Batch 49/50 \t Loss 0.6918 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 5.2707\nTime: train: 263.53 \t Train loss 0.6918 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6916 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8254\nEpoch 28/55 finished.\nTrain time: 263.53 \t Val time 20.64\nTrain loss 0.6918 \t Train acc: 0.5000\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 30/55 \t Batch 49/50 \t Loss 0.6919 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 5.3295\nTime: train: 266.47 \t Train loss 0.6919 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8296\nEpoch 29/55 finished.\nTrain time: 266.47 \t Val time 20.74\nTrain loss 0.6919 \t Train acc: 0.5000\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.26s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 31/55 \t Batch 49/50 \t Loss 0.6876 \t Running Acc 0.525 \t Total Acc 0.525 \t Avg Batch Time 5.2579\nTime: train: 262.89 \t Train loss 0.6876 \t Train acc: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8313\nEpoch 30/55 finished.\nTrain time: 262.89 \t Val time 20.78\nTrain loss 0.6876 \t Train acc: 0.5250\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 32/55 \t Batch 49/50 \t Loss 0.6950 \t Running Acc 0.480 \t Total Acc 0.480 \t Avg Batch Time 5.3266\nTime: train: 266.33 \t Train loss 0.6950 \t Train acc: 0.4800\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.09s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8659\nEpoch 31/55 finished.\nTrain time: 266.33 \t Val time 21.65\nTrain loss 0.6950 \t Train acc: 0.4800\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 33/55 \t Batch 49/50 \t Loss 0.6934 \t Running Acc 0.484 \t Total Acc 0.484 \t Avg Batch Time 5.2757\nTime: train: 263.79 \t Train loss 0.6934 \t Train acc: 0.4838\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8304\nEpoch 32/55 finished.\nTrain time: 263.79 \t Val time 20.76\nTrain loss 0.6934 \t Train acc: 0.4838\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:26,  5.32s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 34/55 \t Batch 49/50 \t Loss 0.6865 \t Running Acc 0.554 \t Total Acc 0.554 \t Avg Batch Time 5.3224\nTime: train: 266.12 \t Train loss 0.6865 \t Train acc: 0.5537\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8225\nEpoch 33/55 finished.\nTrain time: 266.12 \t Val time 20.56\nTrain loss 0.6865 \t Train acc: 0.5537\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.25s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 35/55 \t Batch 49/50 \t Loss 0.6957 \t Running Acc 0.468 \t Total Acc 0.468 \t Avg Batch Time 5.2464\nTime: train: 262.32 \t Train loss 0.6957 \t Train acc: 0.4675\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8238\nEpoch 34/55 finished.\nTrain time: 262.32 \t Val time 20.60\nTrain loss 0.6957 \t Train acc: 0.4675\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.32s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 36/55 \t Batch 49/50 \t Loss 0.6918 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.3186\nTime: train: 265.93 \t Train loss 0.6918 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8318\nEpoch 35/55 finished.\nTrain time: 265.93 \t Val time 20.79\nTrain loss 0.6918 \t Train acc: 0.4988\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:24,  5.29s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 37/55 \t Batch 49/50 \t Loss 0.6950 \t Running Acc 0.489 \t Total Acc 0.489 \t Avg Batch Time 5.2938\nTime: train: 264.69 \t Train loss 0.6950 \t Train acc: 0.4888\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8306\nEpoch 36/55 finished.\nTrain time: 264.69 \t Val time 20.77\nTrain loss 0.6950 \t Train acc: 0.4888\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:27,  5.35s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 38/55 \t Batch 49/50 \t Loss 0.6939 \t Running Acc 0.489 \t Total Acc 0.489 \t Avg Batch Time 5.3502\nTime: train: 267.51 \t Train loss 0.6939 \t Train acc: 0.4888\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8335\nEpoch 37/55 finished.\nTrain time: 267.51 \t Val time 20.84\nTrain loss 0.6939 \t Train acc: 0.4888\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.30s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 39/55 \t Batch 49/50 \t Loss 0.6899 \t Running Acc 0.510 \t Total Acc 0.510 \t Avg Batch Time 5.3016\nTime: train: 265.08 \t Train loss 0.6899 \t Train acc: 0.5100\n","output_type":"stream"},{"name":"stderr","text":"7it [00:22,  3.22s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.9006\nEpoch 38/55 finished.\nTrain time: 265.08 \t Val time 22.52\nTrain loss 0.6899 \t Train acc: 0.5100\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:32,  5.44s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 40/55 \t Batch 49/50 \t Loss 0.6916 \t Running Acc 0.495 \t Total Acc 0.495 \t Avg Batch Time 5.4441\nTime: train: 272.20 \t Train loss 0.6916 \t Train acc: 0.4950\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.05s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8549\nEpoch 39/55 finished.\nTrain time: 272.20 \t Val time 21.37\nTrain loss 0.6916 \t Train acc: 0.4950\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 41/55 \t Batch 49/50 \t Loss 0.6851 \t Running Acc 0.531 \t Total Acc 0.531 \t Avg Batch Time 5.3136\nTime: train: 265.68 \t Train loss 0.6851 \t Train acc: 0.5312\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8273\nEpoch 40/55 finished.\nTrain time: 265.68 \t Val time 20.68\nTrain loss 0.6851 \t Train acc: 0.5312\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:28,  5.38s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 42/55 \t Batch 49/50 \t Loss 0.6913 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 5.3777\nTime: train: 268.88 \t Train loss 0.6913 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8263\nEpoch 41/55 finished.\nTrain time: 268.88 \t Val time 20.66\nTrain loss 0.6913 \t Train acc: 0.5000\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.26s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 43/55 \t Batch 49/50 \t Loss 0.6950 \t Running Acc 0.477 \t Total Acc 0.477 \t Avg Batch Time 5.2646\nTime: train: 263.23 \t Train loss 0.6950 \t Train acc: 0.4775\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8320\nEpoch 42/55 finished.\nTrain time: 263.23 \t Val time 20.80\nTrain loss 0.6950 \t Train acc: 0.4775\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.25s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 44/55 \t Batch 49/50 \t Loss 0.6926 \t Running Acc 0.486 \t Total Acc 0.486 \t Avg Batch Time 5.2550\nTime: train: 262.75 \t Train loss 0.6926 \t Train acc: 0.4863\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.93s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8197\nEpoch 43/55 finished.\nTrain time: 262.75 \t Val time 20.49\nTrain loss 0.6926 \t Train acc: 0.4863\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:19,  5.20s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 45/55 \t Batch 49/50 \t Loss 0.6915 \t Running Acc 0.499 \t Total Acc 0.499 \t Avg Batch Time 5.1992\nTime: train: 259.96 \t Train loss 0.6915 \t Train acc: 0.4988\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.93s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8207\nEpoch 44/55 finished.\nTrain time: 259.96 \t Val time 20.52\nTrain loss 0.6915 \t Train acc: 0.4988\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:25,  5.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 46/55 \t Batch 49/50 \t Loss 0.6958 \t Running Acc 0.482 \t Total Acc 0.482 \t Avg Batch Time 5.3064\nTime: train: 265.32 \t Train loss 0.6958 \t Train acc: 0.4825\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8287\nEpoch 45/55 finished.\nTrain time: 265.32 \t Val time 20.72\nTrain loss 0.6958 \t Train acc: 0.4825\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:21,  5.23s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 47/55 \t Batch 49/50 \t Loss 0.6924 \t Running Acc 0.497 \t Total Acc 0.497 \t Avg Batch Time 5.2261\nTime: train: 261.30 \t Train loss 0.6924 \t Train acc: 0.4975\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8240\nEpoch 46/55 finished.\nTrain time: 261.30 \t Val time 20.60\nTrain loss 0.6924 \t Train acc: 0.4975\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.26s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 48/55 \t Batch 49/50 \t Loss 0.6955 \t Running Acc 0.474 \t Total Acc 0.474 \t Avg Batch Time 5.2597\nTime: train: 262.98 \t Train loss 0.6955 \t Train acc: 0.4738\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8235\nEpoch 47/55 finished.\nTrain time: 262.98 \t Val time 20.59\nTrain loss 0.6955 \t Train acc: 0.4738\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:21,  5.22s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 49/55 \t Batch 49/50 \t Loss 0.6903 \t Running Acc 0.515 \t Total Acc 0.515 \t Avg Batch Time 5.2244\nTime: train: 261.22 \t Train loss 0.6903 \t Train acc: 0.5150\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8285\nEpoch 48/55 finished.\nTrain time: 261.22 \t Val time 20.71\nTrain loss 0.6903 \t Train acc: 0.5150\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:21,  5.23s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 50/55 \t Batch 49/50 \t Loss 0.6938 \t Running Acc 0.491 \t Total Acc 0.491 \t Avg Batch Time 5.2290\nTime: train: 261.45 \t Train loss 0.6938 \t Train acc: 0.4913\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8357\nEpoch 49/55 finished.\nTrain time: 261.45 \t Val time 20.89\nTrain loss 0.6938 \t Train acc: 0.4913\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:21,  5.24s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 51/55 \t Batch 49/50 \t Loss 0.6904 \t Running Acc 0.504 \t Total Acc 0.504 \t Avg Batch Time 5.2369\nTime: train: 261.84 \t Train loss 0.6904 \t Train acc: 0.5038\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.93s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8210\nEpoch 50/55 finished.\nTrain time: 261.84 \t Val time 20.53\nTrain loss 0.6904 \t Train acc: 0.5038\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 52/55 \t Batch 49/50 \t Loss 0.6912 \t Running Acc 0.505 \t Total Acc 0.505 \t Avg Batch Time 5.2757\nTime: train: 263.78 \t Train loss 0.6912 \t Train acc: 0.5050\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8295\nEpoch 51/55 finished.\nTrain time: 263.78 \t Val time 20.74\nTrain loss 0.6912 \t Train acc: 0.5050\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:32,  5.46s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 53/55 \t Batch 49/50 \t Loss 0.6873 \t Running Acc 0.535 \t Total Acc 0.535 \t Avg Batch Time 5.4580\nTime: train: 272.90 \t Train loss 0.6873 \t Train acc: 0.5350\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8341\nEpoch 52/55 finished.\nTrain time: 272.90 \t Val time 20.85\nTrain loss 0.6873 \t Train acc: 0.5350\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:22,  5.26s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 54/55 \t Batch 49/50 \t Loss 0.6937 \t Running Acc 0.482 \t Total Acc 0.482 \t Avg Batch Time 5.2576\nTime: train: 262.88 \t Train loss 0.6937 \t Train acc: 0.4825\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8222\nEpoch 53/55 finished.\nTrain time: 262.88 \t Val time 20.55\nTrain loss 0.6937 \t Train acc: 0.4825\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"50it [04:23,  5.27s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 55/55 \t Batch 49/50 \t Loss 0.6928 \t Running Acc 0.497 \t Total Acc 0.497 \t Avg Batch Time 5.2675\nTime: train: 263.38 \t Train loss 0.6928 \t Train acc: 0.4975\n","output_type":"stream"},{"name":"stderr","text":"7it [00:20,  2.97s/it]\n/tmp/ipykernel_122/861624552.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6915 \t Running Acc 1.821 \t Total Acc 0.510 \t Avg Batch Time 0.8318\nEpoch 54/55 finished.\nTrain time: 263.38 \t Val time 20.80\nTrain loss 0.6928 \t Train acc: 0.4975\nVal loss: 0.6927 \t Val acc: 0.5100\nBest val acc: 0.5400 at epoch 20.\n","output_type":"stream"},{"name":"stderr","text":"7it [00:21,  3.09s/it]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.6858 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 0.8643\nFinal  tensor([[1.0000, 0.4797, 0.5203],\n        [0.0000, 0.5055, 0.4945],\n        [1.0000, 0.4939, 0.5061],\n        [0.0000, 0.5174, 0.4826],\n        [1.0000, 0.5021, 0.4979],\n        [0.0000, 0.5050, 0.4950],\n        [0.0000, 0.5051, 0.4949],\n        [0.0000, 0.5174, 0.4826],\n        [1.0000, 0.4786, 0.5214],\n        [0.0000, 0.5150, 0.4850],\n        [1.0000, 0.5077, 0.4923],\n        [1.0000, 0.4847, 0.5153],\n        [0.0000, 0.5096, 0.4904],\n        [0.0000, 0.4797, 0.5203],\n        [1.0000, 0.5027, 0.4973],\n        [1.0000, 0.5009, 0.4991],\n        [1.0000, 0.4937, 0.5063],\n        [1.0000, 0.4845, 0.5155],\n        [1.0000, 0.4988, 0.5012],\n        [1.0000, 0.4956, 0.5044],\n        [1.0000, 0.4872, 0.5128],\n        [0.0000, 0.5051, 0.4949],\n        [1.0000, 0.4867, 0.5133],\n        [0.0000, 0.5151, 0.4849],\n        [1.0000, 0.5123, 0.4877],\n        [1.0000, 0.4831, 0.5169],\n        [1.0000, 0.5170, 0.4830],\n        [1.0000, 0.4995, 0.5005],\n        [1.0000, 0.4779, 0.5221],\n        [1.0000, 0.5097, 0.4903],\n        [0.0000, 0.5112, 0.4888],\n        [0.0000, 0.4911, 0.5089],\n        [0.0000, 0.5179, 0.4821],\n        [1.0000, 0.5075, 0.4925],\n        [0.0000, 0.5065, 0.4935],\n        [0.0000, 0.5111, 0.4889],\n        [1.0000, 0.5022, 0.4978],\n        [1.0000, 0.4972, 0.5028],\n        [0.0000, 0.5096, 0.4904],\n        [0.0000, 0.5035, 0.4965],\n        [1.0000, 0.4851, 0.5149],\n        [0.0000, 0.5092, 0.4908],\n        [0.0000, 0.5052, 0.4948],\n        [0.0000, 0.5135, 0.4865],\n        [0.0000, 0.5020, 0.4980],\n        [1.0000, 0.4744, 0.5256],\n        [1.0000, 0.4866, 0.5134],\n        [0.0000, 0.5096, 0.4904],\n        [1.0000, 0.5134, 0.4866],\n        [0.0000, 0.5139, 0.4861],\n        [1.0000, 0.4898, 0.5102],\n        [1.0000, 0.5079, 0.4921],\n        [1.0000, 0.4590, 0.5410],\n        [0.0000, 0.5033, 0.4967],\n        [1.0000, 0.5017, 0.4983],\n        [0.0000, 0.5133, 0.4867],\n        [0.0000, 0.5063, 0.4937],\n        [0.0000, 0.5056, 0.4944],\n        [0.0000, 0.5037, 0.4963],\n        [1.0000, 0.4918, 0.5082],\n        [1.0000, 0.5105, 0.4895],\n        [1.0000, 0.5170, 0.4830],\n        [0.0000, 0.4970, 0.5030],\n        [0.0000, 0.4828, 0.5172],\n        [1.0000, 0.5049, 0.4951],\n        [0.0000, 0.4826, 0.5174],\n        [1.0000, 0.5180, 0.4820],\n        [1.0000, 0.5123, 0.4877],\n        [0.0000, 0.5091, 0.4909],\n        [1.0000, 0.4930, 0.5070],\n        [0.0000, 0.5076, 0.4924],\n        [0.0000, 0.5075, 0.4925],\n        [1.0000, 0.4983, 0.5017],\n        [1.0000, 0.5072, 0.4928],\n        [1.0000, 0.5016, 0.4984],\n        [1.0000, 0.5102, 0.4898],\n        [0.0000, 0.5042, 0.4958],\n        [0.0000, 0.5025, 0.4975],\n        [1.0000, 0.4992, 0.5008],\n        [0.0000, 0.4969, 0.5031],\n        [0.0000, 0.5193, 0.4807],\n        [1.0000, 0.4877, 0.5123],\n        [0.0000, 0.5080, 0.4920],\n        [0.0000, 0.4829, 0.5171],\n        [1.0000, 0.4928, 0.5072],\n        [0.0000, 0.5086, 0.4914],\n        [0.0000, 0.4971, 0.5029],\n        [1.0000, 0.5106, 0.4894],\n        [0.0000, 0.4997, 0.5003],\n        [0.0000, 0.5123, 0.4877],\n        [1.0000, 0.4991, 0.5009],\n        [0.0000, 0.5133, 0.4867],\n        [0.0000, 0.4836, 0.5164],\n        [1.0000, 0.4631, 0.5369],\n        [1.0000, 0.5076, 0.4924],\n        [0.0000, 0.5070, 0.4930],\n        [0.0000, 0.5015, 0.4985],\n        [0.0000, 0.4999, 0.5001],\n        [0.0000, 0.5017, 0.4983],\n        [1.0000, 0.4934, 0.5066]])\nTest: Loss 0.6855 \t Acc 0.6700 \t AUC: 0.6820 \t 1/eB 0.3: 10.0000 \t 1/eB 0.5: 8.3333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"LGWyv5SIYVJQ"},"execution_count":null,"outputs":[]}]}