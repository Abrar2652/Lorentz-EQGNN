{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lorentz-Equivariant Quantum Graph Neural Network (Lorentz-EQGNN)","metadata":{"id":"rB_xvk_TXLpz"}},{"cell_type":"code","source":"# For Colab\n!pip install torch_geometric\n# !pip install torch_sparse\n# !pip install torch_scatter","metadata":{"id":"1qx2qWQoXLp2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15e852ad-282a-4d36-e400-53399d904f45","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T22:09:35.199572Z","iopub.execute_input":"2024-10-25T22:09:35.200148Z","iopub.status.idle":"2024-10-25T22:09:52.746285Z","shell.execute_reply.started":"2024-10-25T22:09:35.200090Z","shell.execute_reply":"2024-10-25T22:09:52.744750Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pennylane qiskit pennylane-qiskit pylatexenc","metadata":{"id":"_CF_l60hp0xJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb90877-280c-44bc-a443-4bb69afffde4","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T22:09:52.749074Z","iopub.execute_input":"2024-10-25T22:09:52.749486Z","iopub.status.idle":"2024-10-25T22:10:23.191899Z","shell.execute_reply.started":"2024-10-25T22:09:52.749442Z","shell.execute_reply":"2024-10-25T22:10:23.190223Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.38.0-py3-none-any.whl.metadata (9.3 kB)\nCollecting qiskit\n  Downloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.38.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.14.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.3)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting autograd (from pennylane)\n  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nCollecting pennylane-lightning>=0.38 (from pennylane)\n  Downloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane) (21.3)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.12)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (0.3.8)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from qiskit) (2.9.0.post0)\nCollecting stevedore>=3.0.0 (from qiskit)\n  Downloading stevedore-5.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting symengine<0.14,>=0.11 (from qiskit)\n  Downloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting qiskit-aer (from pennylane-qiskit)\n  Downloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nCollecting qiskit-ibm-runtime<=0.29 (from pennylane-qiskit)\n  Downloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl.metadata (19 kB)\nCollecting qiskit-ibm-provider (from pennylane-qiskit)\n  Downloading qiskit_ibm_provider-0.11.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\nCollecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.26.18)\nRequirement already satisfied: websocket-client>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.8.0)\nCollecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_platform_services-0.58.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.9.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.8.30)\nCollecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane) (3.1.2)\nRequirement already satisfied: psutil>=5 in /opt/conda/lib/python3.10/site-packages (from qiskit-aer->pennylane-qiskit) (5.9.3)\nRequirement already satisfied: websockets>=10.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (12.0)\nCollecting ibm-cloud-sdk-core<4.0.0,>=3.22.0 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.23.4)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (42.0.8)\nCollecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading pyspnego-0.11.1-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.16.0)\nCollecting urllib3>=1.21.1 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.22.0->ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.8.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.22)\nDownloading PennyLane-0.38.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading PennyLane_qiskit-0.38.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading stevedore-5.3.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_provider-0.11.0-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ibm_platform_services-0.58.0-py3-none-any.whl (340 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.7/340.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\nDownloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspnego-0.11.1-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pylatexenc\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=b46a66a88df4a0a13986676c9ec8b4768fbbf81e5bbe5a37d3e4cbcb4337fbb3\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\nSuccessfully built pylatexenc\nInstalling collected packages: pylatexenc, urllib3, symengine, rustworkx, pbr, autoray, autograd, stevedore, qiskit, pyspnego, ibm-cloud-sdk-core, requests-ntlm, qiskit-aer, ibm-platform-services, qiskit-ibm-runtime, qiskit-ibm-provider, pennylane-lightning, pennylane, pennylane-qiskit\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autograd-1.7.0 autoray-0.7.0 ibm-cloud-sdk-core-3.22.0 ibm-platform-services-0.58.0 pbr-6.1.0 pennylane-0.38.0 pennylane-lightning-0.38.0 pennylane-qiskit-0.38.1 pylatexenc-2.10 pyspnego-0.11.1 qiskit-1.2.4 qiskit-aer-0.15.1 qiskit-ibm-provider-0.11.0 qiskit-ibm-runtime-0.29.0 requests-ntlm-1.3.0 rustworkx-0.15.1 stevedore-5.3.0 symengine-0.13.0 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pennylane as qml\nimport qiskit\nprint(qml.__version__)\nprint(qiskit.__version__)\nimport pennylane_qiskit\nprint(pennylane_qiskit.__version__)\nimport pennylane as qml\nfrom pennylane import numpy as np\n# from pennylane_qiskit import AerDevice","metadata":{"id":"wITHoRhbp1XM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9f8012a-a11d-4474-fd25-001a585a7b46","execution":{"iopub.status.busy":"2024-10-25T22:10:23.193640Z","iopub.execute_input":"2024-10-25T22:10:23.194041Z","iopub.status.idle":"2024-10-25T22:10:27.602163Z","shell.execute_reply.started":"2024-10-25T22:10:23.194001Z","shell.execute_reply":"2024-10-25T22:10:27.600995Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0.38.0\n1.2.4\n0.38.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install energyflow","metadata":{"id":"VqjY-j8Njo0M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cb36ad8-2f2f-45d3-f9d2-2492b2f455f7","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T22:10:27.605596Z","iopub.execute_input":"2024-10-25T22:10:27.606486Z","iopub.status.idle":"2024-10-25T22:10:42.440103Z","shell.execute_reply.started":"2024-10-25T22:10:27.606439Z","shell.execute_reply":"2024-10-25T22:10:42.438780Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting energyflow\n  Downloading EnergyFlow-1.3.2-py2.py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from energyflow) (1.26.4)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from energyflow) (1.16.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from energyflow) (3.11.0)\nCollecting wasserstein>=0.3.1 (from energyflow)\n  Downloading wasserstein-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: wurlitzer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wasserstein>=0.3.1->energyflow) (3.1.1)\nDownloading EnergyFlow-1.3.2-py2.py3-none-any.whl (700 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.5/700.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading wasserstein-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (502 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.2/502.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wasserstein, energyflow\nSuccessfully installed energyflow-1.3.2 wasserstein-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport energyflow\nfrom scipy.sparse import coo_matrix\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data.distributed import DistributedSampler\n\n\n# we define a function to return an adjacencyy matrix\n# for our graph data representing the jets.\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    # print(edge_mask[0])\n    # raise\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask *= diag_mask\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edge_mask, edges]\n\ndef retrieve_dataloaders(batch_size, num_data = -1, use_one_hot = False, cache_dir = './data', num_workers=4):\n    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n                            with_bc=False, cache_dir=cache_dir)\n    splits = ['train', 'val', 'test']\n    data = {type:{'raw':None,'label':None} for type in splits}\n    (data['train']['raw'],  data['val']['raw'],   data['test']['raw'],\n    data['train']['label'], data['val']['label'], data['test']['label']) = \\\n        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle = False)\n\n    enc = OneHotEncoder(handle_unknown='ignore').fit([[11],[13],[22],[130],[211],[321],[2112],[2212]])\n\n    for split, value in data.items():\n        pid = torch.from_numpy(np.abs(np.asarray(value['raw'][...,3], dtype=int))).unsqueeze(-1)\n        p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(value['raw'],error_on_unknown=True))\n        one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n        # one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n        one_hot = torch.from_numpy(one_hot)\n        mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n        charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n\n        if use_one_hot:\n            nodes = one_hot\n\n        # else:\n        #     nodes = torch.cat((mass,charge),dim=-1)\n\n        #     nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n\n        else:\n              # Concatenate mass and charge along the last dimension\n              concatenated = torch.cat((mass, charge), dim=-1)  # Shape (batch_size, n_nodes, 2)\n\n              # Reduce along the last dimension (e.g., by summing or averaging)\n              nodes = concatenated.sum(dim=-1, keepdim=True)  # Shape (batch_size, n_nodes, 1)\n\n              # Apply log-sign transformation if needed\n              nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n        atom_mask = (pid[...,0] != 0)\n\n        value['p4s'] = p4s\n        value['nodes'] = nodes\n        value['label'] = torch.from_numpy(value['label'])\n        value['atom_mask'] = atom_mask.to(torch.bool)\n\n        if split == 'train':\n            print(value['atom_mask'])\n\n    datasets = {split: TensorDataset(value['label'], value['p4s'],\n                                     value['nodes'], value['atom_mask'])\n                for split, value in data.items()}\n\n    # distributed training\n    # train_sampler = DistributedSampler(datasets['train'], shuffle=True)\n    # Construct PyTorch dataloaders from datasets\n    dataloaders = {split: DataLoader(dataset,\n                                     batch_size=batch_size,\n                                     # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n                                     pin_memory=False,\n                                     # persistent_workers=True,\n                                     drop_last=True if (split == 'train') else False,\n                                     num_workers=num_workers,\n                                     collate_fn=collate_fn)\n                        for split, dataset in datasets.items()}\n\n    return dataloaders #train_sampler, dataloaders\n\nif __name__ == '__main__':\n    # train_sampler, dataloaders = retrieve_dataloaders(32, 100)\n    dataloaders = retrieve_dataloaders(batch_size=16, num_data = 20, use_one_hot = True)\n    for (label, p4s, nodes, atom_mask, edge_mask, edges) in dataloaders['train']:\n        print(label.shape, p4s.shape, nodes.shape, atom_mask.shape,\n              edge_mask.shape, edges[0].shape, edges[1].shape)\n        break","metadata":{"id":"kSESzCxAXLp5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"61e98bdd-bd95-4736-ff0a-0e8d0d8b974e","execution":{"iopub.status.busy":"2024-10-25T22:10:42.441910Z","iopub.execute_input":"2024-10-25T22:10:42.442331Z","iopub.status.idle":"2024-10-25T22:10:55.715751Z","shell.execute_reply.started":"2024-10-25T22:10:42.442290Z","shell.execute_reply":"2024-10-25T22:10:55.714370Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading QG_jets.npz from https://www.dropbox.com/s/fclsl7pukcpobsb/QG_jets.npz?dl=1 to ./data/datasets\nURL fetch failure on https://www.dropbox.com/s/fclsl7pukcpobsb/QG_jets.npz?dl=1: None -- Bad Request\nFailed to download QG_jets.npz from source 'dropbox', trying next source...\nDownloading QG_jets.npz from https://zenodo.org/record/3164691/files/QG_jets.npz?download=1 to ./data/datasets\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\ntorch.Size([16]) torch.Size([16, 139, 4]) torch.Size([16, 139, 8]) torch.Size([16, 139]) torch.Size([16, 139, 139]) torch.Size([28736]) torch.Size([28736])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the first batch\nfor label, p4s, nodes, atom_mask, edge_mask, edges in dataloaders[\"train\"]:\n    print(f\"Label shape: {label.shape}\")\n    print(f\"4-momenta shape: {p4s.shape}\")\n    print(f\"Node features shape: {nodes.shape}\")\n    print(f\"Atom mask shape: {atom_mask.shape}\")\n    print(f\"Edge mask shape: {edge_mask.shape}\")\n    print(f\"Edge indices shapes: {edges[0].shape}, {edges[1].shape}\")\n    break","metadata":{"id":"5I20CB8IXLp6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5fbcfdb9-80d8-4630-eda4-c2df78a6407d","execution":{"iopub.status.busy":"2024-10-25T22:10:55.717884Z","iopub.execute_input":"2024-10-25T22:10:55.719194Z","iopub.status.idle":"2024-10-25T22:10:55.892920Z","shell.execute_reply.started":"2024-10-25T22:10:55.719144Z","shell.execute_reply":"2024-10-25T22:10:55.891345Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Label shape: torch.Size([16])\n4-momenta shape: torch.Size([16, 139, 4])\nNode features shape: torch.Size([16, 139, 8])\nAtom mask shape: torch.Size([16, 139])\nEdge mask shape: torch.Size([16, 139, 139])\nEdge indices shapes: torch.Size([28736]), torch.Size([28736])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport energyflow\nimport os\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.sparse import coo_matrix\n\ndef save_physics_tensors(num_data=-1, use_one_hot=False, save_dir=\"random/data\"):\n    \"\"\"\n    Generate and save tensor data files needed for physics analysis.\n\n    Args:\n        num_data: Number of data points to generate (-1 for all)\n        save_dir: Directory to save the tensor files\n    \"\"\"\n    # Create save directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load raw data\n    raw = energyflow.qg_jets.load(\n        num_data=num_data,\n        pad=True,\n        ncol=4,\n        generator=\"pythia\",\n        with_bc=False,\n    )\n\n    # Get data and labels\n    data, labels = raw\n\n    # Initialize one-hot encoder for particle IDs\n    enc = OneHotEncoder(handle_unknown=\"ignore\").fit(\n        [[11], [13], [22], [130], [211], [321], [2112], [2212]]\n    )\n\n    # Process data\n    pid = torch.from_numpy(np.abs(np.asarray(data[..., 3], dtype=int))).unsqueeze(-1)\n    p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(data, error_on_unknown=True))\n\n    # Create one-hot encoded nodes\n    one_hot = enc.transform(pid.reshape(-1, 1)).toarray().reshape(pid.shape[:2] + (-1,))\n    nodes = torch.from_numpy(one_hot)\n    mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n    charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n\n    if use_one_hot:\n        nodes = one_hot\n\n    else:\n          # Concatenate mass and charge along the last dimension\n          concatenated = torch.cat((mass, charge), dim=-1)  # Shape (batch_size, n_nodes, 2)\n\n          # Reduce along the last dimension (e.g., by summing or averaging)\n          nodes = concatenated.sum(dim=-1, keepdim=True)  # Shape (batch_size, n_nodes, 1)\n\n          # Apply log-sign transformation if needed\n          nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n    # Create masks\n    atom_mask = (pid[..., 0] != 0).to(torch.bool)\n\n    # Create edge mask\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask = edge_mask * diag_mask\n\n    # Convert labels to tensor\n    labels = torch.from_numpy(labels)\n\n    # Calculate edges for the full dataset\n    n_nodes = p4s.size(1)\n    batch_size = p4s.size(0)\n\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx * n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n    edges = np.stack([rows, cols])\n\n    # Save tensors\n    torch.save(p4s, os.path.join(save_dir, \"p4s.pt\"))\n    torch.save(nodes, os.path.join(save_dir, \"nodes.pt\"))\n    torch.save(labels, os.path.join(save_dir, \"labels.pt\"))\n    torch.save(atom_mask, os.path.join(save_dir, \"atom_mask.pt\"))\n    np.save(os.path.join(save_dir, \"edge_mask.npy\"), edge_mask.numpy())\n    np.save(os.path.join(save_dir, \"edges.npy\"), edges)\n\n    print(f\"Saved tensor files to {save_dir}\")\n    print(f\"Shapes:\")\n    print(f\"p4s: {p4s.shape}\")\n    print(f\"nodes: {nodes.shape}\")\n    print(f\"labels: {labels.shape}\")\n    print(f\"atom_mask: {atom_mask.shape}\")\n    print(f\"edge_mask: {edge_mask.shape}\")\n    print(f\"edges: {edges.shape}\")\n\n# Generate and save the tensor files\nsave_physics_tensors(num_data=1000, use_one_hot=False)  # Use same number of data points as before","metadata":{"id":"SKTo7tNemda4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"adae491b-4682-45b9-924c-31b85a79279d","execution":{"iopub.status.busy":"2024-10-25T22:10:55.898270Z","iopub.execute_input":"2024-10-25T22:10:55.898791Z","iopub.status.idle":"2024-10-25T22:11:08.373735Z","shell.execute_reply.started":"2024-10-25T22:10:55.898743Z","shell.execute_reply":"2024-10-25T22:11:08.372570Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading QG_jets.npz from https://www.dropbox.com/s/fclsl7pukcpobsb/QG_jets.npz?dl=1 to /root/.energyflow/datasets\nURL fetch failure on https://www.dropbox.com/s/fclsl7pukcpobsb/QG_jets.npz?dl=1: None -- Bad Request\nFailed to download QG_jets.npz from source 'dropbox', trying next source...\nDownloading QG_jets.npz from https://zenodo.org/record/3164691/files/QG_jets.npz?download=1 to /root/.energyflow/datasets\nSaved tensor files to random/data\nShapes:\np4s: torch.Size([1000, 139, 4])\nnodes: torch.Size([1000, 139, 1])\nlabels: torch.Size([1000])\natom_mask: torch.Size([1000, 139])\nedge_mask: torch.Size([1000, 139, 139])\nedges: (2, 2145950)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    # edge_mask *= diag_mask\n\n    edge_mask = data[-2]\n\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edges]\n\n\np4s = torch.load('random/data/p4s.pt')\nnodes = torch.load('random/data/nodes.pt')\nlabels = torch.load('random/data/labels.pt')\natom_mask = torch.load('random/data/atom_mask.pt')\nedge_mask = torch.from_numpy(np.load('random/data/edge_mask.npy'))\nedges = torch.from_numpy(np.load('random/data/edges.npy'))\n\n\n# Create a TensorDataset\ndataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n\n# Define the split ratios\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\n# Calculate the lengths for each split\ntotal_size = len(dataset_all)\ntrain_size = int(total_size * train_ratio)\nval_size = int(total_size * val_ratio)\ntest_size = total_size - train_size - val_size  # Ensure all data is used\n\n# Split the dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset_all, [train_size, val_size, test_size])\n\n# Create a dictionary to hold the datasets\ndatasets = {\n    \"train\": train_dataset,\n    \"val\": val_dataset,\n    \"test\": test_dataset\n}\n\ndataloaders = {split: DataLoader(dataset,\n                                 batch_size=16,\n                                 # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n                                 pin_memory=False,\n                                 # persistent_workers=True,\n                                 collate_fn = collate_fn,\n                                 drop_last=True if (split == 'train') else False,\n                                 num_workers=0)\n                    for split, dataset in datasets.items()}","metadata":{"id":"bj-Ig4VZXLp6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"839e7ac0-eeba-4b18-db42-40fcd3d50593","execution":{"iopub.status.busy":"2024-10-25T22:11:08.375472Z","iopub.execute_input":"2024-10-25T22:11:08.375974Z","iopub.status.idle":"2024-10-25T22:11:08.414352Z","shell.execute_reply.started":"2024-10-25T22:11:08.375912Z","shell.execute_reply":"2024-10-25T22:11:08.413048Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1146867464.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  p4s = torch.load('random/data/p4s.pt')\n/tmp/ipykernel_30/1146867464.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  nodes = torch.load('random/data/nodes.pt')\n/tmp/ipykernel_30/1146867464.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  labels = torch.load('random/data/labels.pt')\n/tmp/ipykernel_30/1146867464.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  atom_mask = torch.load('random/data/atom_mask.pt')\n","output_type":"stream"}]},{"cell_type":"code","source":"# # we can peek at a batch to see what it looks like.\n# next(iter(dataloaders['val']))","metadata":{"id":"dh8IsQXgXLp7","execution":{"iopub.status.busy":"2024-10-25T22:11:08.416097Z","iopub.execute_input":"2024-10-25T22:11:08.416502Z","iopub.status.idle":"2024-10-25T22:11:08.421779Z","shell.execute_reply.started":"2024-10-25T22:11:08.416461Z","shell.execute_reply":"2024-10-25T22:11:08.420455Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(p4s.shape) # p4s\nprint(nodes.shape) # mass\nprint(atom_mask.shape) # torch.ones\nprint(edge_mask.shape) # adj_matrix","metadata":{"id":"d5obJPELXLp7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cdc0c11-df10-4e58-c236-b96977e21166","execution":{"iopub.status.busy":"2024-10-25T22:11:08.427265Z","iopub.execute_input":"2024-10-25T22:11:08.427704Z","iopub.status.idle":"2024-10-25T22:11:08.434897Z","shell.execute_reply.started":"2024-10-25T22:11:08.427663Z","shell.execute_reply":"2024-10-25T22:11:08.433700Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([1000, 139, 4])\ntorch.Size([1000, 139, 1])\ntorch.Size([1000, 139])\ntorch.Size([1000, 139, 139])\n","output_type":"stream"}]},{"cell_type":"code","source":"dataloaders","metadata":{"id":"01O7t2mkXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7e39edc-c3fd-4d52-bdfd-217df4810f63","execution":{"iopub.status.busy":"2024-10-25T22:11:08.436448Z","iopub.execute_input":"2024-10-25T22:11:08.436954Z","iopub.status.idle":"2024-10-25T22:11:08.454776Z","shell.execute_reply.started":"2024-10-25T22:11:08.436902Z","shell.execute_reply":"2024-10-25T22:11:08.453432Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'train': <torch.utils.data.dataloader.DataLoader at 0x7efac5965570>,\n 'val': <torch.utils.data.dataloader.DataLoader at 0x7efae2850c70>,\n 'test': <torch.utils.data.dataloader.DataLoader at 0x7efae2850c40>}"},"metadata":{}}]},{"cell_type":"code","source":"# Set desired dimensions\nbatch_size = 1\nn_nodes = 3\ndevice = 'cpu'\ndtype = torch.float32\n\n# Print initial shapes\nprint(\"Initial shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Select subset of data\np4s = p4s[:batch_size, :n_nodes, :]\natom_mask = atom_mask[:batch_size, :n_nodes]\nedge_mask = edge_mask[:batch_size, :n_nodes, :n_nodes]\nnodes = nodes[:batch_size, :n_nodes, :]\n\nprint(\"\\nAfter selection shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Reshape tensors\natom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n# Don't reshape edge_mask yet\nnodes = nodes.view(batch_size * n_nodes, -1).to(device, dtype)\n\nprint(\"\\nAfter reshape shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)  # original shape\nprint(\"nodes:\", nodes.shape)\n\n# Recalculate edges for the subset\nfrom scipy.sparse import coo_matrix\nrows, cols = [], []\nfor batch_idx in range(batch_size):\n    nn = batch_idx * n_nodes\n    # Convert edge_mask to numpy and remove any extra dimensions\n    edge_mask_np = edge_mask[batch_idx].cpu().numpy().squeeze()\n    x = coo_matrix(edge_mask_np)\n    rows.append(nn + x.row)\n    cols.append(nn + x.col)\n\nedges = [torch.LongTensor(np.concatenate(rows)).to(device),\n         torch.LongTensor(np.concatenate(cols)).to(device)]\n\n# Now reshape edge_mask after edges are calculated\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nprint(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"TEs9qVoYXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d50c2761-09f4-4212-db8a-3854e4c63354","execution":{"iopub.status.busy":"2024-10-25T22:11:08.456809Z","iopub.execute_input":"2024-10-25T22:11:08.457249Z","iopub.status.idle":"2024-10-25T22:11:08.481686Z","shell.execute_reply.started":"2024-10-25T22:11:08.457207Z","shell.execute_reply":"2024-10-25T22:11:08.480349Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Initial shapes:\np4s: torch.Size([1000, 139, 4])\natom_mask: torch.Size([1000, 139])\nedge_mask: torch.Size([1000, 139, 139])\nnodes: torch.Size([1000, 139, 1])\n\nAfter selection shapes:\np4s: torch.Size([1, 3, 4])\natom_mask: torch.Size([1, 3])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([1, 3, 1])\n\nAfter reshape shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([3, 1])\n\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 1  #2500 #1\nn_nodes = 3 #139\ndevice = 'cpu'\ndtype = torch.float32\n\natom_positions = p4s[:, :, :].view(batch_size * n_nodes, -1).to(device, dtype)\n\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nedges = [a.to(device) for a in edges]\nnodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)","metadata":{"id":"dj_AzmjvXLp9","execution":{"iopub.status.busy":"2024-10-25T22:11:08.483213Z","iopub.execute_input":"2024-10-25T22:11:08.483676Z","iopub.status.idle":"2024-10-25T22:11:08.492302Z","shell.execute_reply.started":"2024-10-25T22:11:08.483618Z","shell.execute_reply":"2024-10-25T22:11:08.491180Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"cNq2qPiZKqis","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dfa72c5-27b6-467b-83f8-36e2ad8223ea","execution":{"iopub.status.busy":"2024-10-25T22:11:08.493806Z","iopub.execute_input":"2024-10-25T22:11:08.494203Z","iopub.status.idle":"2024-10-25T22:11:08.507628Z","shell.execute_reply.started":"2024-10-25T22:11:08.494165Z","shell.execute_reply":"2024-10-25T22:11:08.506469Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"code","source":"atom_mask[0]#.shape","metadata":{"id":"NQhDkqAQXLp9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c503537a-d0bd-4cdf-db50-825746d3eb2d","execution":{"iopub.status.busy":"2024-10-25T22:11:08.509254Z","iopub.execute_input":"2024-10-25T22:11:08.509713Z","iopub.status.idle":"2024-10-25T22:11:08.527191Z","shell.execute_reply.started":"2024-10-25T22:11:08.509672Z","shell.execute_reply":"2024-10-25T22:11:08.526090Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([1.])"},"metadata":{}}]},{"cell_type":"code","source":"p4s[0][2]","metadata":{"id":"zTcu7crAXLp9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5c5698d-9e54-4c0f-9b24-883312b1485d","execution":{"iopub.status.busy":"2024-10-25T22:11:08.528669Z","iopub.execute_input":"2024-10-25T22:11:08.529131Z","iopub.status.idle":"2024-10-25T22:11:08.539364Z","shell.execute_reply.started":"2024-10-25T22:11:08.529079Z","shell.execute_reply":"2024-10-25T22:11:08.538108Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([ 1.1594, -0.2378, -1.1238, -0.0723], dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"code","source":"atom_mask.shape","metadata":{"id":"TWPwTZmNXLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33e79a36-7004-4a0d-9eb3-52f1e0646462","execution":{"iopub.status.busy":"2024-10-25T22:11:08.541314Z","iopub.execute_input":"2024-10-25T22:11:08.542237Z","iopub.status.idle":"2024-10-25T22:11:08.548859Z","shell.execute_reply.started":"2024-10-25T22:11:08.542191Z","shell.execute_reply":"2024-10-25T22:11:08.547789Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 1])"},"metadata":{}}]},{"cell_type":"code","source":"p4s.shape # batch_size (number of jets or graphs), n_nodes (particles), n_features","metadata":{"id":"dfthGhJ3XLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae0d7ad3-8a66-4e80-9aa2-f74f10f7caa4","execution":{"iopub.status.busy":"2024-10-25T22:11:08.550305Z","iopub.execute_input":"2024-10-25T22:11:08.550934Z","iopub.status.idle":"2024-10-25T22:11:08.563027Z","shell.execute_reply.started":"2024-10-25T22:11:08.550891Z","shell.execute_reply":"2024-10-25T22:11:08.561657Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 4])"},"metadata":{}}]},{"cell_type":"code","source":"# random: x(atom_pos), edge_indx_tensor (edges = adj_matrix), edge_tensor (edge_mask = adj_matrix)\nprint(\"Atom mask: {}\".format(atom_mask[:2]))\nprint(\"Atom positions (x features, 4-momenta): {}\".format(atom_positions[:2]))\nprint(\"Nodes (scalars: mass & charge): {}\".format(nodes[:2]))\nprint(\"Edge mask: {}\".format(edge_mask[:2]))\nprint(\"Edges: {}\".format(edges[:2]))","metadata":{"id":"fDPOFJnXXLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb476565-9a69-4f8e-ffa0-90f75e7a6561","execution":{"iopub.status.busy":"2024-10-25T22:11:08.564545Z","iopub.execute_input":"2024-10-25T22:11:08.565132Z","iopub.status.idle":"2024-10-25T22:11:08.576776Z","shell.execute_reply.started":"2024-10-25T22:11:08.565078Z","shell.execute_reply":"2024-10-25T22:11:08.575404Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Atom mask: tensor([[1.],\n        [1.]])\nAtom positions (x features, 4-momenta): tensor([[ 0.2861,  0.0078, -0.2687,  0.0980],\n        [ 0.1653, -0.0258, -0.1580, -0.0414]])\nNodes (scalars: mass & charge): tensor([[-4.7488e-09],\n        [-2.2813e-09]])\nEdge mask: tensor([[False],\n        [ True]])\nEdges: [tensor([0, 0, 1, 1, 2, 2]), tensor([1, 2, 0, 2, 0, 1])]\n","output_type":"stream"}]},{"cell_type":"code","source":"edges[:2]#[0].shape","metadata":{"id":"lDz3-q69XLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01adc007-56a2-4307-c442-1c4a44f0b14e","execution":{"iopub.status.busy":"2024-10-25T22:11:08.578249Z","iopub.execute_input":"2024-10-25T22:11:08.578674Z","iopub.status.idle":"2024-10-25T22:11:08.587647Z","shell.execute_reply.started":"2024-10-25T22:11:08.578634Z","shell.execute_reply":"2024-10-25T22:11:08.586314Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[tensor([0, 0, 1, 1, 2, 2]), tensor([1, 2, 0, 2, 0, 1])]"},"metadata":{}}]},{"cell_type":"code","source":"# model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n#                          edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"Ss4oXJjGXLp-","execution":{"iopub.status.busy":"2024-10-25T22:11:08.589429Z","iopub.execute_input":"2024-10-25T22:11:08.590009Z","iopub.status.idle":"2024-10-25T22:11:08.595356Z","shell.execute_reply.started":"2024-10-25T22:11:08.589956Z","shell.execute_reply":"2024-10-25T22:11:08.594279Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# 3. LorentzNet\nBefore delving into the realm of quantum graph neural networks (QGNNs), we shall examine the performance and structure of a very well-known equivariant GNN, **LorentzNet** ([arXiv:2201.08187](https://arxiv.org/abs/2201.08187)), which is classical, on our dataset. Understanding the structure underlying LorentzNet will allow us to understand where to fit in our quantum models, and this will be the heart of our approach.\n\n## 3.1. Dataset Representation as Graphs\n\nWe already discussed this in the introduction, but again, let's remmber that in high-energy particle physics, **jets**—collimated streams of particles resulting from particle collisions—are complex objects that can be naturally represented as graphs. In our dataset:\n\n- Each **jet** is modeled as a graph \\( G = (V, E) \\), where:\n  - $V$ is the set of **nodes**, each corresponding to a constituent particle within the jet.\n  - $E$ is the set of **edges**, representing interactions or relationships between particles.\n- Each node (particle) is considered a point in Minkowski space $\\mathbb{R}^{1,3}$, respecting the spacetime symmetries of special relativity.\n- The number of particles (nodes) varies for each jet, reflecting the stochastic nature of particle collisions.\n\n**Reconstructing Four-Momentum Vectors**\n\nIn practice, particle data may not be directly provided as four-momentum vectors. Instead, they are often given in terms of:\n\n- **Transverse Momentum $p_T$**: Momentum perpendicular to the beam axis.\n- **Pseudo-rapidity $\\eta$**: A spatial coordinate describing the angle of a particle relative to the beam (forward-backward) direction.\n- **Azimuthal Angle $\\phi$**: Angle around the beam axis in the transverse plane.\n- **Particle Identification (PID)**: Integer codes representing particle types.\n\n**Conversion to Four-Momentum**\n\n- Using the relationships:\n\n  - $p_x = p_T \\cos\\phi$\n  - $p_y = p_T \\sin\\phi$\n  - $p_z = p_T \\sinh\\eta$\n  - $E = \\sqrt{p_T^2 \\cosh^2\\eta + m^2}$, where $m$ is the particle mass.\n\n- The **[EnergyFlow](https://energyflow.network/)** package converts this for us.\n\n**Implementation in Code**:\n\n- The first step in the data preprocessing involves reconstructing the four-momentum vectors using the available kinematic variables, which is fundamental for us, since:\n    - First, we want to ensure that the input to LorentzNet is correctly formatted and physically meaningful.\n    - Also, given the limitations on current quantum hardware, and since we are performing simulations currently, then the number of particles in the jet has to be cut down.\n\n## 3.2. Architecture Overview\n\nThe **LorentzNet** architecture is designed to process and analyze graphs while respecting the **Lorentz symmetry**, a fundamental symmetry in relativistic physics involving rotations and boosts in spacetime (changes in inertial frames).\n\n**Key Features of LorentzNet**:\n\n- Built upon the **universal approximation theorem** for **Lorentz-equivariant functions**. This theorem ensures that the network can approximate any Lorentz-equivariant function to arbitrary precision, given sufficient capacity.\n- Incorporates **message passing** mechanisms tailored to respect Lorentz symmetry.\n- Utilizes **continuous functions** modeled by neural networks to update node and edge features throughout the network layers.\n\n**Architecture Diagram**:\n\n<center>\n<img src=\"../figures/LorentzNet.png\" width=\"65%\" style=\"margin-left:auto; margin-right:auto\">\n</center>\n\n*(Figure: Schematic representation of the LorentzNet architecture.)*\n\n**Input Layer**\n\nThe **input** to the LorentzNet consists of:\n\n- **Four-momentum vectors** (coordinate embeddings) of particles from collision events.\n  - Each particle $i$ has a four-momentum $v_i = (E_i, p_{x_i}, p_{y_i}, p_{z_i})$, where:\n    - $E_i$ is the energy.\n    - $p_{x_i}, p_{y_i}, p_{z_i}$ are momentum components in three-dimensional space.\n- **Scalar features** (scalar embeddings) $s_i$ associated with each particle, such as:\n  - Mass.\n  - Electric charge.\n  - Particle identification (PID) codes.\n\nThe combined feature vector for each particle is:\n\n$$\nf_i = v_i \\oplus s_i,\n$$\n\nwhere $\\oplus$ denotes concatenation.\n\n**Lorentz Group Equivariant Block (LGEB)**\n\nAt the core of LorentzNet is the **Lorentz Group Equivariant Block (LGEB)**, which updates the features of particles (nodes) and their interactions (edges) while preserving Lorentz equivariance.\n\n**The components of LGEB**:\n\n1. **Edge Message Function $\\phi_e$**:\n   - Computes messages passed between particles.\n   - Captures pairwise interactions and relativistic geometrical relationships.\n\n2. **Coordinate Update Function $\\phi_x$**:\n   - Updates the coordinate embeddings of particles.\n   - Incorporates attention mechanisms respecting Minkowski spacetime.\n\n3. **Scalar Feature Update Function $\\phi_h$**:\n   - Updates scalar features of particles.\n   - Aggregates information from neighboring particles.\n\nThese functions are modeled using neural networks capable of approximating continuous functions.\n\n## 3.3. Detailed Formulation\n\n1. **Edge Message Computation $\\phi_e$**:\n\n   For particles $i$ and $j$ at layer $l$, the **edge message** $m_{ij}^{l}$ is computed as:\n\n   $$\n   m_{ij}^{l} = \\phi_e \\left( h_i^{l}, h_j^{l}, \\psi\\left( \\| x_i^{l} - x_j^{l} \\|^2 \\right), \\psi\\left( \\langle x_i^{l}, x_j^{l} \\rangle \\right) \\right),\n   $$\n\n   where:\n\n   - $h_i^{l}$ and $h_j^{l}$ are the scalar features of particles $i$ and $j$ at layer $l$.\n   - $x_i^{l}$ and $x_j^{l}$ are the coordinate embeddings (four-vectors) at layer $l$.\n   - $\\| x_i^{l} - x_j^{l} \\|^2$ is the squared Minkowski **distance** between particles $i$ and $j$.\n   - $\\langle x_i^{l}, x_j^{l} \\rangle$ is the Minkowski **inner product** (Lorentz dot product).\n   - $\\psi(\\cdot)$ is a normalization function defined as:\n\n     $$\n     \\psi(a) = \\operatorname{sgn}(a) \\cdot \\log\\left( |a| + 1 \\right),\n     $$\n\n     with $\\operatorname{sgn}(a)$ being the sign function.\n\n   **Purpose of $\\psi(\\cdot)$**:\n\n   - Helps normalize values that may have large magnitudes or come from different distributions.\n   - Ensures numerical stability during optimization by mapping inputs to a manageable range.\n\n\n2. **Coordinate Embedding Update $\\phi_x$**:\n\n   The **coordinate embeddings** of particles are updated via:\n\n   $$\n   x_i^{l+1} = x_i^{l} + c \\sum_{j \\in \\mathcal{N}(i)} \\phi_x ( m_{ij}^{l}) \\cdot x_j^{l},\n   $$\n\n   where:\n\n   - $\\mathcal{N}(i)$ denotes the **neighborhood** of particle $i$, i.e., particles connected to $i$ in the graph.\n   - $c$ is a scaling constant controlling the update magnitude.\n   - $\\phi_x ( m_{ij}^{l})$ computes an **attention weight** based on the edge message $m_{ij}^{l}$.\n\n   **Interpretation**:\n\n   - The update adds a weighted sum of neighboring coordinate embeddings $x_j^{l}$ to the current embedding $x_i^{l}$.\n   - This mechanism allows particles to incorporate spatial information from their neighbors, guided by the learned attention weights.\n\n\n3. **Scalar Feature Update $\\phi_h$**:\n\n   The **scalar features** are updated as:\n\n   $$\n   h_i^{l+1} = h_i^{l} + \\phi_h \\left( h_i^{l}, \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^{l} m_{ij}^{l} \\right),\n   $$\n\n   where:\n\n   - $w_{ij}^{l}$ is an **edge significance weight** calculated by:\n\n     $$\n     w_{ij}^{l} = \\phi_m \\left( m_{ij}^{l} \\right) \\in [0, 1],\n     $$\n\n     with $\\phi_m$ being a neural network outputting values in the range [0, 1].\n\n   - $\\phi_h$ aggregates information from neighboring particles to update $h_i^{l}$.\n\n   And for the Purpose of $w_{ij}^{l}$ and $\\phi_h$:\n\n   - $w_{ij}^{l}$ signifies the importance of the edge between particles $i$ and $j$.\n   - $\\phi_h$ integrates these weighted messages to refine the scalar features, enabling the network to learn complex interactions.\n\n\n**Avoiding Redundancy**\n\nA noteworthy aspect of LorentzNet is its approach to handling outputs:\n\n- Although both **coordinate embeddings** $x_i^{l}$ and **scalar features** $h_i^{l}$ are updated through the layers, the final output only uses the **scalar features** $h_i^{L}$ from the last layer $L$.\n- This strategy reduces redundancy and computational overhead because:\n\n  - The edge messages $m_{ij}^{l}$ already incorporate information from both $x_i^{l}$ and $x_j^{l}$.\n  - Focusing on scalar features simplifies the network output without losing critical information.\n\n      \n**Implementation Details**\n\nTo ensure fidelity with the original LorentzNet architecture and leverage existing optimizations, we utilize the official implementation provided by the authors:\n\n- **Repository**: [LorentzNet-release](https://github.com/sdogsq/LorentzNet-release/tree/main)","metadata":{"id":"_3cyDPxrXLp-"}},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\n\n\"\"\"Some auxiliary functions\"\"\"\n\ndef unsorted_segment_sum(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    return result\n\ndef unsorted_segment_mean(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    count = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    count.index_add_(0, segment_ids, torch.ones_like(data))\n    return result / count.clamp(min=1)\n\ndef normsq4(p):\n    r''' Minkowski square norm\n         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n    '''\n    psq = torch.pow(p, 2)\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef dotsq4(p,q):\n    r''' Minkowski inner product\n         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n    '''\n    psq = p*q\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef normA_fn(A):\n    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n\ndef dotA_fn(A):\n    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n\ndef psi(p):\n    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n    '''\n    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n\n\n\"\"\"Lorentz Group-Equivariant Block\"\"\"\n\nclass LGEB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LGEB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2.\n        print('Input size of phi_e: ', n_input)\n\n        self.include_x = include_x\n        self.phi_e = nn.Sequential(\n            nn.Linear(n_input, n_hidden, bias=False), # n_input * 2 + n_edge_attr\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU())\n\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        # Reduce the dimension of 'out' to 4 using a linear layer\n        out = self.dimension_reducer(out)\n        out = self.phi_e(out)\n        # print(\"m_model output: \", out.shape)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.phi_e(out)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m): # norms\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # print(\"m: \", m.shape)\n        # print(\"trans: \", trans.shape)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        # print(\"trans: \", trans.shape)\n        # print(\"x.size: \", x.size(0))\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            # print(\"X: \", x)\n            x = self.x_model(x, edges, x_diff, m)\n            # print(\"phi_x(X) = \", x, '\\n---\\n')\n\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LorentzNet(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LGEB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LorentzNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n\n        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n        return pred.squeeze(1)","metadata":{"id":"49hLoUYRXLp_","execution":{"iopub.status.busy":"2024-10-25T22:11:08.597308Z","iopub.execute_input":"2024-10-25T22:11:08.597838Z","iopub.status.idle":"2024-10-25T22:11:08.644930Z","shell.execute_reply.started":"2024-10-25T22:11:08.597787Z","shell.execute_reply":"2024-10-25T22:11:08.643572Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\\\n                                    n_node_attr=n_scalar, dropout=dropout,\\\n                                    c_weight=c_weight, last_layer=\\(i==n_layers-1), A=A, include_x=include_x)\n                                    \nWe are using n_hidden = 4 and n_layers = 6\n\nn_input=n_hidden, n_output=n_hidden, n_hidden=n_hidden, n_node_attr=n_scalar=8","metadata":{"id":"yKQ1ZOC2U92Y"}},{"cell_type":"markdown","source":"### Now that we have the official code for the classical, just for sanity checking, let's test for equivariance\n\nThe cell below is just an auxiliary function to give us the boosts","metadata":{"id":"v_Wr8LNBXLp_"}},{"cell_type":"code","source":"# @title\nfrom math import sqrt\nimport numpy as np\n\n# Speed of light (m/s)\nc = 299792458\n\n\"\"\"Lorentz transformations describe the transition between two inertial reference\nframes F and F', each of which is moving in some direction with respect to the\nother. This code only calculates Lorentz transformations for movement in the x\ndirection with no spatial rotation (i.e., a Lorentz boost in the x direction).\nThe Lorentz transformations are calculated here as linear transformations of\nfour-vectors [ct, x, y, z] described by Minkowski space. Note that t (time) is\nmultiplied by c (the speed of light) in the first entry of each four-vector.\n\nThus, if X = [ct; x; y; z] and X' = [ct'; x'; y'; z'] are the four-vectors for\ntwo inertial reference frames and X' moves in the x direction with velocity v\nwith respect to X, then the Lorentz transformation from X to X' is X' = BX,\nwhere\n\n    | γ  -γβ  0  0|\nB = |-γβ  γ   0  0|\n    | 0   0   1  0|\n    | 0   0   0  1|\n\nis the matrix describing the Lorentz boost between X and X',\nγ = 1 / √(1 - v²/c²) is the Lorentz factor, and β = v/c is the velocity as\na fraction of c.\n\"\"\"\n\n\ndef beta(velocity: float) -> float:\n    \"\"\"\n    Calculates β = v/c, the given velocity as a fraction of c\n    >>> beta(c)\n    1.0\n    >>> beta(199792458)\n    0.666435904801848\n    \"\"\"\n    if velocity > c:\n        raise ValueError(\"Speed must not exceed light speed 299,792,458 [m/s]!\")\n    elif velocity < 1:\n        # Usually the speed should be much higher than 1 (c order of magnitude)\n        raise ValueError(\"Speed must be greater than or equal to 1!\")\n\n    return velocity / c\n\n\ndef gamma(velocity: float) -> float:\n    \"\"\"\n    Calculate the Lorentz factor γ = 1 / √(1 - v²/c²) for a given velocity\n    >>> gamma(4)\n    1.0000000000000002\n    >>> gamma(1e5)\n    1.0000000556325075\n    >>> gamma(3e7)\n    1.005044845777813\n    >>> gamma(2.8e8)\n    2.7985595722318277\n    \"\"\"\n    return 1 / sqrt(1 - beta(velocity) ** 2)\n\n\ndef transformation_matrix(velocity: float) -> np.ndarray:\n    \"\"\"\n    Calculate the Lorentz transformation matrix for movement in the x direction:\n\n    | γ  -γβ  0  0|\n    |-γβ  γ   0  0|\n    | 0   0   1  0|\n    | 0   0   0  1|\n\n    where γ is the Lorentz factor and β is the velocity as a fraction of c\n    >>> transformation_matrix(29979245)\n    array([[ 1.00503781, -0.10050378,  0.        ,  0.        ],\n           [-0.10050378,  1.00503781,  0.        ,  0.        ],\n           [ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.        ,  0.        ,  0.        ,  1.        ]])\n    \"\"\"\n    return np.array(\n        [\n            [gamma(velocity), -gamma(velocity) * beta(velocity), 0, 0],\n            [-gamma(velocity) * beta(velocity), gamma(velocity), 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n        ]\n    )\n","metadata":{"cellView":"form","id":"KA_EwMJIXLp_","execution":{"iopub.status.busy":"2024-10-25T22:11:08.646779Z","iopub.execute_input":"2024-10-25T22:11:08.647223Z","iopub.status.idle":"2024-10-25T22:11:08.660723Z","shell.execute_reply.started":"2024-10-25T22:11:08.647183Z","shell.execute_reply":"2024-10-25T22:11:08.659460Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Now, the model","metadata":{"id":"hFsrWKHLXLp_"}},{"cell_type":"code","source":"# n_scalar = 8 in original !\nmodel = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)","metadata":{"id":"cp9yZmnOXLqA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b6bec5f-a280-4fe6-ed2a-39c8d24b05c7","execution":{"iopub.status.busy":"2024-10-25T22:11:08.662199Z","iopub.execute_input":"2024-10-25T22:11:08.663033Z","iopub.status.idle":"2024-10-25T22:11:08.682591Z","shell.execute_reply.started":"2024-10-25T22:11:08.662973Z","shell.execute_reply":"2024-10-25T22:11:08.681277Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Input size of phi_e:  4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Let's start with a default prediction","metadata":{"id":"hE-OtUjVXLqA"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"zzcmDxZxXLqA","execution":{"iopub.status.busy":"2024-10-25T22:11:08.684552Z","iopub.execute_input":"2024-10-25T22:11:08.685086Z","iopub.status.idle":"2024-10-25T22:11:08.732588Z","shell.execute_reply.started":"2024-10-25T22:11:08.685032Z","shell.execute_reply":"2024-10-25T22:11:08.731457Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"R4k_rlP6XLqF","execution":{"iopub.status.busy":"2024-10-25T22:11:08.734117Z","iopub.execute_input":"2024-10-25T22:11:08.734541Z","iopub.status.idle":"2024-10-25T22:11:08.746792Z","shell.execute_reply.started":"2024-10-25T22:11:08.734477Z","shell.execute_reply":"2024-10-25T22:11:08.745598Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### ... taking any random nonsense transformation in the four-momentum vectors\ni.e.: multiplying by 0.1. Does the hidden rep stay the same?","metadata":{"id":"1WHsdTBxXLqF"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"8Hxk_CYJXLqG","execution":{"iopub.status.busy":"2024-10-25T22:11:08.749726Z","iopub.execute_input":"2024-10-25T22:11:08.750213Z","iopub.status.idle":"2024-10-25T22:11:08.757906Z","shell.execute_reply.started":"2024-10-25T22:11:08.750161Z","shell.execute_reply":"2024-10-25T22:11:08.756785Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"1EiVpL33XLqG","execution":{"iopub.status.busy":"2024-10-25T22:11:08.765028Z","iopub.execute_input":"2024-10-25T22:11:08.765566Z","iopub.status.idle":"2024-10-25T22:11:08.773523Z","shell.execute_reply.started":"2024-10-25T22:11:08.765498Z","shell.execute_reply":"2024-10-25T22:11:08.772154Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Even though the final logits in this case wasn't different, if we look the last output of h (which contains both scalar and 4-momenta information), it changed! Now, what about Lorentz transformations?","metadata":{"id":"70ulhirrXLqH"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"2IVU9kGJXLqH","execution":{"iopub.status.busy":"2024-10-25T22:11:08.775033Z","iopub.execute_input":"2024-10-25T22:11:08.775808Z","iopub.status.idle":"2024-10-25T22:11:08.787473Z","shell.execute_reply.started":"2024-10-25T22:11:08.775754Z","shell.execute_reply":"2024-10-25T22:11:08.786181Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"U1AUOcTDXLqH","execution":{"iopub.status.busy":"2024-10-25T22:11:08.789564Z","iopub.execute_input":"2024-10-25T22:11:08.790194Z","iopub.status.idle":"2024-10-25T22:11:08.799143Z","shell.execute_reply.started":"2024-10-25T22:11:08.790140Z","shell.execute_reply":"2024-10-25T22:11:08.797737Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Equivariance works. Finally, let's train on some data","metadata":{"id":"dRYlUAEVXLqI"}},{"cell_type":"code","source":"# @title\nimport torch\nimport os, json, random, string\nimport torch.distributed as dist\n\ndef makedir(path):\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n\ndef args_init(args):\n    r''' Initialize seed and exp_name.\n    '''\n    if args.seed is None: # use random seed if not specified\n        args.seed = np.random.randint(100)\n    if args.exp_name == '': # use random strings if not specified\n        args.exp_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n    if (args.local_rank == 0): # master\n        print(args)\n        makedir(f\"{args.logdir}/{args.exp_name}\")\n        with open(f\"{args.logdir}/{args.exp_name}/args.json\", 'w') as f:\n            json.dump(args.__dict__, f, indent=4)\n\ndef sum_reduce(num, device):\n    r''' Sum the tensor across the devices.\n    '''\n    if not torch.is_tensor(num):\n        rt = torch.tensor(num).to(device)\n    else:\n        rt = num.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    return rt\n\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        warmup_epoch: target learning rate is reached at warmup_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    Reference:\n        https://github.com/ildoonet/pytorch-gradual-warmup-lr\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, warmup_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.warmup_epoch = warmup_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    @property\n    def _warmup_lr(self):\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch + 1) / self.warmup_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * (self.last_epoch + 1) / self.warmup_epoch + 1.) for base_lr in self.base_lrs]\n\n    def get_lr(self):\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        return self._warmup_lr\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        self.last_epoch = self.last_epoch + 1 if epoch==None else epoch\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if not self.finished:\n                warmup_lr = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                    param_group['lr'] = lr\n                self.finished = True\n                return\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.warmup_epoch)\n            return\n\n        for param_group, lr in zip(self.optimizer.param_groups, self._warmup_lr):\n            param_group['lr'] = lr\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.warmup_epoch)\n                self.last_epoch = self.after_scheduler.last_epoch + self.warmup_epoch + 1\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        result = {key: value for key, value in self.__dict__.items() if key != 'optimizer' or key != \"after_scheduler\"}\n        if self.after_scheduler:\n            result.update({\"after_scheduler\": self.after_scheduler.state_dict()})\n        return result\n\n    def load_state_dict(self, state_dict):\n        after_scheduler_state = state_dict.pop(\"after_scheduler\", None)\n        self.__dict__.update(state_dict)\n        if after_scheduler_state:\n            self.after_scheduler.load_state_dict(after_scheduler_state)\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport numpy as np\n\ndef buildROC(labels, score, targetEff=[0.3,0.5]):\n    r''' ROC curve is a plot of the true positive rate (Sensitivity) in the function of the false positive rate\n    (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a\n    sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC\n    curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups.\n    '''\n    if not isinstance(targetEff, list):\n        targetEff = [targetEff]\n    fpr, tpr, threshold = roc_curve(labels, score)\n    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n    eB, eS = fpr[idx], tpr[idx]\n    return fpr, tpr, threshold, eB, eS","metadata":{"cellView":"form","id":"KRBzC37VorM9","execution":{"iopub.status.busy":"2024-10-25T22:11:08.801253Z","iopub.execute_input":"2024-10-25T22:11:08.801780Z","iopub.status.idle":"2024-10-25T22:11:08.931309Z","shell.execute_reply.started":"2024-10-25T22:11:08.801729Z","shell.execute_reply":"2024-10-25T22:11:08.930037Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\n\ndef run(model, epoch, loader, partition, N_EPOCHS=None):\n    if partition == 'train':\n        model.train()\n    else:\n        model.eval()\n\n    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n\n    tik = time.time()\n    loader_length = len(loader)\n\n    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n        if partition == 'train':\n            optimizer.zero_grad()\n\n        batch_size, n_nodes, _ = p4s.size()\n        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n        edges = [a.to(device) for a in edges]\n        label = label.to(device, dtype).long()\n\n        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                         edge_mask=edge_mask, n_nodes=n_nodes)\n\n        predict = pred.max(1).indices\n        correct = torch.sum(predict == label).item()\n        loss = loss_fn(pred, label)\n\n        if partition == 'train':\n            loss.backward()\n            optimizer.step()\n        elif partition == 'test':\n            # save labels and probilities for ROC / AUC\n            # print(\"Preds \", pred)\n            score = torch.nn.functional.softmax(pred, dim = -1)\n            # print(\"Score test \", score)\n            # raise\n            res['label'].append(label)\n            res['score'].append(score)\n\n        res['time'] = time.time() - tik\n        res['correct'] += correct\n        res['loss'] += loss.item() * batch_size\n        res['counter'] += batch_size\n        res['loss_arr'].append(loss.item())\n        res['correct_arr'].append(correct)\n\n        # if i != 0 and i % args.log_interval == 0:\n\n    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n    avg_time = res['time']/res['counter'] * batch_size\n    tmp_counter = res['counter']\n    tmp_loss = res['loss'] / tmp_counter\n    tmp_acc = res['correct'] / tmp_counter\n\n    if N_EPOCHS:\n        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n    else:\n        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, running_loss, running_acc, tmp_acc, avg_time))\n\n    torch.cuda.empty_cache()\n    # ---------- reduce -----------\n    if partition == 'test':\n        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n        res['score'] = torch.cat(res['score'])\n        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n    res['counter'] = res['counter']\n    res['loss'] = res['loss'] / res['counter']\n    res['acc'] = res['correct'] / res['counter']\n    return res\n\ndef train(model, res, N_EPOCHS, model_path, log_path):\n    ### training and validation\n    os.makedirs(model_path, exist_ok=True)\n    os.makedirs(log_path, exist_ok=True)\n\n    for epoch in range(N_EPOCHS):\n        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n        # if epoch % args.val_interval == 0:\n\n        # if (args.local_rank == 0):\n        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n        with torch.no_grad():\n            val_res = run(model, epoch, dataloaders['val'], partition='val')\n\n        # if (args.local_rank == 0): # only master process save\n        res['lr'].append(optimizer.param_groups[0]['lr'])\n        res['train_time'].append(train_res['time'])\n        res['val_time'].append(val_res['time'])\n        res['train_loss'].append(train_res['loss'])\n        res['train_acc'].append(train_res['acc'])\n        res['val_loss'].append(val_res['loss'])\n        res['val_acc'].append(val_res['acc'])\n        res['epochs'].append(epoch)\n\n        ## save best model\n        if val_res['acc'] > res['best_val']:\n            print(\"New best validation model, saving...\")\n            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n            res['best_val'] = val_res['acc']\n            res['best_epoch'] = epoch\n\n        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n\n        json_object = json.dumps(res, indent=4)\n        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n            outfile.write(json_object)\n\n        ## adjust learning rate\n        if (epoch < 31):\n            lr_scheduler.step(metrics=val_res['acc'])\n        else:\n            for g in optimizer.param_groups:\n                g['lr'] = g['lr']*0.5\n\n\ndef test(model, res, model_path, log_path):\n    ### test on best model\n    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n    model.load_state_dict(best_model)\n    with torch.no_grad():\n        test_res = run(model, 0, dataloaders['test'], partition='test')\n\n    print(\"Final \", test_res['score'])\n    pred = test_res['score'].cpu()\n\n    np.save(os.path.join(log_path, \"score.npy\"), pred)\n    fpr, tpr, thres, eB, eS  = buildROC(pred[...,0], pred[...,2])\n    auc = roc_auc_score(pred[...,0], pred[...,2])\n\n    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n    res.update(metric)\n    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n    json_object = json.dumps(res, indent=4)\n    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n        outfile.write(json_object)\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 45 # 60\n\n    model_path = \"models/LorentzNet/\"\n    log_path = \"logs/LorentzNet/\"\n    # args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cuda\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cpu\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    ### create parallel model\n    model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"scrolled":true,"id":"8azxxVjtXLqI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3deb62b1-8ab6-43e5-d765-f7dc0e49a783","execution":{"iopub.status.busy":"2024-10-25T22:11:08.933311Z","iopub.execute_input":"2024-10-25T22:11:08.933726Z","iopub.status.idle":"2024-10-25T22:11:34.044665Z","shell.execute_reply.started":"2024-10-25T22:11:08.933684Z","shell.execute_reply":"2024-10-25T22:11:34.043365Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Input size of phi_e:  4\nModel Size: 199\n train samples: 800\n val samples: 100\n test samples: 100\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 95.57it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/45 \t Batch 49/50 \t Loss 0.6825 \t Running Acc 0.517 \t Total Acc 0.517 \t Avg Batch Time 0.0106\nTime: train: 0.53 \t Train loss 0.6825 \t Train acc: 0.5175\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 208.22it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6789 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 0/45 finished.\nTrain time: 0.53 \t Val time 0.04\nTrain loss 0.6825 \t Train acc: 0.5175\nVal loss: 0.6762 \t Val acc: 0.5400\nBest val acc: 0.5400 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.61it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/45 \t Batch 49/50 \t Loss 0.6855 \t Running Acc 0.501 \t Total Acc 0.501 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.6855 \t Train acc: 0.5012\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 212.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6760 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 0.0014\nEpoch 1/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.6855 \t Train acc: 0.5012\nVal loss: 0.6729 \t Val acc: 0.5400\nBest val acc: 0.5400 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.68it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/45 \t Batch 49/50 \t Loss 0.6805 \t Running Acc 0.511 \t Total Acc 0.511 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6805 \t Train acc: 0.5112\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 195.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6718 \t Running Acc 2.000 \t Total Acc 0.560 \t Avg Batch Time 0.0015\nNew best validation model, saving...\nEpoch 2/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.6805 \t Train acc: 0.5112\nVal loss: 0.6680 \t Val acc: 0.5600\nBest val acc: 0.5600 at epoch 2.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.01it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/45 \t Batch 49/50 \t Loss 0.6710 \t Running Acc 0.552 \t Total Acc 0.552 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6710 \t Train acc: 0.5525\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 215.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6666 \t Running Acc 2.250 \t Total Acc 0.630 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 3/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6710 \t Train acc: 0.5525\nVal loss: 0.6619 \t Val acc: 0.6300\nBest val acc: 0.6300 at epoch 3.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.06it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/45 \t Batch 49/50 \t Loss 0.6622 \t Running Acc 0.588 \t Total Acc 0.588 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6622 \t Train acc: 0.5875\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 204.45it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6588 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 0.0015\nNew best validation model, saving...\nEpoch 4/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.6622 \t Train acc: 0.5875\nVal loss: 0.6526 \t Val acc: 0.6700\nBest val acc: 0.6700 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.65it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/45 \t Batch 49/50 \t Loss 0.6528 \t Running Acc 0.661 \t Total Acc 0.661 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6528 \t Train acc: 0.6613\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 214.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6524 \t Running Acc 2.429 \t Total Acc 0.680 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 5/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6528 \t Train acc: 0.6613\nVal loss: 0.6450 \t Val acc: 0.6800\nBest val acc: 0.6800 at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.55it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/45 \t Batch 49/50 \t Loss 0.6459 \t Running Acc 0.665 \t Total Acc 0.665 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.6459 \t Train acc: 0.6650\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 222.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6485 \t Running Acc 2.357 \t Total Acc 0.660 \t Avg Batch Time 0.0014\nEpoch 6/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6459 \t Train acc: 0.6650\nVal loss: 0.6403 \t Val acc: 0.6600\nBest val acc: 0.6800 at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/45 \t Batch 49/50 \t Loss 0.6413 \t Running Acc 0.681 \t Total Acc 0.681 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6413 \t Train acc: 0.6813\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 211.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6477 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 0.0014\nEpoch 7/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.6413 \t Train acc: 0.6813\nVal loss: 0.6392 \t Val acc: 0.6700\nBest val acc: 0.6800 at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/45 \t Batch 49/50 \t Loss 0.6381 \t Running Acc 0.695 \t Total Acc 0.695 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.6381 \t Train acc: 0.6950\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 205.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6352 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0015\nNew best validation model, saving...\nEpoch 8/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.6381 \t Train acc: 0.6950\nVal loss: 0.6238 \t Val acc: 0.6900\nBest val acc: 0.6900 at epoch 8.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/45 \t Batch 49/50 \t Loss 0.6154 \t Running Acc 0.725 \t Total Acc 0.725 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.6154 \t Train acc: 0.7250\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 202.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6204 \t Running Acc 2.429 \t Total Acc 0.680 \t Avg Batch Time 0.0015\nEpoch 9/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.6154 \t Train acc: 0.7250\nVal loss: 0.6043 \t Val acc: 0.6800\nBest val acc: 0.6900 at epoch 8.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 94.07it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/45 \t Batch 49/50 \t Loss 0.5964 \t Running Acc 0.754 \t Total Acc 0.754 \t Avg Batch Time 0.0107\nTime: train: 0.53 \t Train loss 0.5964 \t Train acc: 0.7538\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 212.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6124 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0014\nEpoch 10/45 finished.\nTrain time: 0.53 \t Val time 0.03\nTrain loss 0.5964 \t Train acc: 0.7538\nVal loss: 0.5953 \t Val acc: 0.6900\nBest val acc: 0.6900 at epoch 8.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.39it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/45 \t Batch 49/50 \t Loss 0.5956 \t Running Acc 0.726 \t Total Acc 0.726 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5956 \t Train acc: 0.7262\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 200.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6077 \t Running Acc 2.429 \t Total Acc 0.680 \t Avg Batch Time 0.0015\nEpoch 11/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5956 \t Train acc: 0.7262\nVal loss: 0.5897 \t Val acc: 0.6800\nBest val acc: 0.6900 at epoch 8.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.90it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/45 \t Batch 49/50 \t Loss 0.5785 \t Running Acc 0.751 \t Total Acc 0.751 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.5785 \t Train acc: 0.7512\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 209.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6048 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0014\nEpoch 12/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5785 \t Train acc: 0.7512\nVal loss: 0.5860 \t Val acc: 0.6900\nBest val acc: 0.6900 at epoch 8.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.57it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/45 \t Batch 49/50 \t Loss 0.5808 \t Running Acc 0.740 \t Total Acc 0.740 \t Avg Batch Time 0.0099\nTime: train: 0.49 \t Train loss 0.5808 \t Train acc: 0.7400\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 211.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6029 \t Running Acc 2.607 \t Total Acc 0.730 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 13/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5808 \t Train acc: 0.7400\nVal loss: 0.5840 \t Val acc: 0.7300\nBest val acc: 0.7300 at epoch 13.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.31it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/45 \t Batch 49/50 \t Loss 0.5773 \t Running Acc 0.741 \t Total Acc 0.741 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5773 \t Train acc: 0.7412\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 221.14it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6024 \t Running Acc 2.643 \t Total Acc 0.740 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 14/45 finished.\nTrain time: 0.50 \t Val time 0.03\nTrain loss 0.5773 \t Train acc: 0.7412\nVal loss: 0.5834 \t Val acc: 0.7400\nBest val acc: 0.7400 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/45 \t Batch 49/50 \t Loss 0.5816 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5816 \t Train acc: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 186.87it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6023 \t Running Acc 2.643 \t Total Acc 0.740 \t Avg Batch Time 0.0016\nEpoch 15/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5816 \t Train acc: 0.7500\nVal loss: 0.5834 \t Val acc: 0.7400\nBest val acc: 0.7400 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.91it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/45 \t Batch 49/50 \t Loss 0.5808 \t Running Acc 0.754 \t Total Acc 0.754 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5808 \t Train acc: 0.7538\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 206.90it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5948 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0015\nEpoch 16/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5808 \t Train acc: 0.7538\nVal loss: 0.5730 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.55it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/45 \t Batch 49/50 \t Loss 0.5707 \t Running Acc 0.751 \t Total Acc 0.751 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.5707 \t Train acc: 0.7512\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 199.53it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5902 \t Running Acc 2.643 \t Total Acc 0.740 \t Avg Batch Time 0.0015\nEpoch 17/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5707 \t Train acc: 0.7512\nVal loss: 0.5675 \t Val acc: 0.7400\nBest val acc: 0.7400 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.91it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/45 \t Batch 49/50 \t Loss 0.5673 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5673 \t Train acc: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 210.29it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5868 \t Running Acc 2.714 \t Total Acc 0.760 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 18/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5673 \t Train acc: 0.7500\nVal loss: 0.5631 \t Val acc: 0.7600\nBest val acc: 0.7600 at epoch 18.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.43it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/45 \t Batch 49/50 \t Loss 0.5534 \t Running Acc 0.759 \t Total Acc 0.759 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5534 \t Train acc: 0.7588\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 184.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5832 \t Running Acc 2.714 \t Total Acc 0.760 \t Avg Batch Time 0.0016\nEpoch 19/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5534 \t Train acc: 0.7588\nVal loss: 0.5579 \t Val acc: 0.7600\nBest val acc: 0.7600 at epoch 18.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.25it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/45 \t Batch 49/50 \t Loss 0.5535 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5535 \t Train acc: 0.7450\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 198.52it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5808 \t Running Acc 2.714 \t Total Acc 0.760 \t Avg Batch Time 0.0015\nEpoch 20/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5535 \t Train acc: 0.7450\nVal loss: 0.5541 \t Val acc: 0.7600\nBest val acc: 0.7600 at epoch 18.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.68it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/45 \t Batch 49/50 \t Loss 0.5495 \t Running Acc 0.743 \t Total Acc 0.743 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5495 \t Train acc: 0.7425\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 201.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5768 \t Running Acc 2.714 \t Total Acc 0.760 \t Avg Batch Time 0.0015\nEpoch 21/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5495 \t Train acc: 0.7425\nVal loss: 0.5492 \t Val acc: 0.7600\nBest val acc: 0.7600 at epoch 18.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.12it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/45 \t Batch 49/50 \t Loss 0.5445 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5445 \t Train acc: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 197.96it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5764 \t Running Acc 2.750 \t Total Acc 0.770 \t Avg Batch Time 0.0015\nNew best validation model, saving...\nEpoch 22/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5445 \t Train acc: 0.7500\nVal loss: 0.5478 \t Val acc: 0.7700\nBest val acc: 0.7700 at epoch 22.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.70it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/45 \t Batch 49/50 \t Loss 0.5416 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5416 \t Train acc: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 202.03it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5757 \t Running Acc 2.750 \t Total Acc 0.770 \t Avg Batch Time 0.0015\nEpoch 23/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5416 \t Train acc: 0.7500\nVal loss: 0.5463 \t Val acc: 0.7700\nBest val acc: 0.7700 at epoch 22.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.80it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/45 \t Batch 49/50 \t Loss 0.5418 \t Running Acc 0.752 \t Total Acc 0.752 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5418 \t Train acc: 0.7525\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 218.36it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5749 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 24/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5418 \t Train acc: 0.7525\nVal loss: 0.5456 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.93it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 26/45 \t Batch 49/50 \t Loss 0.5372 \t Running Acc 0.746 \t Total Acc 0.746 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5372 \t Train acc: 0.7462\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 209.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5730 \t Running Acc 2.786 \t Total Acc 0.780 \t Avg Batch Time 0.0015\nEpoch 25/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5372 \t Train acc: 0.7462\nVal loss: 0.5429 \t Val acc: 0.7800\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 107.01it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 27/45 \t Batch 49/50 \t Loss 0.5411 \t Running Acc 0.746 \t Total Acc 0.746 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.5411 \t Train acc: 0.7462\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 220.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5727 \t Running Acc 2.786 \t Total Acc 0.780 \t Avg Batch Time 0.0014\nEpoch 26/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.5411 \t Train acc: 0.7462\nVal loss: 0.5424 \t Val acc: 0.7800\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.02it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 28/45 \t Batch 49/50 \t Loss 0.5394 \t Running Acc 0.752 \t Total Acc 0.752 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.5394 \t Train acc: 0.7525\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 219.51it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5727 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 27/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.5394 \t Train acc: 0.7525\nVal loss: 0.5427 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 94.78it/s] \n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 29/45 \t Batch 49/50 \t Loss 0.5389 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0106\nTime: train: 0.53 \t Train loss 0.5389 \t Train acc: 0.7575\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 205.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5720 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 28/45 finished.\nTrain time: 0.53 \t Val time 0.04\nTrain loss 0.5389 \t Train acc: 0.7575\nVal loss: 0.5417 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 30/45 \t Batch 49/50 \t Loss 0.5317 \t Running Acc 0.746 \t Total Acc 0.746 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5317 \t Train acc: 0.7462\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 207.60it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 29/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5317 \t Train acc: 0.7462\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.65it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 31/45 \t Batch 49/50 \t Loss 0.5410 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5410 \t Train acc: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 202.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 30/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5410 \t Train acc: 0.7500\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.11it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 32/45 \t Batch 49/50 \t Loss 0.5361 \t Running Acc 0.749 \t Total Acc 0.749 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5361 \t Train acc: 0.7488\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 215.22it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 31/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.5361 \t Train acc: 0.7488\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.01it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 33/45 \t Batch 49/50 \t Loss 0.5319 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5319 \t Train acc: 0.7575\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 207.05it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 32/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5319 \t Train acc: 0.7575\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.69it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 34/45 \t Batch 49/50 \t Loss 0.5300 \t Running Acc 0.761 \t Total Acc 0.761 \t Avg Batch Time 0.0097\nTime: train: 0.49 \t Train loss 0.5300 \t Train acc: 0.7612\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 207.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 33/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5300 \t Train acc: 0.7612\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.28it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 35/45 \t Batch 49/50 \t Loss 0.5316 \t Running Acc 0.749 \t Total Acc 0.749 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5316 \t Train acc: 0.7488\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 208.45it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 34/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5316 \t Train acc: 0.7488\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.79it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 36/45 \t Batch 49/50 \t Loss 0.5314 \t Running Acc 0.762 \t Total Acc 0.762 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.5314 \t Train acc: 0.7625\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 212.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 35/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5314 \t Train acc: 0.7625\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 37/45 \t Batch 49/50 \t Loss 0.5332 \t Running Acc 0.749 \t Total Acc 0.749 \t Avg Batch Time 0.0097\nTime: train: 0.49 \t Train loss 0.5332 \t Train acc: 0.7488\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 210.50it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 36/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5332 \t Train acc: 0.7488\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.29it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 38/45 \t Batch 49/50 \t Loss 0.5346 \t Running Acc 0.764 \t Total Acc 0.764 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5346 \t Train acc: 0.7638\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 211.28it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 37/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5346 \t Train acc: 0.7638\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.28it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 39/45 \t Batch 49/50 \t Loss 0.5302 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5302 \t Train acc: 0.7575\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 205.49it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 38/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5302 \t Train acc: 0.7575\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.68it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 40/45 \t Batch 49/50 \t Loss 0.5334 \t Running Acc 0.755 \t Total Acc 0.755 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5334 \t Train acc: 0.7550\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 214.11it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 39/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5334 \t Train acc: 0.7550\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 41/45 \t Batch 49/50 \t Loss 0.5328 \t Running Acc 0.755 \t Total Acc 0.755 \t Avg Batch Time 0.0099\nTime: train: 0.49 \t Train loss 0.5328 \t Train acc: 0.7550\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 203.93it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 40/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5328 \t Train acc: 0.7550\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 42/45 \t Batch 49/50 \t Loss 0.5353 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5353 \t Train acc: 0.7575\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 195.48it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0015\nEpoch 41/45 finished.\nTrain time: 0.49 \t Val time 0.04\nTrain loss 0.5353 \t Train acc: 0.7575\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.96it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 43/45 \t Batch 49/50 \t Loss 0.5338 \t Running Acc 0.748 \t Total Acc 0.748 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5338 \t Train acc: 0.7475\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 210.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 42/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5338 \t Train acc: 0.7475\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 89.39it/s] \n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 44/45 \t Batch 49/50 \t Loss 0.5306 \t Running Acc 0.762 \t Total Acc 0.762 \t Avg Batch Time 0.0112\nTime: train: 0.56 \t Train loss 0.5306 \t Train acc: 0.7625\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 186.58it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0016\nEpoch 43/45 finished.\nTrain time: 0.56 \t Val time 0.04\nTrain loss 0.5306 \t Train acc: 0.7625\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 97.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 45/45 \t Batch 49/50 \t Loss 0.5352 \t Running Acc 0.754 \t Total Acc 0.754 \t Avg Batch Time 0.0103\nTime: train: 0.52 \t Train loss 0.5352 \t Train acc: 0.7538\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 213.61it/s]\n/tmp/ipykernel_30/813438993.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5718 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 0.0014\nEpoch 44/45 finished.\nTrain time: 0.52 \t Val time 0.03\nTrain loss 0.5352 \t Train acc: 0.7538\nVal loss: 0.5415 \t Val acc: 0.8000\nBest val acc: 0.8000 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 198.18it/s]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.5655 \t Running Acc 2.750 \t Total Acc 0.770 \t Avg Batch Time 0.0015\nFinal  tensor([[1.0000, 0.3362, 0.6638],\n        [1.0000, 0.2920, 0.7080],\n        [0.0000, 0.7204, 0.2796],\n        [1.0000, 0.5617, 0.4383],\n        [1.0000, 0.3015, 0.6985],\n        [0.0000, 0.6670, 0.3330],\n        [0.0000, 0.7227, 0.2773],\n        [1.0000, 0.4954, 0.5046],\n        [1.0000, 0.3361, 0.6639],\n        [0.0000, 0.5018, 0.4982],\n        [0.0000, 0.6074, 0.3926],\n        [0.0000, 0.6310, 0.3690],\n        [0.0000, 0.3891, 0.6109],\n        [0.0000, 0.5062, 0.4938],\n        [0.0000, 0.5912, 0.4088],\n        [0.0000, 0.5481, 0.4519],\n        [0.0000, 0.5460, 0.4540],\n        [1.0000, 0.4910, 0.5090],\n        [0.0000, 0.7435, 0.2565],\n        [1.0000, 0.2933, 0.7067],\n        [1.0000, 0.4500, 0.5500],\n        [0.0000, 0.4926, 0.5074],\n        [0.0000, 0.2876, 0.7124],\n        [0.0000, 0.5232, 0.4768],\n        [1.0000, 0.7077, 0.2923],\n        [0.0000, 0.8423, 0.1577],\n        [0.0000, 0.8399, 0.1601],\n        [0.0000, 0.8068, 0.1932],\n        [0.0000, 0.4492, 0.5508],\n        [1.0000, 0.5491, 0.4509],\n        [0.0000, 0.3679, 0.6321],\n        [1.0000, 0.2870, 0.7130],\n        [1.0000, 0.3480, 0.6520],\n        [1.0000, 0.3943, 0.6057],\n        [0.0000, 0.6297, 0.3703],\n        [1.0000, 0.2870, 0.7130],\n        [1.0000, 0.3880, 0.6120],\n        [0.0000, 0.5404, 0.4596],\n        [1.0000, 0.3873, 0.6127],\n        [0.0000, 0.6454, 0.3546],\n        [0.0000, 0.7906, 0.2094],\n        [0.0000, 0.8585, 0.1415],\n        [1.0000, 0.3789, 0.6211],\n        [0.0000, 0.6809, 0.3191],\n        [1.0000, 0.5040, 0.4960],\n        [1.0000, 0.4481, 0.5519],\n        [1.0000, 0.4844, 0.5156],\n        [0.0000, 0.6720, 0.3280],\n        [1.0000, 0.3789, 0.6211],\n        [0.0000, 0.5221, 0.4779],\n        [1.0000, 0.4232, 0.5768],\n        [1.0000, 0.3999, 0.6001],\n        [1.0000, 0.5130, 0.4870],\n        [0.0000, 0.4939, 0.5061],\n        [1.0000, 0.4243, 0.5757],\n        [1.0000, 0.3470, 0.6530],\n        [0.0000, 0.5542, 0.4458],\n        [0.0000, 0.3092, 0.6908],\n        [0.0000, 0.6584, 0.3416],\n        [0.0000, 0.5491, 0.4510],\n        [1.0000, 0.3679, 0.6321],\n        [0.0000, 0.5778, 0.4222],\n        [0.0000, 0.5672, 0.4328],\n        [1.0000, 0.2870, 0.7130],\n        [1.0000, 0.4358, 0.5642],\n        [0.0000, 0.6231, 0.3769],\n        [1.0000, 0.2940, 0.7060],\n        [0.0000, 0.6847, 0.3153],\n        [0.0000, 0.8168, 0.1832],\n        [1.0000, 0.4125, 0.5875],\n        [1.0000, 0.6735, 0.3265],\n        [0.0000, 0.7919, 0.2081],\n        [0.0000, 0.2904, 0.7096],\n        [1.0000, 0.4016, 0.5984],\n        [0.0000, 0.8018, 0.1982],\n        [1.0000, 0.2986, 0.7014],\n        [0.0000, 0.7792, 0.2208],\n        [1.0000, 0.2870, 0.7130],\n        [0.0000, 0.4974, 0.5026],\n        [0.0000, 0.5280, 0.4720],\n        [0.0000, 0.3398, 0.6602],\n        [0.0000, 0.4868, 0.5132],\n        [1.0000, 0.4967, 0.5033],\n        [0.0000, 0.3653, 0.6347],\n        [0.0000, 0.6270, 0.3730],\n        [0.0000, 0.3657, 0.6343],\n        [0.0000, 0.6263, 0.3737],\n        [0.0000, 0.9035, 0.0965],\n        [1.0000, 0.5785, 0.4215],\n        [1.0000, 0.2870, 0.7130],\n        [1.0000, 0.5981, 0.4019],\n        [0.0000, 0.7960, 0.2040],\n        [1.0000, 0.2870, 0.7130],\n        [1.0000, 0.3479, 0.6521],\n        [1.0000, 0.2950, 0.7050],\n        [1.0000, 0.2870, 0.7130],\n        [0.0000, 0.4009, 0.5991],\n        [0.0000, 0.6613, 0.3387],\n        [1.0000, 0.7207, 0.2793],\n        [0.0000, 0.8844, 0.1156]])\nTest: Loss 0.5516 \t Acc 0.7700 \t AUC: 0.8218 \t 1/eB 0.3: 27.5000 \t 1/eB 0.5: 7.8571\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Equivariant Quantum Neural Networks\nNow, let's move to quantum machine learning. Given some group $\\mathcal{G}$, one common way to achieve equivariance [6] is to have a quantum neural network of the form $h_{\\theta} = Tr[\\rho \\tilde{O}_{\\theta}]$ such that:\n\n$$\\begin{align*}\n\\tilde{O}_{\\theta} \\in Comm(G) = \\{A \\in \\mathbb{C}^{d\\times d} / [A, R(g)] = 0 \\text{ for all } g \\in G\\}\n\\end{align*}$$\n\nTo see why, we need to observe that the trace is cyclical, so:\n\n$$\\begin{align*}\n    h_{\\theta} (g\\cdot \\rho) = Tr[R(g)\\rho R^{\\dagger}(g)\\tilde{O}_{\\theta}] = Tr[\\rho R^{\\dagger}(g)\\tilde{O}_{\\theta}R(g)] &= Tr[\\rho R^{\\dagger}(g)R(g)\\tilde{O}_{\\theta}]\\\\\n    &= Tr[\\rho \\tilde{O}_{\\theta}]\\\\\n    &= h_{\\theta}(\\rho).\n\\end{align*}$$\n\nEssentially, we are using the Heisenberg picture, where we apply the time evolution to the measurement operator instead of the initial quantum state. When the observable is included in the commutant of $\\mathcal{G}$, we can see how invariance is achieved.\n\nThe challenge with this approach is that it only works for finite-dimensional and compact groups, like $p4m$, $SO(3)$, etc. The Lorentz group is known to be continuous and non-compact, so it has no finite-dimensional unitary representation. Hence, the approach above is of no use for us. Hopefully, there is another way: instead of baking equivariance directly into the ansatze, we'll do it in the feature space and in the message passing function. When the input is invariant, the message passing becomes equivariant.\n\nSimilarly to LorentzNet, for standard jet tagging approach, our input is made of $4$-momentum vectors and any associated particle scalar one may wish to include, like color and charge. In fact, in this project, we start with the traditional LorentzNet architecture, but two modifications are made: first, the invariant metric can be extracted from the machine learned algebra; secondly, the $\\phi_e, \\phi_x, \\phi_h$ and $\\phi_m$ - classical parts modeled as classical multilayer perceptrons in Lorentznet, are now substituted by quantum parameterized circuits. Below we show how invariance-equivariance is preserved under this modification.\n\n## Infrared safe observables\nAnother interesting bias to incorporate is the infrared and collinear (IRC) safety. An infrared and collinear safe observable is the same in the presence or absence of soft or collinear particles. In [6], an IRC-safe equivariant (classical) GNN was proposed for tagging simulated semi-visible jets from Hidden Valley models, showing superior performance on this data for Beyond the Standard Model (BSM) search.\n\nWe saw before that in LorentzNet, the message is calculated as:\n\n\\begin{equation}\nm_{ij}^{l} = \\phi_{e}(h_i, h_j, \\psi(||x_{i}^{l} - x_{j}^{l}||^2), \\psi(\\langle x_{i}, x_{j}\\rangle)),\n\\end{equation}\n\nNow, intuitively, an IRC-safe model should give us a graph that stays invariant under any particle corresponding to an infinitesimal emission, or a collinear one. This means that such particles have no influence on other particles in our point cloud. But, how can we do this? Message passing!\n\n\\begin{align}\n\\text{IR safety}:& m^{l}(i,j) \\rightarrow 0 \\text{ as } z \\rightarrow 0,\\\\\n\\text{C safety}:& m^{l}(i,j + r) = m^{l}(i,j) + m^{l}(i,r) \\text{ as } \\Delta_{jr} \\rightarrow 0,\n\\end{align}\n\nTo ensure IR safety, we can not use $z_j$ directly, as it breaks equivariance. We propose, thus, the following substitution:\n\n\\begin{equation}\nm_{ij}^{l} = \\frac{\\langle x_i , x_j\\rangle}{\\sum_{k \\in \\mathcal{N(j)} } \\langle x_i , x_k\\rangle } \\cdot \\phi_{e}(h_i, h_j, \\psi(||x_{i}^{l} - x_{j}^{l}||^2), \\psi(\\langle x_{i}, x_{j}\\rangle)),\n\\end{equation}\n\n\nWhere $\\langle \\cdot,\\cdot\\rangle$ is the Minkowski inner product, and $\\mathcal{N(j)}$ represents all neighboring particles of $j$. If $j$ is a soft particle, then the Minkowski inner product should be small, thus , which makes the edge connection irrelevant, thus ensuring IR safety. Also, any Lorentz transformation preserves the inner product, so the message should remain symmetry-preserving.\n\n\n## 4.1. Lorentz Equivariant Quantum Block (LEQB)\n\nLEQB is the main piece of our model. We aim to fundamentally learn deeper quantum representations of $|\\psi_{x}^{l+1}\\rangle$ and $|\\psi_h^{l+1} \\rangle$ from $|\\psi_{x}^{l} \\rangle$ and $|\\psi_{h}^{l}\\rangle$, where:\n\n$$\\begin{align}\n    |\\psi_{x}^{l+1}\\rangle &= \\mathcal{U}_{x^{l+1}}({x}^{l})|0\\rangle,\\\\\n    |\\psi_{h}^{l+1}\\rangle &= \\mathcal{U}_{h^{l+1}}({h}^{l})|0\\rangle,\n\\end{align}$$\n\nwhere $\\mathcal{U_{x^{l}}}, \\mathcal{U_{x^{l+1}}}, \\mathcal{U_{h^{l}}}, \\mathcal{U_{h^{l+1}}}$ are all parameterized standard gate unitaries, or variational circuits. Note that $x^{l}$ are the observables and $h^{l}$ are the particle scalars when $l=0$, but $x^{l} = \\langle \\psi_x | \\mathcal{M} | \\psi_x\\rangle$ and $h^{l} = \\langle \\psi_h | \\mathcal{M} | \\psi_h\\rangle$ for $l > 0$, where $\\mathcal{M}$ is some measurement operator.\n\n## 4.2. Theoretical analysis\n\nLet's start with the following proposition:\n\n> The coordinate embedding $x^{l} = \\{x_1^{l} , x_2^{l} , \\dots , x_n^{l}\\}$ is Lorentz group equivariant and the node embedding $h^{l} = \\{h_1^{l} , h_2^{l}, \\dots , h_n^{l}\\}$ - representing the particle scalars - is Lorentz group invariant.\n\nTo prove it, let $Q$ be some Lie group transformation. If the message $m_{ij}^{l}$ is invariant under the action of $Q$ for all $i,j,l,$ then $x_{i}^{l}$ is naturally Lie group equivariant since:\n\n$$\\begin{align*}\n    Q\\cdot x_i^{l+1} &= Q(x_i^{l} + \\sum_{j\\in \\mathcal{N}(i)} x_j^{l}\\cdot \\phi_x (m_{ij}^{l}))\\\\\n    &= Q\\cdot x_i^{l} + \\sum_{j\\in \\mathcal{N}(i)} Q\\cdot x_j^{l}\\cdot \\phi_x (m_{ij}^{l}),\n\\end{align*}$$\n\nwhere $Q$ acts under matrix multiplication. The equation above means that acting with $Q$ from the outside is the same as acting with $Q$ from the inside - directly into the node embeddings from the layer before. Then, for the invariance of $m_{ij}^{l}$, since the norm induced by the extracted metric is invariant under the action of $Q$, it holds that $\\|\\|x_{i}^{0} - x_{j}^{0}\\|\\|^2 = \\|\\|Q\\cdot x_{i}^{0} - Q\\cdot x_{j}^{0}\\|\\|^2$, and $\\langle x_{i}^{0}, x_{j}^{0} \\rangle = \\langle Q\\cdot x_{i}^{0}, Q\\cdot x_{j}^{0} \\rangle$. Since $m_{ij}^{l+1} = \\phi_e(h_i^{l}, h_j^{l}, \\|\\|x_{i}^{l} - x_{j}^{l}\\|\\|^2, \\langle x_{i}^{l}, x_{j}^{l} \\rangle)$, and the norm and the inner product are already invariant, we just have to show that $h^{l}$ is also invariant, since:\n\n$$\\begin{equation*}\n    h_i^{l+1} = h_i^{l} + \\phi_h (h_i^{l}, \\sum_{j\\in \\mathcal{N}(i)} w_{ij} m_{ij}^{l}).\n\\end{equation*}$$\n    \nFor layer $l=0$, $h_{i}^{l}$ is already invariant (since it contains information only about the particle scalars). Then, $m_{ij}^{l+1}$ will be invariant, since all of its inputs are also invariant, and we follow the same logic for $x_{i}^{l+1}$. Given that these properties of $x,h,m$ hold for the first layer and the next, we reach the conclusion recursively.\n\nHaving a quick glance at the discussion we had about groups, equivariance, particles and quantum machine learning, we are getting a hint that the marriage between Physics and symmetries is actually deep. Indeed it is! To quote Philip Anderson, who won the 1977 Nobel prize “for their fundamental theoretical investigations of the electronic structure of magnetic and disordered systems”:\n\n> It is only slightly overstating the case to say that physics is the study of symmetry.","metadata":{"id":"gZwcNRHBXLqI"}},{"cell_type":"code","source":"import torch\nimport pennylane as qml\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch_geometric.utils import to_dense_adj\n\nn_qubits = 4\n\ndev = qml.device('default.qubit', wires=n_qubits)\n# dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n\n\ndef H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\ndef RY_RX_layer(weights):\n    \"\"\"Applies a layer of parametrized RY and RX rotations.\"\"\"\n    for i, w in enumerate(weights):\n        qml.RY(w, wires=i)\n        qml.RX(w, wires=i)\n\ndef full_entangling_layer(n_qubits):\n    \"\"\"Applies CNOT gates between all pairs of qubits.\"\"\"\n    for i in range(n_qubits):\n        for j in range(i+1, n_qubits):\n            qml.CNOT(wires=[i, j])\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(nqubits - 1):\n        qml.CRZ(np.pi / 2, wires=[i, i + 1])\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.SWAP(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.SWAP(wires=[i, i + 1])\n\n\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    # RY_layer(q_input_features)\n    qml.AngleEmbedding(features=q_input_features, wires=range(n_qubits), rotation='Z')\n\n    # Sequence of trainable variational layers\n    # for k in range(q_depth):\n    #     entangling_layer(n_qubits)\n    #     RY_RX_layer(q_weights[k])\n    #     # RY_layer(q_weights[k])\n    for k in range(q_depth):\n        if k % 2 == 0:\n            entangling_layer(n_qubits)\n            RY_layer(q_weights[k])\n        else:\n            full_entangling_layer(n_qubits)\n            RY_RX_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\n\nclass DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n        print('n_qubits: ', n_qubits)\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.q_depth = q_depth\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n\n    def forward(self, input_features):\n        \"\"\"\n        Optimized forward pass to reduce runtime.\n        \"\"\"\n\n        # Quantum Embedding (U(X))\n        q_in = torch.tanh(input_features) * np.pi / 2.0\n\n        # Preallocate output tensor\n        batch_size = q_in.shape[0]\n        q_out = torch.zeros(batch_size, self.n_qubits, device=q_in.device)\n\n        # Vectorized execution\n        for i, elem in enumerate(q_in):\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float()\n            q_out[i] = q_out_elem\n\n        return q_out","metadata":{"id":"3T8yKHYCZbPk","execution":{"iopub.status.busy":"2024-10-25T22:11:34.046751Z","iopub.execute_input":"2024-10-25T22:11:34.047871Z","iopub.status.idle":"2024-10-25T22:11:36.275118Z","shell.execute_reply.started":"2024-10-25T22:11:34.047804Z","shell.execute_reply":"2024-10-25T22:11:36.273762Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pennylane as qml\n\n\"\"\"\n    Lie-Equivariant Quantum Block (LEQB).\n\n        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n          that we develop further), once the metric tensor J is found via the equation:\n\n                          L.J + J.(L^T) = 0,\n\n          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group.\n          In the cells below, we can see how the model preserves symmetries (starting with the default Lorentz group),\n          and when we change J to some other metric (Euclidean, for example), Lorentz boosts **break** equivariance, while other\n          transformations preserve it (rotations, for the example shown in the cells below)\n\"\"\"\nclass LEQB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LEQB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        self.dimension_reducer2 = nn.Linear(9, 4) # New linear layer for dimension reduction for phi_h\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2. n_input = n_hidden = 4\n        print('Input size of phi_e: ', n_input)\n        self.include_x = include_x\n\n        \"\"\"\n            phi_e: input size: n_qubits -> output size: n_qubits\n            n_hidden has to be equal to n_input,\n            but this is just considering that this is a simple working example.\n        \"\"\"\n#         self.phi_e = DressedQuantumNet(n_input)\n        self.phi_e = nn.Sequential(\n            nn.Linear(n_input, n_hidden, bias=False),  # n_input * 2 + n_edge_attr\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU())\n\n        n_hidden = n_input # n_input * 2 + n_edge_attr\n#         self.phi_h = nn.Sequential(\n#             nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n#             nn.BatchNorm1d(n_hidden),\n#             nn.ReLU(),\n#             nn.Linear(n_hidden, n_output))\n\n        self.phi_h = DressedQuantumNet(n_hidden)\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n        \n#         self.phi_x = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             layer)\n\n#         self.phi_m = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             nn.Linear(n_hidden, 1),\n#             nn.Sigmoid())\n        \n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        # self.phi_e = nn.Sequential(\n        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n        #     nn.BatchNorm1d(n_hidden),\n        #     nn.ReLU(),\n        #     nn.Linear(n_hidden, n_hidden),\n        #     nn.ReLU())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        out = self.dimension_reducer(out) # extra\n        # print(\"Before embedding to |psi> : \", out)\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.dimension_reducer(out) # extra\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        agg = self.dimension_reducer2(agg) # extra for phi_h\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m):\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            x = self.x_model(x, edges, x_diff, m)\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LieEQGNN(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LEQB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LieEQGNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LEQBs = nn.ModuleList([LEQB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LEQBs[i](h, x, edges, node_attr=scalars)\n\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n        return pred.squeeze(1)","metadata":{"id":"9sBE05_9XLqJ","execution":{"iopub.status.busy":"2024-10-25T22:11:36.277133Z","iopub.execute_input":"2024-10-25T22:11:36.278090Z","iopub.status.idle":"2024-10-25T22:11:36.317699Z","shell.execute_reply.started":"2024-10-25T22:11:36.278031Z","shell.execute_reply":"2024-10-25T22:11:36.316141Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 7 # 60\n\n    model_path = \"models/LieEQGNN/\"\n    log_path = \"logs/LieEQGNN/\"\n    # utils_lorentz.args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cpu\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cuda\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    model = LieEQGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCLi_VJSZiEE","outputId":"54936a42-d9e0-4fc5-f638-c477eea493f7","scrolled":true,"execution":{"iopub.status.busy":"2024-10-25T22:11:36.319870Z","iopub.execute_input":"2024-10-25T22:11:36.320299Z","iopub.status.idle":"2024-10-26T02:01:47.557116Z","shell.execute_reply.started":"2024-10-25T22:11:36.320257Z","shell.execute_reply":"2024-10-26T02:01:47.555862Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input size of phi_e:  4\nn_qubits:  4\nModel Size: 175\n train samples: 800\n val samples: 100\n test samples: 100\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"50it [30:29, 36.59s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/7 \t Batch 49/50 \t Loss 0.6938 \t Running Acc 0.525 \t Total Acc 0.525 \t Avg Batch Time 36.5897\nTime: train: 1829.49 \t Train loss 0.6938 \t Train acc: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"7it [02:24, 20.64s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7071 \t Running Acc 1.643 \t Total Acc 0.460 \t Avg Batch Time 5.7779\nNew best validation model, saving...\nEpoch 0/7 finished.\nTrain time: 1829.49 \t Val time 144.45\nTrain loss 0.6938 \t Train acc: 0.5250\nVal loss: 0.7078 \t Val acc: 0.4600\nBest val acc: 0.4600 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [30:15, 36.31s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/7 \t Batch 49/50 \t Loss 0.6900 \t Running Acc 0.525 \t Total Acc 0.525 \t Avg Batch Time 36.3126\nTime: train: 1815.63 \t Train loss 0.6900 \t Train acc: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"7it [02:23, 20.53s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7006 \t Running Acc 1.643 \t Total Acc 0.460 \t Avg Batch Time 5.7496\nEpoch 1/7 finished.\nTrain time: 1815.63 \t Val time 143.74\nTrain loss 0.6900 \t Train acc: 0.5250\nVal loss: 0.7006 \t Val acc: 0.4600\nBest val acc: 0.4600 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [29:57, 35.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/7 \t Batch 49/50 \t Loss 0.6823 \t Running Acc 0.526 \t Total Acc 0.526 \t Avg Batch Time 35.9510\nTime: train: 1797.55 \t Train loss 0.6823 \t Train acc: 0.5262\n","output_type":"stream"},{"name":"stderr","text":"7it [02:23, 20.55s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6914 \t Running Acc 1.643 \t Total Acc 0.460 \t Avg Batch Time 5.7553\nEpoch 2/7 finished.\nTrain time: 1797.55 \t Val time 143.88\nTrain loss 0.6823 \t Train acc: 0.5262\nVal loss: 0.6903 \t Val acc: 0.4600\nBest val acc: 0.4600 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [30:04, 36.10s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/7 \t Batch 49/50 \t Loss 0.6730 \t Running Acc 0.610 \t Total Acc 0.610 \t Avg Batch Time 36.0982\nTime: train: 1804.91 \t Train loss 0.6730 \t Train acc: 0.6100\n","output_type":"stream"},{"name":"stderr","text":"7it [02:21, 20.27s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6780 \t Running Acc 2.143 \t Total Acc 0.600 \t Avg Batch Time 5.6744\nNew best validation model, saving...\nEpoch 3/7 finished.\nTrain time: 1804.91 \t Val time 141.86\nTrain loss 0.6730 \t Train acc: 0.6100\nVal loss: 0.6749 \t Val acc: 0.6000\nBest val acc: 0.6000 at epoch 3.\n","output_type":"stream"},{"name":"stderr","text":"50it [30:01, 36.04s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/7 \t Batch 49/50 \t Loss 0.6595 \t Running Acc 0.708 \t Total Acc 0.708 \t Avg Batch Time 36.0386\nTime: train: 1801.93 \t Train loss 0.6595 \t Train acc: 0.7075\n","output_type":"stream"},{"name":"stderr","text":"7it [02:22, 20.40s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6634 \t Running Acc 2.750 \t Total Acc 0.770 \t Avg Batch Time 5.7113\nNew best validation model, saving...\nEpoch 4/7 finished.\nTrain time: 1801.93 \t Val time 142.78\nTrain loss 0.6595 \t Train acc: 0.7075\nVal loss: 0.6576 \t Val acc: 0.7700\nBest val acc: 0.7700 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [30:01, 36.02s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/7 \t Batch 49/50 \t Loss 0.6440 \t Running Acc 0.723 \t Total Acc 0.723 \t Avg Batch Time 36.0219\nTime: train: 1801.10 \t Train loss 0.6440 \t Train acc: 0.7225\n","output_type":"stream"},{"name":"stderr","text":"7it [02:21, 20.18s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6513 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 5.6500\nEpoch 5/7 finished.\nTrain time: 1801.10 \t Val time 141.25\nTrain loss 0.6440 \t Train acc: 0.7225\nVal loss: 0.6429 \t Val acc: 0.7000\nBest val acc: 0.7700 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [30:06, 36.12s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/7 \t Batch 49/50 \t Loss 0.6374 \t Running Acc 0.719 \t Total Acc 0.719 \t Avg Batch Time 36.1234\nTime: train: 1806.17 \t Train loss 0.6374 \t Train acc: 0.7188\n","output_type":"stream"},{"name":"stderr","text":"7it [02:25, 20.81s/it]\n/tmp/ipykernel_30/813438993.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6462 \t Running Acc 2.571 \t Total Acc 0.720 \t Avg Batch Time 5.8282\nEpoch 6/7 finished.\nTrain time: 1806.17 \t Val time 145.71\nTrain loss 0.6374 \t Train acc: 0.7188\nVal loss: 0.6364 \t Val acc: 0.7200\nBest val acc: 0.7700 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"7it [02:22, 20.32s/it]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.6578 \t Running Acc 2.643 \t Total Acc 0.740 \t Avg Batch Time 5.6903\nFinal  tensor([[1.0000, 0.4694, 0.5306],\n        [1.0000, 0.4544, 0.5456],\n        [0.0000, 0.5374, 0.4626],\n        [1.0000, 0.5105, 0.4895],\n        [1.0000, 0.4691, 0.5309],\n        [0.0000, 0.5211, 0.4789],\n        [0.0000, 0.5316, 0.4684],\n        [1.0000, 0.5077, 0.4923],\n        [1.0000, 0.4692, 0.5308],\n        [0.0000, 0.5031, 0.4969],\n        [0.0000, 0.5121, 0.4879],\n        [0.0000, 0.5324, 0.4676],\n        [0.0000, 0.4891, 0.5109],\n        [0.0000, 0.4977, 0.5023],\n        [0.0000, 0.5266, 0.4734],\n        [0.0000, 0.5010, 0.4990],\n        [0.0000, 0.5178, 0.4822],\n        [1.0000, 0.4933, 0.5067],\n        [0.0000, 0.5447, 0.4553],\n        [1.0000, 0.4586, 0.5414],\n        [1.0000, 0.4948, 0.5052],\n        [0.0000, 0.5033, 0.4967],\n        [0.0000, 0.4568, 0.5432],\n        [0.0000, 0.4942, 0.5058],\n        [1.0000, 0.5280, 0.4720],\n        [0.0000, 0.5739, 0.4261],\n        [0.0000, 0.5670, 0.4330],\n        [0.0000, 0.5591, 0.4409],\n        [0.0000, 0.4900, 0.5100],\n        [1.0000, 0.5074, 0.4926],\n        [0.0000, 0.4794, 0.5206],\n        [1.0000, 0.4477, 0.5523],\n        [1.0000, 0.4716, 0.5284],\n        [1.0000, 0.4726, 0.5274],\n        [0.0000, 0.5320, 0.4680],\n        [1.0000, 0.4491, 0.5509],\n        [1.0000, 0.4709, 0.5291],\n        [0.0000, 0.5042, 0.4958],\n        [1.0000, 0.4766, 0.5234],\n        [0.0000, 0.5245, 0.4755],\n        [0.0000, 0.5446, 0.4554],\n        [0.0000, 0.5614, 0.4386],\n        [1.0000, 0.4751, 0.5249],\n        [0.0000, 0.5231, 0.4769],\n        [1.0000, 0.5055, 0.4945],\n        [1.0000, 0.4870, 0.5130],\n        [1.0000, 0.4931, 0.5069],\n        [0.0000, 0.5280, 0.4720],\n        [1.0000, 0.4761, 0.5239],\n        [0.0000, 0.5062, 0.4938],\n        [1.0000, 0.4824, 0.5176],\n        [1.0000, 0.4867, 0.5133],\n        [1.0000, 0.5021, 0.4979],\n        [0.0000, 0.4981, 0.5019],\n        [1.0000, 0.4812, 0.5188],\n        [1.0000, 0.4715, 0.5285],\n        [0.0000, 0.5068, 0.4932],\n        [0.0000, 0.4649, 0.5351],\n        [0.0000, 0.5158, 0.4842],\n        [0.0000, 0.5141, 0.4859],\n        [1.0000, 0.4750, 0.5250],\n        [0.0000, 0.5129, 0.4871],\n        [0.0000, 0.5101, 0.4899],\n        [1.0000, 0.4489, 0.5511],\n        [1.0000, 0.4859, 0.5141],\n        [0.0000, 0.5221, 0.4779],\n        [1.0000, 0.4490, 0.5510],\n        [0.0000, 0.5210, 0.4790],\n        [0.0000, 0.5524, 0.4476],\n        [1.0000, 0.4804, 0.5196],\n        [1.0000, 0.5361, 0.4639],\n        [0.0000, 0.5491, 0.4509],\n        [0.0000, 0.4626, 0.5374],\n        [1.0000, 0.4803, 0.5197],\n        [0.0000, 0.5639, 0.4361],\n        [1.0000, 0.4584, 0.5416],\n        [0.0000, 0.5489, 0.4511],\n        [1.0000, 0.4457, 0.5543],\n        [0.0000, 0.4974, 0.5026],\n        [0.0000, 0.5087, 0.4913],\n        [0.0000, 0.4771, 0.5229],\n        [0.0000, 0.4889, 0.5111],\n        [1.0000, 0.5004, 0.4996],\n        [0.0000, 0.4727, 0.5273],\n        [0.0000, 0.5278, 0.4722],\n        [0.0000, 0.4756, 0.5244],\n        [0.0000, 0.5211, 0.4789],\n        [0.0000, 0.5849, 0.4151],\n        [1.0000, 0.5044, 0.4956],\n        [1.0000, 0.4448, 0.5552],\n        [1.0000, 0.5252, 0.4748],\n        [0.0000, 0.5660, 0.4340],\n        [1.0000, 0.4445, 0.5555],\n        [1.0000, 0.4779, 0.5221],\n        [1.0000, 0.4655, 0.5345],\n        [1.0000, 0.4410, 0.5590],\n        [0.0000, 0.4872, 0.5128],\n        [0.0000, 0.5355, 0.4645],\n        [1.0000, 0.5394, 0.4606],\n        [0.0000, 0.5832, 0.4168]])\nTest: Loss 0.6570 \t Acc 0.7400 \t AUC: 0.8303 \t 1/eB 0.3: 55.0000 \t 1/eB 0.5: 11.0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}