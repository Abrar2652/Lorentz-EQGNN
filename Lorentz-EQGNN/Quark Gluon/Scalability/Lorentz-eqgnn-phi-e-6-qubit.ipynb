{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN)","metadata":{"id":"rB_xvk_TXLpz"}},{"cell_type":"code","source":"# For Colab\n!pip install torch_geometric\n# !pip install torch_sparse\n# !pip install torch_scatter","metadata":{"id":"1qx2qWQoXLp2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15e852ad-282a-4d36-e400-53399d904f45","scrolled":true,"execution":{"iopub.status.busy":"2024-12-30T07:41:17.596311Z","iopub.execute_input":"2024-12-30T07:41:17.596770Z","iopub.status.idle":"2024-12-30T07:41:30.560803Z","shell.execute_reply.started":"2024-12-30T07:41:17.596721Z","shell.execute_reply":"2024-12-30T07:41:30.559555Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pennylane qiskit pennylane-qiskit pylatexenc","metadata":{"id":"_CF_l60hp0xJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb90877-280c-44bc-a443-4bb69afffde4","scrolled":true,"execution":{"iopub.status.busy":"2024-12-30T07:41:30.563142Z","iopub.execute_input":"2024-12-30T07:41:30.563530Z","iopub.status.idle":"2024-12-30T07:41:54.433021Z","shell.execute_reply.started":"2024-12-30T07:41:30.563496Z","shell.execute_reply":"2024-12-30T07:41:54.431737Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.39.0-py3-none-any.whl.metadata (9.2 kB)\nCollecting qiskit\n  Downloading qiskit-1.3.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.39.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.1 in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.14.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.3)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting autograd (from pennylane)\n  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nCollecting pennylane-lightning>=0.39 (from pennylane)\n  Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane) (21.3)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.12)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (0.3.8)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from qiskit) (2.9.0.post0)\nCollecting stevedore>=3.0.0 (from qiskit)\n  Downloading stevedore-5.4.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting symengine<0.14,>=0.11 (from qiskit)\n  Downloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting qiskit\n  Downloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting qiskit-aer (from pennylane-qiskit)\n  Downloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nCollecting qiskit-ibm-runtime<=0.29 (from pennylane-qiskit)\n  Downloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl.metadata (19 kB)\nCollecting qiskit-ibm-provider (from pennylane-qiskit)\n  Downloading qiskit_ibm_provider-0.11.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\nCollecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.26.18)\nRequirement already satisfied: websocket-client>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.8.0)\nCollecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_platform_services-0.59.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.9.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.8.30)\nCollecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane) (3.1.2)\nRequirement already satisfied: psutil>=5 in /opt/conda/lib/python3.10/site-packages (from qiskit-aer->pennylane-qiskit) (5.9.3)\nRequirement already satisfied: websockets>=10.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (12.0)\nCollecting ibm-cloud-sdk-core<4.0.0,>=3.22.0 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.23.4)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (42.0.8)\nCollecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading pyspnego-0.11.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.16.0)\nCollecting urllib3>=1.21.1 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.22.0->ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.8.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.22)\nDownloading PennyLane-0.39.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading PennyLane_qiskit-0.39.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading stevedore-5.4.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_provider-0.11.0-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ibm_platform_services-0.59.0-py3-none-any.whl (340 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.8/340.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\nDownloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pylatexenc\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=4e60ca8bf8125298da498b66608603eb6fc636572981ffefafdbcf7d9880ff1f\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\nSuccessfully built pylatexenc\nInstalling collected packages: pylatexenc, urllib3, symengine, rustworkx, pbr, autoray, autograd, stevedore, qiskit, pyspnego, ibm-cloud-sdk-core, requests-ntlm, qiskit-aer, ibm-platform-services, qiskit-ibm-runtime, qiskit-ibm-provider, pennylane-lightning, pennylane, pennylane-qiskit\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.3.0 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autograd-1.7.0 autoray-0.7.0 ibm-cloud-sdk-core-3.22.0 ibm-platform-services-0.59.0 pbr-6.1.0 pennylane-0.39.0 pennylane-lightning-0.39.0 pennylane-qiskit-0.39.1 pylatexenc-2.10 pyspnego-0.11.2 qiskit-1.2.4 qiskit-aer-0.15.1 qiskit-ibm-provider-0.11.0 qiskit-ibm-runtime-0.29.0 requests-ntlm-1.3.0 rustworkx-0.15.1 stevedore-5.4.0 symengine-0.13.0 urllib3-2.2.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pennylane as qml\nimport qiskit\nprint(qml.__version__)\nprint(qiskit.__version__)\nimport pennylane_qiskit\nprint(pennylane_qiskit.__version__)\nimport pennylane as qml\nfrom pennylane import numpy as np\n# from pennylane_qiskit import AerDevice","metadata":{"id":"wITHoRhbp1XM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9f8012a-a11d-4474-fd25-001a585a7b46","execution":{"iopub.status.busy":"2024-12-30T07:41:54.434850Z","iopub.execute_input":"2024-12-30T07:41:54.435344Z","iopub.status.idle":"2024-12-30T07:41:58.343608Z","shell.execute_reply.started":"2024-12-30T07:41:54.435291Z","shell.execute_reply":"2024-12-30T07:41:58.342378Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0.39.0\n1.2.4\n0.39.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install energyflow","metadata":{"id":"VqjY-j8Njo0M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cb36ad8-2f2f-45d3-f9d2-2492b2f455f7","scrolled":true,"execution":{"iopub.status.busy":"2024-12-30T07:41:58.346052Z","iopub.execute_input":"2024-12-30T07:41:58.346707Z","iopub.status.idle":"2024-12-30T07:42:09.890579Z","shell.execute_reply.started":"2024-12-30T07:41:58.346667Z","shell.execute_reply":"2024-12-30T07:42:09.889234Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting energyflow\n  Downloading energyflow-1.4.0-py3-none-any.whl.metadata (5.6 kB)\nCollecting h5py!=3.11.0,>=2.9.0 (from energyflow)\n  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from energyflow) (1.26.4)\nCollecting wasserstein>=1.0.1 (from energyflow)\n  Downloading wasserstein-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: wurlitzer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wasserstein>=1.0.1->energyflow) (3.1.1)\nDownloading energyflow-1.4.0-py3-none-any.whl (700 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.8/700.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading wasserstein-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (502 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.2/502.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wasserstein, h5py, energyflow\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.11.0\n    Uninstalling h5py-3.11.0:\n      Successfully uninstalled h5py-3.11.0\nSuccessfully installed energyflow-1.4.0 h5py-3.12.1 wasserstein-1.1.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport energyflow\nfrom scipy.sparse import coo_matrix\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data.distributed import DistributedSampler\n\n\n# we define a function to return an adjacencyy matrix\n# for our graph data representing the jets.\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    # print(edge_mask[0])\n    # raise\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask *= diag_mask\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edge_mask, edges]\n\ndef retrieve_dataloaders(batch_size, num_data = -1, use_one_hot = False, cache_dir = './data', num_workers=4):\n    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n                            with_bc=False, cache_dir=cache_dir)\n    splits = ['train', 'val', 'test']\n    data = {type:{'raw':None,'label':None} for type in splits}\n    (data['train']['raw'],  data['val']['raw'],   data['test']['raw'],\n    data['train']['label'], data['val']['label'], data['test']['label']) = \\\n        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle = False)\n\n    enc = OneHotEncoder(handle_unknown='ignore').fit([[11],[13],[22],[130],[211],[321],[2112],[2212]])\n\n    for split, value in data.items():\n        pid = torch.from_numpy(np.abs(np.asarray(value['raw'][...,3], dtype=int))).unsqueeze(-1)\n        p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(value['raw'],error_on_unknown=True))\n        one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n        # one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n        one_hot = torch.from_numpy(one_hot)\n        mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n        charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n\n        if use_one_hot:\n            nodes = one_hot\n\n        # else:\n        #     nodes = torch.cat((mass,charge),dim=-1)\n\n        #     nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n\n        else:\n              # Concatenate mass and charge along the last dimension\n              concatenated = torch.cat((mass, charge), dim=-1)  # Shape (batch_size, n_nodes, 2)\n\n              # Reduce along the last dimension (e.g., by summing or averaging)\n              nodes = concatenated.sum(dim=-1, keepdim=True)  # Shape (batch_size, n_nodes, 1)\n\n              # Apply log-sign transformation if needed\n              nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n        atom_mask = (pid[...,0] != 0)\n\n        value['p4s'] = p4s\n        value['nodes'] = nodes\n        value['label'] = torch.from_numpy(value['label'])\n        value['atom_mask'] = atom_mask.to(torch.bool)\n\n        if split == 'train':\n            print(value['atom_mask'])\n\n    datasets = {split: TensorDataset(value['label'], value['p4s'],\n                                     value['nodes'], value['atom_mask'])\n                for split, value in data.items()}\n\n    # distributed training\n    # train_sampler = DistributedSampler(datasets['train'], shuffle=True)\n    # Construct PyTorch dataloaders from datasets\n    dataloaders = {split: DataLoader(dataset,\n                                     batch_size=batch_size,\n                                     # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n                                     pin_memory=False,\n                                     # persistent_workers=True,\n                                     drop_last=True if (split == 'train') else False,\n                                     num_workers=num_workers,\n                                     collate_fn=collate_fn)\n                        for split, dataset in datasets.items()}\n\n    return dataloaders #train_sampler, dataloaders\n\nif __name__ == '__main__':\n    # train_sampler, dataloaders = retrieve_dataloaders(32, 100)\n    dataloaders = retrieve_dataloaders(batch_size=16, num_data = 20, use_one_hot = True)\n    for (label, p4s, nodes, atom_mask, edge_mask, edges) in dataloaders['train']:\n        print(label.shape, p4s.shape, nodes.shape, atom_mask.shape,\n              edge_mask.shape, edges[0].shape, edges[1].shape)\n        break","metadata":{"id":"kSESzCxAXLp5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"61e98bdd-bd95-4736-ff0a-0e8d0d8b974e","execution":{"iopub.status.busy":"2024-12-30T07:42:09.892339Z","iopub.execute_input":"2024-12-30T07:42:09.892735Z","iopub.status.idle":"2024-12-30T07:42:22.033892Z","shell.execute_reply.started":"2024-12-30T07:42:09.892698Z","shell.execute_reply":"2024-12-30T07:42:22.032447Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading QG_jets.npz from https://zenodo.org/record/3164691/files/QG_jets.npz?download=1 to ./data/datasets\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\ntorch.Size([16]) torch.Size([16, 139, 4]) torch.Size([16, 139, 8]) torch.Size([16, 139]) torch.Size([16, 139, 139]) torch.Size([28736]) torch.Size([28736])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Test the first batch\nfor label, p4s, nodes, atom_mask, edge_mask, edges in dataloaders[\"train\"]:\n    print(f\"Label shape: {label.shape}\")\n    print(f\"4-momenta shape: {p4s.shape}\")\n    print(f\"Node features shape: {nodes.shape}\")\n    print(f\"Atom mask shape: {atom_mask.shape}\")\n    print(f\"Edge mask shape: {edge_mask.shape}\")\n    print(f\"Edge indices shapes: {edges[0].shape}, {edges[1].shape}\")\n    break","metadata":{"id":"5I20CB8IXLp6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5fbcfdb9-80d8-4630-eda4-c2df78a6407d","execution":{"iopub.status.busy":"2024-12-30T07:42:22.035802Z","iopub.execute_input":"2024-12-30T07:42:22.036959Z","iopub.status.idle":"2024-12-30T07:42:22.204644Z","shell.execute_reply.started":"2024-12-30T07:42:22.036913Z","shell.execute_reply":"2024-12-30T07:42:22.203059Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Label shape: torch.Size([16])\n4-momenta shape: torch.Size([16, 139, 4])\nNode features shape: torch.Size([16, 139, 8])\nAtom mask shape: torch.Size([16, 139])\nEdge mask shape: torch.Size([16, 139, 139])\nEdge indices shapes: torch.Size([28736]), torch.Size([28736])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport energyflow\nimport os\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.sparse import coo_matrix\n\ndef save_physics_tensors(num_data=-1, use_one_hot=False, save_dir=\"random/data\"):\n    \"\"\"\n    Generate and save tensor data files needed for physics analysis.\n\n    Args:\n        num_data: Number of data points to generate (-1 for all)\n        save_dir: Directory to save the tensor files\n    \"\"\"\n    # Create save directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load raw data\n    raw = energyflow.qg_jets.load(\n        num_data=num_data,\n        pad=True,\n        ncol=4,\n        generator=\"pythia\",\n        with_bc=False,\n    )\n\n    # Get data and labels\n    data, labels = raw\n\n    # Initialize one-hot encoder for particle IDs\n    enc = OneHotEncoder(handle_unknown=\"ignore\").fit(\n        [[11], [13], [22], [130], [211], [321], [2112], [2212]]\n    )\n\n    # Process data\n    pid = torch.from_numpy(np.abs(np.asarray(data[..., 3], dtype=int))).unsqueeze(-1)\n    p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(data, error_on_unknown=True))\n\n    # Create one-hot encoded nodes\n    one_hot = enc.transform(pid.reshape(-1, 1)).toarray().reshape(pid.shape[:2] + (-1,))\n    nodes = torch.from_numpy(one_hot)\n    mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n    charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n\n    if use_one_hot:\n        nodes = one_hot\n\n    else:\n          # Concatenate mass and charge along the last dimension\n          concatenated = torch.cat((mass, charge), dim=-1)  # Shape (batch_size, n_nodes, 2)\n\n          # Reduce along the last dimension (e.g., by summing or averaging)\n          nodes = concatenated.sum(dim=-1, keepdim=True)  # Shape (batch_size, n_nodes, 1)\n\n          # Apply log-sign transformation if needed\n          nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n\n    # Create masks\n    atom_mask = (pid[..., 0] != 0).to(torch.bool)\n\n    # Create edge mask\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask = edge_mask * diag_mask\n\n    # Convert labels to tensor\n    labels = torch.from_numpy(labels)\n\n    # Calculate edges for the full dataset\n    n_nodes = p4s.size(1)\n    batch_size = p4s.size(0)\n\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx * n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n    edges = np.stack([rows, cols])\n\n    # Save tensors\n    torch.save(p4s, os.path.join(save_dir, \"p4s.pt\"))\n    torch.save(nodes, os.path.join(save_dir, \"nodes.pt\"))\n    torch.save(labels, os.path.join(save_dir, \"labels.pt\"))\n    torch.save(atom_mask, os.path.join(save_dir, \"atom_mask.pt\"))\n    np.save(os.path.join(save_dir, \"edge_mask.npy\"), edge_mask.numpy())\n    np.save(os.path.join(save_dir, \"edges.npy\"), edges)\n\n    print(f\"Saved tensor files to {save_dir}\")\n    print(f\"Shapes:\")\n    print(f\"p4s: {p4s.shape}\")\n    print(f\"nodes: {nodes.shape}\")\n    print(f\"labels: {labels.shape}\")\n    print(f\"atom_mask: {atom_mask.shape}\")\n    print(f\"edge_mask: {edge_mask.shape}\")\n    print(f\"edges: {edges.shape}\")\n\n# Generate and save the tensor files\nsave_physics_tensors(num_data=1000, use_one_hot=False)  # Use same number of data points as before","metadata":{"id":"SKTo7tNemda4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"adae491b-4682-45b9-924c-31b85a79279d","execution":{"iopub.status.busy":"2024-12-30T07:42:22.206643Z","iopub.execute_input":"2024-12-30T07:42:22.207049Z","iopub.status.idle":"2024-12-30T07:43:43.990447Z","shell.execute_reply.started":"2024-12-30T07:42:22.207011Z","shell.execute_reply":"2024-12-30T07:43:43.989051Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading QG_jets.npz from https://zenodo.org/record/3164691/files/QG_jets.npz?download=1 to /root/.energyflow/datasets\nSaved tensor files to random/data\nShapes:\np4s: torch.Size([1000, 139, 4])\nnodes: torch.Size([1000, 139, 1])\nlabels: torch.Size([1000])\natom_mask: torch.Size([1000, 139])\nedge_mask: torch.Size([1000, 139, 139])\nedges: (2, 2145950)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    # edge_mask *= diag_mask\n\n    edge_mask = data[-2]\n\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edges]\n\n\np4s = torch.load('random/data/p4s.pt')\nnodes = torch.load('random/data/nodes.pt')\nlabels = torch.load('random/data/labels.pt')\natom_mask = torch.load('random/data/atom_mask.pt')\nedge_mask = torch.from_numpy(np.load('random/data/edge_mask.npy'))\nedges = torch.from_numpy(np.load('random/data/edges.npy'))\n\n\n# Create a TensorDataset\ndataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n\n# Define the split ratios\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\n# Calculate the lengths for each split\ntotal_size = len(dataset_all)\ntrain_size = int(total_size * train_ratio)\nval_size = int(total_size * val_ratio)\ntest_size = total_size - train_size - val_size  # Ensure all data is used\n\n# Split the dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset_all, [train_size, val_size, test_size])\n\n# Create a dictionary to hold the datasets\ndatasets = {\n    \"train\": train_dataset,\n    \"val\": val_dataset,\n    \"test\": test_dataset\n}\n\ndataloaders = {split: DataLoader(dataset,\n                                 batch_size=16,\n                                 # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n                                 pin_memory=False,\n                                 # persistent_workers=True,\n                                 collate_fn = collate_fn,\n                                 drop_last=True if (split == 'train') else False,\n                                 num_workers=0)\n                    for split, dataset in datasets.items()}","metadata":{"id":"bj-Ig4VZXLp6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"839e7ac0-eeba-4b18-db42-40fcd3d50593","execution":{"iopub.status.busy":"2024-12-30T07:43:43.991787Z","iopub.execute_input":"2024-12-30T07:43:43.992113Z","iopub.status.idle":"2024-12-30T07:43:44.025700Z","shell.execute_reply.started":"2024-12-30T07:43:43.992083Z","shell.execute_reply":"2024-12-30T07:43:44.024437Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1146867464.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  p4s = torch.load('random/data/p4s.pt')\n/tmp/ipykernel_30/1146867464.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  nodes = torch.load('random/data/nodes.pt')\n/tmp/ipykernel_30/1146867464.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  labels = torch.load('random/data/labels.pt')\n/tmp/ipykernel_30/1146867464.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  atom_mask = torch.load('random/data/atom_mask.pt')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# # we can peek at a batch to see what it looks like.\n# next(iter(dataloaders['val']))","metadata":{"id":"dh8IsQXgXLp7","execution":{"iopub.status.busy":"2024-12-30T07:43:44.027316Z","iopub.execute_input":"2024-12-30T07:43:44.027766Z","iopub.status.idle":"2024-12-30T07:43:44.033058Z","shell.execute_reply.started":"2024-12-30T07:43:44.027719Z","shell.execute_reply":"2024-12-30T07:43:44.031891Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(p4s.shape) # p4s\nprint(nodes.shape) # mass\nprint(atom_mask.shape) # torch.ones\nprint(edge_mask.shape) # adj_matrix","metadata":{"id":"d5obJPELXLp7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cdc0c11-df10-4e58-c236-b96977e21166","execution":{"iopub.status.busy":"2024-12-30T07:43:44.037037Z","iopub.execute_input":"2024-12-30T07:43:44.037401Z","iopub.status.idle":"2024-12-30T07:43:44.047523Z","shell.execute_reply.started":"2024-12-30T07:43:44.037369Z","shell.execute_reply":"2024-12-30T07:43:44.046262Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1000, 139, 4])\ntorch.Size([1000, 139, 1])\ntorch.Size([1000, 139])\ntorch.Size([1000, 139, 139])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"dataloaders","metadata":{"id":"01O7t2mkXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7e39edc-c3fd-4d52-bdfd-217df4810f63","execution":{"iopub.status.busy":"2024-12-30T07:43:44.048777Z","iopub.execute_input":"2024-12-30T07:43:44.049153Z","iopub.status.idle":"2024-12-30T07:43:44.068368Z","shell.execute_reply.started":"2024-12-30T07:43:44.049120Z","shell.execute_reply":"2024-12-30T07:43:44.067025Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'train': <torch.utils.data.dataloader.DataLoader at 0x7854a7fe2440>,\n 'val': <torch.utils.data.dataloader.DataLoader at 0x78548b444ca0>,\n 'test': <torch.utils.data.dataloader.DataLoader at 0x78548b446170>}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Set desired dimensions\nbatch_size = 1\nn_nodes = 3\ndevice = 'cpu'\ndtype = torch.float32\n\n# Print initial shapes\nprint(\"Initial shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Select subset of data\np4s = p4s[:batch_size, :n_nodes, :]\natom_mask = atom_mask[:batch_size, :n_nodes]\nedge_mask = edge_mask[:batch_size, :n_nodes, :n_nodes]\nnodes = nodes[:batch_size, :n_nodes, :]\n\nprint(\"\\nAfter selection shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Reshape tensors\natom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n# Don't reshape edge_mask yet\nnodes = nodes.view(batch_size * n_nodes, -1).to(device, dtype)\n\nprint(\"\\nAfter reshape shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)  # original shape\nprint(\"nodes:\", nodes.shape)\n\n# Recalculate edges for the subset\nfrom scipy.sparse import coo_matrix\nrows, cols = [], []\nfor batch_idx in range(batch_size):\n    nn = batch_idx * n_nodes\n    # Convert edge_mask to numpy and remove any extra dimensions\n    edge_mask_np = edge_mask[batch_idx].cpu().numpy().squeeze()\n    x = coo_matrix(edge_mask_np)\n    rows.append(nn + x.row)\n    cols.append(nn + x.col)\n\nedges = [torch.LongTensor(np.concatenate(rows)).to(device),\n         torch.LongTensor(np.concatenate(cols)).to(device)]\n\n# Now reshape edge_mask after edges are calculated\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nprint(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"TEs9qVoYXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d50c2761-09f4-4212-db8a-3854e4c63354","execution":{"iopub.status.busy":"2024-12-30T07:43:44.069949Z","iopub.execute_input":"2024-12-30T07:43:44.070430Z","iopub.status.idle":"2024-12-30T07:43:44.092768Z","shell.execute_reply.started":"2024-12-30T07:43:44.070379Z","shell.execute_reply":"2024-12-30T07:43:44.091531Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Initial shapes:\np4s: torch.Size([1000, 139, 4])\natom_mask: torch.Size([1000, 139])\nedge_mask: torch.Size([1000, 139, 139])\nnodes: torch.Size([1000, 139, 1])\n\nAfter selection shapes:\np4s: torch.Size([1, 3, 4])\natom_mask: torch.Size([1, 3])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([1, 3, 1])\n\nAfter reshape shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([3, 1])\n\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"batch_size = 1  #2500 #1\nn_nodes = 3 #139\ndevice = 'cpu'\ndtype = torch.float32\n\natom_positions = p4s[:, :, :].view(batch_size * n_nodes, -1).to(device, dtype)\n\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nedges = [a.to(device) for a in edges]\nnodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)","metadata":{"id":"dj_AzmjvXLp9","execution":{"iopub.status.busy":"2024-12-30T07:43:44.094089Z","iopub.execute_input":"2024-12-30T07:43:44.094511Z","iopub.status.idle":"2024-12-30T07:43:44.103259Z","shell.execute_reply.started":"2024-12-30T07:43:44.094479Z","shell.execute_reply":"2024-12-30T07:43:44.102147Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"cNq2qPiZKqis","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dfa72c5-27b6-467b-83f8-36e2ad8223ea","execution":{"iopub.status.busy":"2024-12-30T07:43:44.104671Z","iopub.execute_input":"2024-12-30T07:43:44.105089Z","iopub.status.idle":"2024-12-30T07:43:44.120784Z","shell.execute_reply.started":"2024-12-30T07:43:44.105055Z","shell.execute_reply":"2024-12-30T07:43:44.119576Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"atom_mask[0]#.shape","metadata":{"id":"NQhDkqAQXLp9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c503537a-d0bd-4cdf-db50-825746d3eb2d","execution":{"iopub.status.busy":"2024-12-30T07:43:44.122123Z","iopub.execute_input":"2024-12-30T07:43:44.122498Z","iopub.status.idle":"2024-12-30T07:43:44.144361Z","shell.execute_reply.started":"2024-12-30T07:43:44.122465Z","shell.execute_reply":"2024-12-30T07:43:44.143223Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([1.])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"p4s[0][2]","metadata":{"id":"zTcu7crAXLp9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5c5698d-9e54-4c0f-9b24-883312b1485d","execution":{"iopub.status.busy":"2024-12-30T07:43:44.145854Z","iopub.execute_input":"2024-12-30T07:43:44.146390Z","iopub.status.idle":"2024-12-30T07:43:44.154002Z","shell.execute_reply.started":"2024-12-30T07:43:44.146352Z","shell.execute_reply":"2024-12-30T07:43:44.153035Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([ 1.1594, -0.2378, -1.1238, -0.0723], dtype=torch.float64)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"atom_mask.shape","metadata":{"id":"TWPwTZmNXLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33e79a36-7004-4a0d-9eb3-52f1e0646462","execution":{"iopub.status.busy":"2024-12-30T07:43:44.155369Z","iopub.execute_input":"2024-12-30T07:43:44.155708Z","iopub.status.idle":"2024-12-30T07:43:44.164065Z","shell.execute_reply.started":"2024-12-30T07:43:44.155672Z","shell.execute_reply":"2024-12-30T07:43:44.162639Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 1])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"p4s.shape # batch_size (number of jets or graphs), n_nodes (particles), n_features","metadata":{"id":"dfthGhJ3XLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae0d7ad3-8a66-4e80-9aa2-f74f10f7caa4","execution":{"iopub.status.busy":"2024-12-30T07:43:44.165497Z","iopub.execute_input":"2024-12-30T07:43:44.165822Z","iopub.status.idle":"2024-12-30T07:43:44.177599Z","shell.execute_reply.started":"2024-12-30T07:43:44.165780Z","shell.execute_reply":"2024-12-30T07:43:44.176388Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 4])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# random: x(atom_pos), edge_indx_tensor (edges = adj_matrix), edge_tensor (edge_mask = adj_matrix)\nprint(\"Atom mask: {}\".format(atom_mask[:2]))\nprint(\"Atom positions (x features, 4-momenta): {}\".format(atom_positions[:2]))\nprint(\"Nodes (scalars: mass & charge): {}\".format(nodes[:2]))\nprint(\"Edge mask: {}\".format(edge_mask[:2]))\nprint(\"Edges: {}\".format(edges[:2]))","metadata":{"id":"fDPOFJnXXLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb476565-9a69-4f8e-ffa0-90f75e7a6561","execution":{"iopub.status.busy":"2024-12-30T07:43:44.179668Z","iopub.execute_input":"2024-12-30T07:43:44.180237Z","iopub.status.idle":"2024-12-30T07:43:44.195689Z","shell.execute_reply.started":"2024-12-30T07:43:44.180187Z","shell.execute_reply":"2024-12-30T07:43:44.194410Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Atom mask: tensor([[1.],\n        [1.]])\nAtom positions (x features, 4-momenta): tensor([[ 0.2861,  0.0078, -0.2687,  0.0980],\n        [ 0.1653, -0.0258, -0.1580, -0.0414]])\nNodes (scalars: mass & charge): tensor([[-4.7488e-09],\n        [-2.2813e-09]])\nEdge mask: tensor([[False],\n        [ True]])\nEdges: [tensor([0, 0, 1, 1, 2, 2]), tensor([1, 2, 0, 2, 0, 1])]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"edges[:2]#[0].shape","metadata":{"id":"lDz3-q69XLp-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01adc007-56a2-4307-c442-1c4a44f0b14e","execution":{"iopub.status.busy":"2024-12-30T07:43:44.197317Z","iopub.execute_input":"2024-12-30T07:43:44.197672Z","iopub.status.idle":"2024-12-30T07:43:44.207356Z","shell.execute_reply.started":"2024-12-30T07:43:44.197638Z","shell.execute_reply":"2024-12-30T07:43:44.206283Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[tensor([0, 0, 1, 1, 2, 2]), tensor([1, 2, 0, 2, 0, 1])]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n#                          edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"Ss4oXJjGXLp-","execution":{"iopub.status.busy":"2024-12-30T07:43:44.208988Z","iopub.execute_input":"2024-12-30T07:43:44.209474Z","iopub.status.idle":"2024-12-30T07:43:44.220120Z","shell.execute_reply.started":"2024-12-30T07:43:44.209425Z","shell.execute_reply":"2024-12-30T07:43:44.218972Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\n\n\"\"\"Some auxiliary functions\"\"\"\n\ndef unsorted_segment_sum(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    return result\n\ndef unsorted_segment_mean(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    count = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    count.index_add_(0, segment_ids, torch.ones_like(data))\n    return result / count.clamp(min=1)\n\ndef normsq4(p):\n    r''' Minkowski square norm\n         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n    '''\n    psq = torch.pow(p, 2)\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef dotsq4(p,q):\n    r''' Minkowski inner product\n         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n    '''\n    psq = p*q\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef normA_fn(A):\n    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n\ndef dotA_fn(A):\n    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n\ndef psi(p):\n    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n    '''\n    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n\n\n\"\"\"Lorentz Group-Equivariant Block\"\"\"\n\nclass LGEB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LGEB, self).__init__()\n        self.c_weight = c_weight\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        self.dimension_reducer = nn.Linear(n_input * 2 + n_edge_attr, n_input) # New linear layer for dimension reduction\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2.\n        print('Input size of phi_e: ', n_input)\n\n        self.include_x = include_x\n        self.phi_e = nn.Sequential(\n            nn.Linear(n_input, n_hidden, bias=False), # n_input * 2 + n_edge_attr\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU())\n\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        # Reduce the dimension of 'out' to 4 using a linear layer\n        out = self.dimension_reducer(out)\n        out = self.phi_e(out)\n        print(\"m_model output: \", out.shape)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.phi_e(out)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m): # norms\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # print(\"m: \", m.shape)\n        # print(\"trans: \", trans.shape)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        # print(\"trans: \", trans.shape)\n        # print(\"x.size: \", x.size(0))\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            # print(\"X: \", x)\n            x = self.x_model(x, edges, x_diff, m)\n            # print(\"phi_x(X) = \", x, '\\n---\\n')\n\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LorentzNet(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LGEB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LorentzNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n\n        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n        return pred.squeeze(1)","metadata":{"id":"49hLoUYRXLp_","execution":{"iopub.status.busy":"2024-12-30T07:43:44.221656Z","iopub.execute_input":"2024-12-30T07:43:44.222041Z","iopub.status.idle":"2024-12-30T07:43:44.253421Z","shell.execute_reply.started":"2024-12-30T07:43:44.222005Z","shell.execute_reply":"2024-12-30T07:43:44.252102Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\\\n                                    n_node_attr=n_scalar, dropout=dropout,\\\n                                    c_weight=c_weight, last_layer=\\(i==n_layers-1), A=A, include_x=include_x)\n                                    \nWe are using n_hidden = 4 and n_layers = 6\n\nn_input=n_hidden, n_output=n_hidden, n_hidden=n_hidden, n_node_attr=n_scalar=8","metadata":{"id":"yKQ1ZOC2U92Y"}},{"cell_type":"markdown","source":"### Now that we have the official code for the classical, just for sanity checking, let's test for equivariance\n\nThe cell below is just an auxiliary function to give us the boosts","metadata":{"id":"v_Wr8LNBXLp_"}},{"cell_type":"code","source":"# @title\nfrom math import sqrt\nimport numpy as np\n\n# Speed of light (m/s)\nc = 299792458\n\n\"\"\"Lorentz transformations describe the transition between two inertial reference\nframes F and F', each of which is moving in some direction with respect to the\nother. This code only calculates Lorentz transformations for movement in the x\ndirection with no spatial rotation (i.e., a Lorentz boost in the x direction).\nThe Lorentz transformations are calculated here as linear transformations of\nfour-vectors [ct, x, y, z] described by Minkowski space. Note that t (time) is\nmultiplied by c (the speed of light) in the first entry of each four-vector.\n\nThus, if X = [ct; x; y; z] and X' = [ct'; x'; y'; z'] are the four-vectors for\ntwo inertial reference frames and X' moves in the x direction with velocity v\nwith respect to X, then the Lorentz transformation from X to X' is X' = BX,\nwhere\n\n    | γ  -γβ  0  0|\nB = |-γβ  γ   0  0|\n    | 0   0   1  0|\n    | 0   0   0  1|\n\nis the matrix describing the Lorentz boost between X and X',\nγ = 1 / √(1 - v²/c²) is the Lorentz factor, and β = v/c is the velocity as\na fraction of c.\n\"\"\"\n\n\ndef beta(velocity: float) -> float:\n    \"\"\"\n    Calculates β = v/c, the given velocity as a fraction of c\n    >>> beta(c)\n    1.0\n    >>> beta(199792458)\n    0.666435904801848\n    \"\"\"\n    if velocity > c:\n        raise ValueError(\"Speed must not exceed light speed 299,792,458 [m/s]!\")\n    elif velocity < 1:\n        # Usually the speed should be much higher than 1 (c order of magnitude)\n        raise ValueError(\"Speed must be greater than or equal to 1!\")\n\n    return velocity / c\n\n\ndef gamma(velocity: float) -> float:\n    \"\"\"\n    Calculate the Lorentz factor γ = 1 / √(1 - v²/c²) for a given velocity\n    >>> gamma(4)\n    1.0000000000000002\n    >>> gamma(1e5)\n    1.0000000556325075\n    >>> gamma(3e7)\n    1.005044845777813\n    >>> gamma(2.8e8)\n    2.7985595722318277\n    \"\"\"\n    return 1 / sqrt(1 - beta(velocity) ** 2)\n\n\ndef transformation_matrix(velocity: float) -> np.ndarray:\n    \"\"\"\n    Calculate the Lorentz transformation matrix for movement in the x direction:\n\n    | γ  -γβ  0  0|\n    |-γβ  γ   0  0|\n    | 0   0   1  0|\n    | 0   0   0  1|\n\n    where γ is the Lorentz factor and β is the velocity as a fraction of c\n    >>> transformation_matrix(29979245)\n    array([[ 1.00503781, -0.10050378,  0.        ,  0.        ],\n           [-0.10050378,  1.00503781,  0.        ,  0.        ],\n           [ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.        ,  0.        ,  0.        ,  1.        ]])\n    \"\"\"\n    return np.array(\n        [\n            [gamma(velocity), -gamma(velocity) * beta(velocity), 0, 0],\n            [-gamma(velocity) * beta(velocity), gamma(velocity), 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n        ]\n    )\n","metadata":{"cellView":"form","id":"KA_EwMJIXLp_","execution":{"iopub.status.busy":"2024-12-30T07:43:44.254979Z","iopub.execute_input":"2024-12-30T07:43:44.255386Z","iopub.status.idle":"2024-12-30T07:43:44.272398Z","shell.execute_reply.started":"2024-12-30T07:43:44.255351Z","shell.execute_reply":"2024-12-30T07:43:44.270892Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Now, the model","metadata":{"id":"hFsrWKHLXLp_"}},{"cell_type":"code","source":"# n_scalar = 8 in original !\nmodel = LorentzNet(n_scalar = 1, n_hidden = 6, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)","metadata":{"id":"cp9yZmnOXLqA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b6bec5f-a280-4fe6-ed2a-39c8d24b05c7","execution":{"iopub.status.busy":"2024-12-30T07:43:44.273789Z","iopub.execute_input":"2024-12-30T07:43:44.274302Z","iopub.status.idle":"2024-12-30T07:43:44.296335Z","shell.execute_reply.started":"2024-12-30T07:43:44.274254Z","shell.execute_reply":"2024-12-30T07:43:44.295059Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input size of phi_e:  6\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Let's start with a default prediction","metadata":{"id":"hE-OtUjVXLqA"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"zzcmDxZxXLqA","execution":{"iopub.status.busy":"2024-12-30T07:43:44.297682Z","iopub.execute_input":"2024-12-30T07:43:44.298004Z","iopub.status.idle":"2024-12-30T07:43:44.350238Z","shell.execute_reply.started":"2024-12-30T07:43:44.297974Z","shell.execute_reply":"2024-12-30T07:43:44.349120Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"R4k_rlP6XLqF","execution":{"iopub.status.busy":"2024-12-30T07:43:44.351649Z","iopub.execute_input":"2024-12-30T07:43:44.352103Z","iopub.status.idle":"2024-12-30T07:43:44.365094Z","shell.execute_reply.started":"2024-12-30T07:43:44.352052Z","shell.execute_reply":"2024-12-30T07:43:44.363764Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### ... taking any random nonsense transformation in the four-momentum vectors\ni.e.: multiplying by 0.1. Does the hidden rep stay the same?","metadata":{"id":"1WHsdTBxXLqF"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"8Hxk_CYJXLqG","execution":{"iopub.status.busy":"2024-12-30T07:43:44.366466Z","iopub.execute_input":"2024-12-30T07:43:44.366889Z","iopub.status.idle":"2024-12-30T07:43:44.375437Z","shell.execute_reply.started":"2024-12-30T07:43:44.366854Z","shell.execute_reply":"2024-12-30T07:43:44.374261Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"1EiVpL33XLqG","execution":{"iopub.status.busy":"2024-12-30T07:43:44.379950Z","iopub.execute_input":"2024-12-30T07:43:44.380351Z","iopub.status.idle":"2024-12-30T07:43:44.388925Z","shell.execute_reply.started":"2024-12-30T07:43:44.380314Z","shell.execute_reply":"2024-12-30T07:43:44.387857Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Even though the final logits in this case wasn't different, if we look the last output of h (which contains both scalar and 4-momenta information), it changed! Now, what about Lorentz transformations?","metadata":{"id":"70ulhirrXLqH"}},{"cell_type":"code","source":"pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"2IVU9kGJXLqH","execution":{"iopub.status.busy":"2024-12-30T07:43:44.390439Z","iopub.execute_input":"2024-12-30T07:43:44.390822Z","iopub.status.idle":"2024-12-30T07:43:44.408299Z","shell.execute_reply.started":"2024-12-30T07:43:44.390786Z","shell.execute_reply":"2024-12-30T07:43:44.407146Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n                     edge_mask=edge_mask, n_nodes=n_nodes)","metadata":{"id":"U1AUOcTDXLqH","execution":{"iopub.status.busy":"2024-12-30T07:43:44.409626Z","iopub.execute_input":"2024-12-30T07:43:44.409999Z","iopub.status.idle":"2024-12-30T07:43:44.419928Z","shell.execute_reply.started":"2024-12-30T07:43:44.409940Z","shell.execute_reply":"2024-12-30T07:43:44.418600Z"},"trusted":true},"outputs":[{"name":"stdout","text":"m_model output:  torch.Size([6, 6])\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Equivariance works. Finally, let's train on some data","metadata":{"id":"dRYlUAEVXLqI"}},{"cell_type":"code","source":"# @title\nimport torch\nimport os, json, random, string\nimport torch.distributed as dist\n\ndef makedir(path):\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n\ndef args_init(args):\n    r''' Initialize seed and exp_name.\n    '''\n    if args.seed is None: # use random seed if not specified\n        args.seed = np.random.randint(100)\n    if args.exp_name == '': # use random strings if not specified\n        args.exp_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n    if (args.local_rank == 0): # master\n        print(args)\n        makedir(f\"{args.logdir}/{args.exp_name}\")\n        with open(f\"{args.logdir}/{args.exp_name}/args.json\", 'w') as f:\n            json.dump(args.__dict__, f, indent=4)\n\ndef sum_reduce(num, device):\n    r''' Sum the tensor across the devices.\n    '''\n    if not torch.is_tensor(num):\n        rt = torch.tensor(num).to(device)\n    else:\n        rt = num.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    return rt\n\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        warmup_epoch: target learning rate is reached at warmup_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    Reference:\n        https://github.com/ildoonet/pytorch-gradual-warmup-lr\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, warmup_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.warmup_epoch = warmup_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    @property\n    def _warmup_lr(self):\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch + 1) / self.warmup_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * (self.last_epoch + 1) / self.warmup_epoch + 1.) for base_lr in self.base_lrs]\n\n    def get_lr(self):\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        return self._warmup_lr\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        self.last_epoch = self.last_epoch + 1 if epoch==None else epoch\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if not self.finished:\n                warmup_lr = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                    param_group['lr'] = lr\n                self.finished = True\n                return\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.warmup_epoch)\n            return\n\n        for param_group, lr in zip(self.optimizer.param_groups, self._warmup_lr):\n            param_group['lr'] = lr\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.warmup_epoch)\n                self.last_epoch = self.after_scheduler.last_epoch + self.warmup_epoch + 1\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        result = {key: value for key, value in self.__dict__.items() if key != 'optimizer' or key != \"after_scheduler\"}\n        if self.after_scheduler:\n            result.update({\"after_scheduler\": self.after_scheduler.state_dict()})\n        return result\n\n    def load_state_dict(self, state_dict):\n        after_scheduler_state = state_dict.pop(\"after_scheduler\", None)\n        self.__dict__.update(state_dict)\n        if after_scheduler_state:\n            self.after_scheduler.load_state_dict(after_scheduler_state)\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport numpy as np\n\ndef buildROC(labels, score, targetEff=[0.3,0.5]):\n    r''' ROC curve is a plot of the true positive rate (Sensitivity) in the function of the false positive rate\n    (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a\n    sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC\n    curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups.\n    '''\n    if not isinstance(targetEff, list):\n        targetEff = [targetEff]\n    fpr, tpr, threshold = roc_curve(labels, score)\n    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n    eB, eS = fpr[idx], tpr[idx]\n    return fpr, tpr, threshold, eB, eS","metadata":{"cellView":"form","id":"KRBzC37VorM9","execution":{"iopub.status.busy":"2024-12-30T07:43:44.421788Z","iopub.execute_input":"2024-12-30T07:43:44.422297Z","iopub.status.idle":"2024-12-30T07:43:44.553195Z","shell.execute_reply.started":"2024-12-30T07:43:44.422249Z","shell.execute_reply":"2024-12-30T07:43:44.551985Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\n\ndef run(model, epoch, loader, partition, N_EPOCHS=None):\n    if partition == 'train':\n        model.train()\n    else:\n        model.eval()\n\n    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n\n    tik = time.time()\n    loader_length = len(loader)\n\n    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n        if partition == 'train':\n            optimizer.zero_grad()\n\n        batch_size, n_nodes, _ = p4s.size()\n        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n        edges = [a.to(device) for a in edges]\n        label = label.to(device, dtype).long()\n\n        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                         edge_mask=edge_mask, n_nodes=n_nodes)\n\n        predict = pred.max(1).indices\n        correct = torch.sum(predict == label).item()\n        loss = loss_fn(pred, label)\n\n        if partition == 'train':\n            loss.backward()\n            optimizer.step()\n        elif partition == 'test':\n            # save labels and probilities for ROC / AUC\n            # print(\"Preds \", pred)\n            score = torch.nn.functional.softmax(pred, dim = -1)\n            # print(\"Score test \", score)\n            # raise\n            res['label'].append(label)\n            res['score'].append(score)\n\n        res['time'] = time.time() - tik\n        res['correct'] += correct\n        res['loss'] += loss.item() * batch_size\n        res['counter'] += batch_size\n        res['loss_arr'].append(loss.item())\n        res['correct_arr'].append(correct)\n\n        # if i != 0 and i % args.log_interval == 0:\n\n    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n    avg_time = res['time']/res['counter'] * batch_size\n    tmp_counter = res['counter']\n    tmp_loss = res['loss'] / tmp_counter\n    tmp_acc = res['correct'] / tmp_counter\n\n    if N_EPOCHS:\n        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n    else:\n        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, running_loss, running_acc, tmp_acc, avg_time))\n\n    torch.cuda.empty_cache()\n    # ---------- reduce -----------\n    if partition == 'test':\n        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n        res['score'] = torch.cat(res['score'])\n        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n    res['counter'] = res['counter']\n    res['loss'] = res['loss'] / res['counter']\n    res['acc'] = res['correct'] / res['counter']\n    return res\n\ndef train(model, res, N_EPOCHS, model_path, log_path):\n    ### training and validation\n    os.makedirs(model_path, exist_ok=True)\n    os.makedirs(log_path, exist_ok=True)\n\n    for epoch in range(N_EPOCHS):\n        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n        # if epoch % args.val_interval == 0:\n\n        # if (args.local_rank == 0):\n        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n        with torch.no_grad():\n            val_res = run(model, epoch, dataloaders['val'], partition='val')\n\n        # if (args.local_rank == 0): # only master process save\n        res['lr'].append(optimizer.param_groups[0]['lr'])\n        res['train_time'].append(train_res['time'])\n        res['val_time'].append(val_res['time'])\n        res['train_loss'].append(train_res['loss'])\n        res['train_acc'].append(train_res['acc'])\n        res['val_loss'].append(val_res['loss'])\n        res['val_acc'].append(val_res['acc'])\n        res['epochs'].append(epoch)\n\n        ## save best model\n        if val_res['acc'] > res['best_val']:\n            print(\"New best validation model, saving...\")\n            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n            res['best_val'] = val_res['acc']\n            res['best_epoch'] = epoch\n\n        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n\n        json_object = json.dumps(res, indent=4)\n        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n            outfile.write(json_object)\n\n        ## adjust learning rate\n        if (epoch < 31):\n            lr_scheduler.step(metrics=val_res['acc'])\n        else:\n            for g in optimizer.param_groups:\n                g['lr'] = g['lr']*0.5\n\n\ndef test(model, res, model_path, log_path):\n    ### test on best model\n    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n    model.load_state_dict(best_model)\n    with torch.no_grad():\n        test_res = run(model, 0, dataloaders['test'], partition='test')\n\n    print(\"Final \", test_res['score'])\n    pred = test_res['score'].cpu()\n\n    np.save(os.path.join(log_path, \"score.npy\"), pred)\n    fpr, tpr, thres, eB, eS  = buildROC(pred[...,0], pred[...,2])\n    auc = roc_auc_score(pred[...,0], pred[...,2])\n\n    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n    res.update(metric)\n    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n    json_object = json.dumps(res, indent=4)\n    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n        outfile.write(json_object)\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 45 # 60\n\n    model_path = \"models/LorentzNet/\"\n    log_path = \"logs/LorentzNet/\"\n    # args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cuda\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cpu\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    ### create parallel model\n    model = LorentzNet(n_scalar = 1, n_hidden = 6, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"scrolled":true,"id":"8azxxVjtXLqI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3deb62b1-8ab6-43e5-d765-f7dc0e49a783","execution":{"iopub.status.busy":"2024-12-30T07:53:21.484946Z","iopub.execute_input":"2024-12-30T07:53:21.485374Z","iopub.status.idle":"2024-12-30T07:53:45.083586Z","shell.execute_reply.started":"2024-12-30T07:53:21.485338Z","shell.execute_reply":"2024-12-30T07:53:45.082516Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input size of phi_e:  6\nModel Size: 393\n train samples: 800\n val samples: 100\n test samples: 100\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"6it [00:00, 58.44it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\n","output_type":"stream"},{"name":"stderr","text":"18it [00:00, 93.01it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([802, 6])\n","output_type":"stream"},{"name":"stderr","text":"30it [00:00, 105.10it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\n","output_type":"stream"},{"name":"stderr","text":"42it [00:00, 109.66it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([783, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 1/45 \t Batch 49/50 \t Loss 0.7017 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.7017 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 244.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6812 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 0.0012\nNew best validation model, saving...\nEpoch 0/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.7017 \t Train acc: 0.4612\nVal loss: 0.6839 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 112.84it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 109.61it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 110.66it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 2/45 \t Batch 49/50 \t Loss 0.6991 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.6991 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 226.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6782 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 0.0014\nEpoch 1/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6991 \t Train acc: 0.4612\nVal loss: 0.6804 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 111.59it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 109.41it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 110.56it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 109.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 3/45 \t Batch 49/50 \t Loss 0.6912 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 0.0092\nTime: train: 0.46 \t Train loss 0.6912 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 211.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6688 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 0.0014\nEpoch 2/45 finished.\nTrain time: 0.46 \t Val time 0.03\nTrain loss 0.6912 \t Train acc: 0.4612\nVal loss: 0.6709 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.75it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 106.70it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 109.19it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 112.56it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 111.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 4/45 \t Batch 49/50 \t Loss 0.6854 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 0.0090\nTime: train: 0.45 \t Train loss 0.6854 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 234.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6637 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 0.0013\nEpoch 3/45 finished.\nTrain time: 0.45 \t Val time 0.03\nTrain loss 0.6854 \t Train acc: 0.4612\nVal loss: 0.6653 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 111.18it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 110.38it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 109.50it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"47it [00:00, 108.67it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 5/45 \t Batch 49/50 \t Loss 0.6742 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6742 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 230.70it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6535 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 0.0013\nEpoch 4/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6742 \t Train acc: 0.4612\nVal loss: 0.6547 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.19it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 104.04it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 105.82it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 104.47it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 6/45 \t Batch 49/50 \t Loss 0.6692 \t Running Acc 0.537 \t Total Acc 0.537 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6692 \t Train acc: 0.5375\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 228.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6516 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 0.0013\nNew best validation model, saving...\nEpoch 5/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6692 \t Train acc: 0.5375\nVal loss: 0.6521 \t Val acc: 0.6700\nBest val acc: 0.6700 at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 103.97it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"23it [00:00, 107.78it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 104.50it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 107.75it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 7/45 \t Batch 49/50 \t Loss 0.6607 \t Running Acc 0.583 \t Total Acc 0.583 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.6607 \t Train acc: 0.5825\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 232.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6479 \t Running Acc 2.571 \t Total Acc 0.720 \t Avg Batch Time 0.0013\nNew best validation model, saving...\nEpoch 6/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6607 \t Train acc: 0.5825\nVal loss: 0.6482 \t Val acc: 0.7200\nBest val acc: 0.7200 at epoch 6.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 109.69it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 106.63it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 105.59it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 104.78it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 8/45 \t Batch 49/50 \t Loss 0.6608 \t Running Acc 0.598 \t Total Acc 0.598 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6608 \t Train acc: 0.5975\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 234.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6474 \t Running Acc 2.607 \t Total Acc 0.730 \t Avg Batch Time 0.0013\nNew best validation model, saving...\nEpoch 7/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6608 \t Train acc: 0.5975\nVal loss: 0.6476 \t Val acc: 0.7300\nBest val acc: 0.7300 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 104.88it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"23it [00:00, 108.26it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 104.40it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 108.44it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 9/45 \t Batch 49/50 \t Loss 0.6609 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6609 \t Train acc: 0.6038\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 237.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6482 \t Running Acc 2.607 \t Total Acc 0.730 \t Avg Batch Time 0.0013\nEpoch 8/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6609 \t Train acc: 0.6038\nVal loss: 0.6475 \t Val acc: 0.7300\nBest val acc: 0.7300 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 107.52it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 104.76it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 107.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"45it [00:00, 107.60it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/45 \t Batch 49/50 \t Loss 0.6496 \t Running Acc 0.669 \t Total Acc 0.669 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6496 \t Train acc: 0.6687\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 215.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6396 \t Running Acc 2.643 \t Total Acc 0.740 \t Avg Batch Time 0.0014\nNew best validation model, saving...\nEpoch 9/45 finished.\nTrain time: 0.47 \t Val time 0.04\nTrain loss 0.6496 \t Train acc: 0.6687\nVal loss: 0.6386 \t Val acc: 0.7400\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 107.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 107.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.37it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 105.95it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 11/45 \t Batch 49/50 \t Loss 0.6390 \t Running Acc 0.681 \t Total Acc 0.681 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.6390 \t Train acc: 0.6813\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 240.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6374 \t Running Acc 2.571 \t Total Acc 0.720 \t Avg Batch Time 0.0013\nEpoch 10/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6390 \t Train acc: 0.6813\nVal loss: 0.6354 \t Val acc: 0.7200\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 102.90it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 106.54it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 106.91it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 106.76it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 12/45 \t Batch 49/50 \t Loss 0.6403 \t Running Acc 0.682 \t Total Acc 0.682 \t Avg Batch Time 0.0095\nTime: train: 0.47 \t Train loss 0.6403 \t Train acc: 0.6825\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 203.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6337 \t Running Acc 2.536 \t Total Acc 0.710 \t Avg Batch Time 0.0015\nEpoch 11/45 finished.\nTrain time: 0.47 \t Val time 0.04\nTrain loss 0.6403 \t Train acc: 0.6825\nVal loss: 0.6320 \t Val acc: 0.7100\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.22it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 101.32it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 99.60it/s] ","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 103.36it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.02it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/45 \t Batch 49/50 \t Loss 0.6350 \t Running Acc 0.686 \t Total Acc 0.686 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.6350 \t Train acc: 0.6863\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 243.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6329 \t Running Acc 2.536 \t Total Acc 0.710 \t Avg Batch Time 0.0012\nEpoch 12/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6350 \t Train acc: 0.6863\nVal loss: 0.6309 \t Val acc: 0.7100\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 104.02it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 103.35it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.49it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 104.72it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.34it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/45 \t Batch 49/50 \t Loss 0.6378 \t Running Acc 0.677 \t Total Acc 0.677 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6378 \t Train acc: 0.6775\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 218.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6329 \t Running Acc 2.571 \t Total Acc 0.720 \t Avg Batch Time 0.0014\nEpoch 13/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6378 \t Train acc: 0.6775\nVal loss: 0.6308 \t Val acc: 0.7200\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 108.85it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\n","output_type":"stream"},{"name":"stderr","text":"23it [00:00, 109.41it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 108.99it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 110.85it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 109.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 15/45 \t Batch 49/50 \t Loss 0.6323 \t Running Acc 0.690 \t Total Acc 0.690 \t Avg Batch Time 0.0092\nTime: train: 0.46 \t Train loss 0.6323 \t Train acc: 0.6900\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 200.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6324 \t Running Acc 2.607 \t Total Acc 0.730 \t Avg Batch Time 0.0015\nEpoch 14/45 finished.\nTrain time: 0.46 \t Val time 0.04\nTrain loss 0.6323 \t Train acc: 0.6900\nVal loss: 0.6302 \t Val acc: 0.7300\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 115.39it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 107.13it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 109.91it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 16/45 \t Batch 49/50 \t Loss 0.6315 \t Running Acc 0.689 \t Total Acc 0.689 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6315 \t Train acc: 0.6887\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 216.44it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6321 \t Running Acc 2.607 \t Total Acc 0.730 \t Avg Batch Time 0.0014\nEpoch 15/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6315 \t Train acc: 0.6887\nVal loss: 0.6300 \t Val acc: 0.7300\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 105.38it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 108.04it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.85it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 105.56it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.37it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/45 \t Batch 49/50 \t Loss 0.6315 \t Running Acc 0.694 \t Total Acc 0.694 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6315 \t Train acc: 0.6937\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 198.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6233 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0015\nEpoch 16/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.6315 \t Train acc: 0.6937\nVal loss: 0.6208 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.15it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 107.52it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 111.48it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 107.32it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 106.53it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/45 \t Batch 49/50 \t Loss 0.6248 \t Running Acc 0.703 \t Total Acc 0.703 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6248 \t Train acc: 0.7025\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 214.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6241 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 17/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6248 \t Train acc: 0.7025\nVal loss: 0.6208 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 99.84it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 100.98it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 102.27it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 103.68it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/45 \t Batch 49/50 \t Loss 0.6165 \t Running Acc 0.705 \t Total Acc 0.705 \t Avg Batch Time 0.0097\nTime: train: 0.49 \t Train loss 0.6165 \t Train acc: 0.7050\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 244.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6272 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 0.0012\nEpoch 18/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6165 \t Train acc: 0.7050\nVal loss: 0.6225 \t Val acc: 0.6700\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 117.08it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 109.06it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"35it [00:00, 108.97it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 107.39it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 108.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 20/45 \t Batch 49/50 \t Loss 0.6119 \t Running Acc 0.721 \t Total Acc 0.721 \t Avg Batch Time 0.0093\nTime: train: 0.46 \t Train loss 0.6119 \t Train acc: 0.7212\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 205.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6131 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0015\nEpoch 19/45 finished.\nTrain time: 0.46 \t Val time 0.04\nTrain loss 0.6119 \t Train acc: 0.7212\nVal loss: 0.6091 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 105.03it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 105.16it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.88it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 102.29it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.72it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/45 \t Batch 49/50 \t Loss 0.6116 \t Running Acc 0.714 \t Total Acc 0.714 \t Avg Batch Time 0.0099\nTime: train: 0.49 \t Train loss 0.6116 \t Train acc: 0.7137\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 229.45it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6174 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0013\nEpoch 20/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6116 \t Train acc: 0.7137\nVal loss: 0.6125 \t Val acc: 0.6900\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 87.78it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\n","output_type":"stream"},{"name":"stderr","text":"19it [00:00, 88.32it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\n","output_type":"stream"},{"name":"stderr","text":"31it [00:00, 100.55it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\n","output_type":"stream"},{"name":"stderr","text":"42it [00:00, 102.51it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.54it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n>> train \t Epoch 22/45 \t Batch 49/50 \t Loss 0.6136 \t Running Acc 0.714 \t Total Acc 0.714 \t Avg Batch Time 0.0099\nTime: train: 0.49 \t Train loss 0.6136 \t Train acc: 0.7137\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 215.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6211 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0014\nEpoch 21/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6136 \t Train acc: 0.7137\nVal loss: 0.6164 \t Val acc: 0.6900\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 105.00it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 104.25it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 105.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 106.61it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 23/45 \t Batch 49/50 \t Loss 0.6026 \t Running Acc 0.719 \t Total Acc 0.719 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.6026 \t Train acc: 0.7188\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 207.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6153 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0015\nEpoch 22/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.6026 \t Train acc: 0.7188\nVal loss: 0.6099 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 107.12it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 105.35it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 105.34it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 106.21it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.53it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/45 \t Batch 49/50 \t Loss 0.6098 \t Running Acc 0.711 \t Total Acc 0.711 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6098 \t Train acc: 0.7113\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 228.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6186 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0013\nEpoch 23/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6098 \t Train acc: 0.7113\nVal loss: 0.6132 \t Val acc: 0.6900\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 109.31it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 106.77it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 103.39it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 103.99it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 25/45 \t Batch 49/50 \t Loss 0.5995 \t Running Acc 0.718 \t Total Acc 0.718 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.5995 \t Train acc: 0.7175\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 227.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6156 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 0.0013\nEpoch 24/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.5995 \t Train acc: 0.7175\nVal loss: 0.6103 \t Val acc: 0.6900\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 107.95it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 108.83it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 106.66it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 105.86it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.45it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 26/45 \t Batch 49/50 \t Loss 0.6051 \t Running Acc 0.710 \t Total Acc 0.710 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6051 \t Train acc: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 212.74it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6175 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 25/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6051 \t Train acc: 0.7100\nVal loss: 0.6119 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 108.85it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 99.95it/s] ","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 102.56it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 102.14it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 27/45 \t Batch 49/50 \t Loss 0.5912 \t Running Acc 0.721 \t Total Acc 0.721 \t Avg Batch Time 0.0097\nTime: train: 0.49 \t Train loss 0.5912 \t Train acc: 0.7212\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 227.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6131 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 26/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5912 \t Train acc: 0.7212\nVal loss: 0.6076 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 103.32it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 103.83it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.27it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 104.96it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 104.04it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 28/45 \t Batch 49/50 \t Loss 0.5947 \t Running Acc 0.718 \t Total Acc 0.718 \t Avg Batch Time 0.0096\nTime: train: 0.48 \t Train loss 0.5947 \t Train acc: 0.7175\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 224.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6156 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 27/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.5947 \t Train acc: 0.7175\nVal loss: 0.6102 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.02it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 102.20it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.07it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 101.65it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.39it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 29/45 \t Batch 49/50 \t Loss 0.5948 \t Running Acc 0.710 \t Total Acc 0.710 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5948 \t Train acc: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"\n7it [00:00, 241.16it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6133 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 28/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5948 \t Train acc: 0.7100\nVal loss: 0.6079 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 101.71it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 103.86it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 103.82it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 104.22it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.86it/s]","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 30/45 \t Batch 49/50 \t Loss 0.6020 \t Running Acc 0.706 \t Total Acc 0.706 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.6020 \t Train acc: 0.7063\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 223.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6148 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 29/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6020 \t Train acc: 0.7063\nVal loss: 0.6094 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 113.48it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 105.19it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 109.55it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\n","output_type":"stream"},{"name":"stderr","text":"47it [00:00, 106.18it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 107.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 31/45 \t Batch 49/50 \t Loss 0.6015 \t Running Acc 0.709 \t Total Acc 0.709 \t Avg Batch Time 0.0094\nTime: train: 0.47 \t Train loss 0.6015 \t Train acc: 0.7087\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 230.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6145 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 30/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.6015 \t Train acc: 0.7087\nVal loss: 0.6091 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 99.73it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\n","output_type":"stream"},{"name":"stderr","text":"21it [00:00, 101.73it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"32it [00:00, 100.20it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"43it [00:00, 101.28it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 32/45 \t Batch 49/50 \t Loss 0.5973 \t Running Acc 0.710 \t Total Acc 0.710 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5973 \t Train acc: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 209.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6146 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 31/45 finished.\nTrain time: 0.50 \t Val time 0.04\nTrain loss 0.5973 \t Train acc: 0.7100\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 105.77it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 101.55it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 101.80it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 101.73it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.53it/s]","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 33/45 \t Batch 49/50 \t Loss 0.5999 \t Running Acc 0.709 \t Total Acc 0.709 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5999 \t Train acc: 0.7087\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 251.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0012\nEpoch 32/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5999 \t Train acc: 0.7087\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 96.90it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\n","output_type":"stream"},{"name":"stderr","text":"21it [00:00, 103.89it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"32it [00:00, 99.23it/s] ","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 103.15it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.79it/s]","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 34/45 \t Batch 49/50 \t Loss 0.5855 \t Running Acc 0.735 \t Total Acc 0.735 \t Avg Batch Time 0.0099\nTime: train: 0.49 \t Train loss 0.5855 \t Train acc: 0.7350\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 223.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 33/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5855 \t Train acc: 0.7350\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 102.54it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 96.71it/s] ","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 98.87it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 100.30it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 99.83it/s] \n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 35/45 \t Batch 49/50 \t Loss 0.5930 \t Running Acc 0.724 \t Total Acc 0.724 \t Avg Batch Time 0.0101\nTime: train: 0.50 \t Train loss 0.5930 \t Train acc: 0.7238\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 229.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 34/45 finished.\nTrain time: 0.50 \t Val time 0.03\nTrain loss 0.5930 \t Train acc: 0.7238\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.28it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 104.20it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.01it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 101.67it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.70it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 36/45 \t Batch 49/50 \t Loss 0.5939 \t Running Acc 0.724 \t Total Acc 0.724 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.5939 \t Train acc: 0.7238\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 226.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 35/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.5939 \t Train acc: 0.7238\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 106.48it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 105.34it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"34it [00:00, 108.92it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"46it [00:00, 109.38it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 108.34it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 37/45 \t Batch 49/50 \t Loss 0.5944 \t Running Acc 0.716 \t Total Acc 0.716 \t Avg Batch Time 0.0093\nTime: train: 0.46 \t Train loss 0.5944 \t Train acc: 0.7163\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 174.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0017\nEpoch 36/45 finished.\nTrain time: 0.46 \t Val time 0.04\nTrain loss 0.5944 \t Train acc: 0.7163\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 109.07it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 107.04it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 105.65it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 105.88it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.31it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 38/45 \t Batch 49/50 \t Loss 0.6010 \t Running Acc 0.710 \t Total Acc 0.710 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.6010 \t Train acc: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 241.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 37/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.6010 \t Train acc: 0.7100\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 110.01it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 106.43it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\n","output_type":"stream"},{"name":"stderr","text":"35it [00:00, 107.37it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\n","output_type":"stream"},{"name":"stderr","text":"47it [00:00, 108.35it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 107.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 39/45 \t Batch 49/50 \t Loss 0.5865 \t Running Acc 0.733 \t Total Acc 0.733 \t Avg Batch Time 0.0093\nTime: train: 0.47 \t Train loss 0.5865 \t Train acc: 0.7325\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 221.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 38/45 finished.\nTrain time: 0.47 \t Val time 0.03\nTrain loss 0.5865 \t Train acc: 0.7325\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 99.74it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\n","output_type":"stream"},{"name":"stderr","text":"21it [00:00, 103.91it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\n","output_type":"stream"},{"name":"stderr","text":"32it [00:00, 100.58it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\n","output_type":"stream"},{"name":"stderr","text":"43it [00:00, 102.28it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 100.42it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 40/45 \t Batch 49/50 \t Loss 0.5912 \t Running Acc 0.719 \t Total Acc 0.719 \t Avg Batch Time 0.0100\nTime: train: 0.50 \t Train loss 0.5912 \t Train acc: 0.7188\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 213.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 39/45 finished.\nTrain time: 0.50 \t Val time 0.03\nTrain loss 0.5912 \t Train acc: 0.7188\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 89.67it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\n","output_type":"stream"},{"name":"stderr","text":"19it [00:00, 81.46it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\n","output_type":"stream"},{"name":"stderr","text":"29it [00:00, 88.36it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"39it [00:00, 91.42it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 91.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 41/45 \t Batch 49/50 \t Loss 0.5998 \t Running Acc 0.711 \t Total Acc 0.711 \t Avg Batch Time 0.0109\nTime: train: 0.55 \t Train loss 0.5998 \t Train acc: 0.7113\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 213.98it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0014\nEpoch 40/45 finished.\nTrain time: 0.55 \t Val time 0.03\nTrain loss 0.5998 \t Train acc: 0.7113\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 105.73it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 103.66it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 101.39it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 101.62it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 101.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 42/45 \t Batch 49/50 \t Loss 0.5936 \t Running Acc 0.729 \t Total Acc 0.729 \t Avg Batch Time 0.0099\nTime: train: 0.50 \t Train loss 0.5936 \t Train acc: 0.7288\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 220.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 41/45 finished.\nTrain time: 0.50 \t Val time 0.03\nTrain loss 0.5936 \t Train acc: 0.7288\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\n","output_type":"stream"},{"name":"stderr","text":"10it [00:00, 99.21it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\nm_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\n","output_type":"stream"},{"name":"stderr","text":"21it [00:00, 103.06it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\n","output_type":"stream"},{"name":"stderr","text":"32it [00:00, 100.14it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\n","output_type":"stream"},{"name":"stderr","text":"43it [00:00, 102.91it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 102.05it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 43/45 \t Batch 49/50 \t Loss 0.6020 \t Running Acc 0.710 \t Total Acc 0.710 \t Avg Batch Time 0.0098\nTime: train: 0.49 \t Train loss 0.6020 \t Train acc: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 243.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0012\nEpoch 42/45 finished.\nTrain time: 0.49 \t Val time 0.03\nTrain loss 0.6020 \t Train acc: 0.7100\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"12it [00:00, 111.98it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\nm_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"24it [00:00, 104.55it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 106.84it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\n","output_type":"stream"},{"name":"stderr","text":"47it [00:00, 104.71it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 105.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([710, 6])\n>> train \t Epoch 44/45 \t Batch 49/50 \t Loss 0.5962 \t Running Acc 0.719 \t Total Acc 0.719 \t Avg Batch Time 0.0095\nTime: train: 0.48 \t Train loss 0.5962 \t Train acc: 0.7188\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 195.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0015\nEpoch 43/45 finished.\nTrain time: 0.48 \t Val time 0.04\nTrain loss 0.5962 \t Train acc: 0.7188\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([631, 6])\nm_model output:  torch.Size([644, 6])\nm_model output:  torch.Size([717, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([561, 6])\nm_model output:  torch.Size([701, 6])\nm_model output:  torch.Size([549, 6])\nm_model output:  torch.Size([596, 6])\nm_model output:  torch.Size([685, 6])\nm_model output:  torch.Size([776, 6])\nm_model output:  torch.Size([634, 6])\n","output_type":"stream"},{"name":"stderr","text":"11it [00:00, 109.47it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([684, 6])\nm_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([622, 6])\nm_model output:  torch.Size([721, 6])\nm_model output:  torch.Size([620, 6])\nm_model output:  torch.Size([770, 6])\nm_model output:  torch.Size([802, 6])\nm_model output:  torch.Size([595, 6])\nm_model output:  torch.Size([803, 6])\nm_model output:  torch.Size([555, 6])\nm_model output:  torch.Size([794, 6])\n","output_type":"stream"},{"name":"stderr","text":"22it [00:00, 103.44it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([642, 6])\nm_model output:  torch.Size([710, 6])\nm_model output:  torch.Size([646, 6])\nm_model output:  torch.Size([734, 6])\nm_model output:  torch.Size([784, 6])\nm_model output:  torch.Size([801, 6])\nm_model output:  torch.Size([817, 6])\nm_model output:  torch.Size([605, 6])\nm_model output:  torch.Size([653, 6])\nm_model output:  torch.Size([775, 6])\n","output_type":"stream"},{"name":"stderr","text":"33it [00:00, 104.03it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([755, 6])\nm_model output:  torch.Size([474, 6])\nm_model output:  torch.Size([600, 6])\nm_model output:  torch.Size([598, 6])\nm_model output:  torch.Size([877, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([633, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([667, 6])\nm_model output:  torch.Size([783, 6])\nm_model output:  torch.Size([665, 6])\nm_model output:  torch.Size([746, 6])\n","output_type":"stream"},{"name":"stderr","text":"44it [00:00, 103.87it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([611, 6])\nm_model output:  torch.Size([694, 6])\nm_model output:  torch.Size([840, 6])\nm_model output:  torch.Size([678, 6])\nm_model output:  torch.Size([597, 6])\nm_model output:  torch.Size([710, 6])\n","output_type":"stream"},{"name":"stderr","text":"50it [00:00, 103.76it/s]","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 45/45 \t Batch 49/50 \t Loss 0.5866 \t Running Acc 0.738 \t Total Acc 0.738 \t Avg Batch Time 0.0097\nTime: train: 0.48 \t Train loss 0.5866 \t Train acc: 0.7375\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([746, 6])\nm_model output:  torch.Size([754, 6])\nm_model output:  torch.Size([659, 6])\nm_model output:  torch.Size([643, 6])\nm_model output:  torch.Size([692, 6])\nm_model output:  torch.Size([669, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 235.72it/s]\n/tmp/ipykernel_30/586310492.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([264, 6])\n>> val \t Loss 0.6147 \t Running Acc 2.500 \t Total Acc 0.700 \t Avg Batch Time 0.0013\nEpoch 44/45 finished.\nTrain time: 0.48 \t Val time 0.03\nTrain loss 0.5866 \t Train acc: 0.7375\nVal loss: 0.6093 \t Val acc: 0.7000\nBest val acc: 0.7400 at epoch 9.\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([669, 6])\nm_model output:  torch.Size([727, 6])\nm_model output:  torch.Size([596, 6])\n","output_type":"stream"},{"name":"stderr","text":"7it [00:00, 207.91it/s]","output_type":"stream"},{"name":"stdout","text":"m_model output:  torch.Size([782, 6])\nm_model output:  torch.Size([789, 6])\nm_model output:  torch.Size([782, 6])\nm_model output:  torch.Size([132, 6])\n>> test \t Loss 0.6169 \t Running Acc 2.821 \t Total Acc 0.790 \t Avg Batch Time 0.0014\nFinal  tensor([[1.0000, 0.5051, 0.4949],\n        [0.0000, 0.6103, 0.3897],\n        [1.0000, 0.5002, 0.4998],\n        [0.0000, 0.4902, 0.5098],\n        [1.0000, 0.4657, 0.5343],\n        [0.0000, 0.5151, 0.4849],\n        [0.0000, 0.5503, 0.4497],\n        [0.0000, 0.6385, 0.3615],\n        [0.0000, 0.6312, 0.3688],\n        [0.0000, 0.5446, 0.4554],\n        [0.0000, 0.4874, 0.5126],\n        [1.0000, 0.4646, 0.5354],\n        [0.0000, 0.4861, 0.5139],\n        [0.0000, 0.5266, 0.4734],\n        [0.0000, 0.5287, 0.4713],\n        [1.0000, 0.4573, 0.5427],\n        [0.0000, 0.5569, 0.4431],\n        [0.0000, 0.5011, 0.4989],\n        [0.0000, 0.5735, 0.4265],\n        [0.0000, 0.5643, 0.4357],\n        [1.0000, 0.4579, 0.5421],\n        [0.0000, 0.5846, 0.4154],\n        [0.0000, 0.5346, 0.4654],\n        [0.0000, 0.6434, 0.3566],\n        [1.0000, 0.4573, 0.5427],\n        [0.0000, 0.6363, 0.3637],\n        [0.0000, 0.4779, 0.5221],\n        [0.0000, 0.5567, 0.4433],\n        [1.0000, 0.4573, 0.5427],\n        [1.0000, 0.4655, 0.5345],\n        [0.0000, 0.5599, 0.4401],\n        [1.0000, 0.5493, 0.4507],\n        [1.0000, 0.4886, 0.5114],\n        [0.0000, 0.5289, 0.4711],\n        [1.0000, 0.4983, 0.5017],\n        [1.0000, 0.4662, 0.5338],\n        [1.0000, 0.4799, 0.5201],\n        [1.0000, 0.4820, 0.5180],\n        [0.0000, 0.5702, 0.4298],\n        [1.0000, 0.4857, 0.5143],\n        [1.0000, 0.4936, 0.5064],\n        [0.0000, 0.5821, 0.4179],\n        [1.0000, 0.5276, 0.4724],\n        [0.0000, 0.6341, 0.3659],\n        [0.0000, 0.5572, 0.4428],\n        [0.0000, 0.5065, 0.4935],\n        [1.0000, 0.4573, 0.5427],\n        [1.0000, 0.5191, 0.4809],\n        [0.0000, 0.5937, 0.4063],\n        [0.0000, 0.4992, 0.5008],\n        [1.0000, 0.5343, 0.4657],\n        [0.0000, 0.6006, 0.3994],\n        [0.0000, 0.6248, 0.3752],\n        [0.0000, 0.5828, 0.4172],\n        [0.0000, 0.5036, 0.4964],\n        [1.0000, 0.5436, 0.4564],\n        [1.0000, 0.4650, 0.5350],\n        [1.0000, 0.5192, 0.4808],\n        [0.0000, 0.6655, 0.3345],\n        [1.0000, 0.4573, 0.5427],\n        [0.0000, 0.5673, 0.4327],\n        [1.0000, 0.5257, 0.4743],\n        [0.0000, 0.5873, 0.4127],\n        [0.0000, 0.5419, 0.4581],\n        [1.0000, 0.4776, 0.5224],\n        [0.0000, 0.5723, 0.4277],\n        [0.0000, 0.5257, 0.4743],\n        [1.0000, 0.5304, 0.4696],\n        [0.0000, 0.5638, 0.4362],\n        [0.0000, 0.6116, 0.3884],\n        [1.0000, 0.4580, 0.5420],\n        [1.0000, 0.5065, 0.4935],\n        [0.0000, 0.5746, 0.4254],\n        [1.0000, 0.4790, 0.5210],\n        [0.0000, 0.6664, 0.3336],\n        [0.0000, 0.6346, 0.3654],\n        [0.0000, 0.5352, 0.4648],\n        [1.0000, 0.4721, 0.5279],\n        [0.0000, 0.7412, 0.2588],\n        [1.0000, 0.5788, 0.4212],\n        [0.0000, 0.5752, 0.4248],\n        [1.0000, 0.5212, 0.4788],\n        [0.0000, 0.6327, 0.3673],\n        [0.0000, 0.6463, 0.3537],\n        [0.0000, 0.4998, 0.5002],\n        [0.0000, 0.5588, 0.4412],\n        [0.0000, 0.5224, 0.4776],\n        [0.0000, 0.5625, 0.4375],\n        [0.0000, 0.5752, 0.4248],\n        [0.0000, 0.4622, 0.5378],\n        [0.0000, 0.5518, 0.4482],\n        [0.0000, 0.5285, 0.4715],\n        [1.0000, 0.4609, 0.5391],\n        [0.0000, 0.5036, 0.4964],\n        [0.0000, 0.5492, 0.4508],\n        [0.0000, 0.6742, 0.3258],\n        [1.0000, 0.4573, 0.5427],\n        [0.0000, 0.5694, 0.4306],\n        [1.0000, 0.4981, 0.5019],\n        [1.0000, 0.5311, 0.4689]])\nTest: Loss 0.6123 \t Acc 0.7900 \t AUC: 0.8879 \t 1/eB 0.3: inf \t 1/eB 0.5: 31.0000\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_30/586310492.py:155: RuntimeWarning: divide by zero encountered in scalar divide\n  'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n/tmp/ipykernel_30/586310492.py:158: RuntimeWarning: divide by zero encountered in scalar divide\n  % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import torch\nimport pennylane as qml\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch_geometric.utils import to_dense_adj\n\nn_qubits = 6\n\ndev = qml.device('default.qubit', wires=n_qubits)\n# dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n\n\ndef H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\ndef RY_RX_layer(weights):\n    \"\"\"Applies a layer of parametrized RY and RX rotations.\"\"\"\n    for i, w in enumerate(weights):\n        qml.RY(w, wires=i)\n        qml.RX(w, wires=i)\n\ndef full_entangling_layer(n_qubits):\n    \"\"\"Applies CNOT gates between all pairs of qubits.\"\"\"\n    for i in range(n_qubits):\n        for j in range(i+1, n_qubits):\n            qml.CNOT(wires=[i, j])\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(nqubits - 1):\n        qml.CRZ(np.pi / 2, wires=[i, i + 1])\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.SWAP(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.SWAP(wires=[i, i + 1])\n\n\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    # RY_layer(q_input_features)\n    qml.AngleEmbedding(features=q_input_features, wires=range(n_qubits), rotation='Z')\n\n    # Sequence of trainable variational layers\n    # for k in range(q_depth):\n    #     entangling_layer(n_qubits)\n    #     RY_RX_layer(q_weights[k])\n    #     # RY_layer(q_weights[k])\n    for k in range(q_depth):\n        if k % 2 == 0:\n            entangling_layer(n_qubits)\n            RY_layer(q_weights[k])\n        else:\n            full_entangling_layer(n_qubits)\n            RY_RX_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\n\nclass DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n        print('n_qubits: ', n_qubits)\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.q_depth = q_depth\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n\n    def forward(self, input_features):\n        \"\"\"\n        Optimized forward pass to reduce runtime.\n        \"\"\"\n\n        # Quantum Embedding (U(X))\n        q_in = torch.tanh(input_features) * np.pi / 2.0\n\n        # Preallocate output tensor\n        batch_size = q_in.shape[0]\n        q_out = torch.zeros(batch_size, self.n_qubits, device=q_in.device)\n\n        # Vectorized execution\n        for i, elem in enumerate(q_in):\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float()\n            q_out[i] = q_out_elem\n\n        return q_out","metadata":{"id":"3T8yKHYCZbPk","execution":{"iopub.status.busy":"2024-12-30T07:53:45.085992Z","iopub.execute_input":"2024-12-30T07:53:45.086565Z","iopub.status.idle":"2024-12-30T07:53:45.105003Z","shell.execute_reply.started":"2024-12-30T07:53:45.086513Z","shell.execute_reply":"2024-12-30T07:53:45.103823Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pennylane as qml\n\n\"\"\"\n    Lie-Equivariant Quantum Block (LEQB).\n\n        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n          that we develop further), once the metric tensor J is found via the equation:\n\n                          L.J + J.(L^T) = 0,\n\n          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group.\n          In the cells below, we can see how the model preserves symmetries (starting with the default Lorentz group),\n          and when we change J to some other metric (Euclidean, for example), Lorentz boosts **break** equivariance, while other\n          transformations preserve it (rotations, for the example shown in the cells below)\n\"\"\"\nclass LEQB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LEQB, self).__init__()\n        self.c_weight = c_weight\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        self.dimension_reducer = nn.Linear(n_input * 2 + n_edge_attr, n_input) # New linear layer for dimension reduction\n        self.dimension_reducer2 = nn.Linear(n_input * 2 + n_edge_attr - 1, n_input) # New linear layer for dimension reduction for phi_h\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2. n_input = n_hidden = 4\n        print('Input size of phi_e: ', n_input)\n        self.include_x = include_x\n\n        \"\"\"\n            phi_e: input size: n_qubits -> output size: n_qubits\n            n_hidden has to be equal to n_input,\n            but this is just considering that this is a simple working example.\n        \"\"\"\n        self.phi_e = DressedQuantumNet(n_input)\n        # self.phi_e = nn.Sequential(\n        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),  # n_input * 2 + n_edge_attr\n        #     nn.BatchNorm1d(n_hidden),\n        #     nn.ReLU(),\n        #     nn.Linear(n_hidden, n_hidden),\n        #     nn.ReLU())\n\n        n_hidden = n_input # n_input * 2 + n_edge_attr\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n        # self.phi_h = DressedQuantumNet(n_hidden)\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n        \n#         self.phi_x = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             layer)\n\n#         self.phi_m = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             nn.Linear(n_hidden, 1),\n#             nn.Sigmoid())\n        \n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        # self.phi_e = nn.Sequential(\n        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n        #     nn.BatchNorm1d(n_hidden),\n        #     nn.ReLU(),\n        #     nn.Linear(n_hidden, n_hidden),\n        #     nn.ReLU())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        out = self.dimension_reducer(out) # extra\n        # print(\"Before embedding to |psi> : \", out)\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.dimension_reducer(out) # extra\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        # agg = self.dimension_reducer2(agg) # extra for phi_h\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m):\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            x = self.x_model(x, edges, x_diff, m)\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LieEQGNN(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LEQB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LieEQGNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LEQBs = nn.ModuleList([LEQB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LEQBs[i](h, x, edges, node_attr=scalars)\n\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n        return pred.squeeze(1)","metadata":{"id":"9sBE05_9XLqJ","execution":{"iopub.status.busy":"2024-12-30T07:53:45.106454Z","iopub.execute_input":"2024-12-30T07:53:45.106862Z","iopub.status.idle":"2024-12-30T07:53:45.133059Z","shell.execute_reply.started":"2024-12-30T07:53:45.106827Z","shell.execute_reply":"2024-12-30T07:53:45.131852Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS =  25 # 60\n\n    model_path = \"models/LieEQGNN/\"\n    log_path = \"logs/LieEQGNN/\"\n    # utils_lorentz.args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cpu\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cuda\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    model = LieEQGNN(n_scalar = 1, n_hidden = 6, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCLi_VJSZiEE","outputId":"54936a42-d9e0-4fc5-f638-c477eea493f7","scrolled":true,"execution":{"iopub.status.busy":"2024-12-30T07:53:45.135223Z","iopub.execute_input":"2024-12-30T07:53:45.135579Z","iopub.status.idle":"2024-12-30T13:54:54.918022Z","shell.execute_reply.started":"2024-12-30T07:53:45.135545Z","shell.execute_reply":"2024-12-30T13:54:54.916941Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input size of phi_e:  6\nn_qubits:  6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model Size: 393\n train samples: 800\n val samples: 100\n test samples: 100\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"50it [13:37, 16.35s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/25 \t Batch 49/50 \t Loss 0.7124 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 16.3505\nTime: train: 817.53 \t Train loss 0.7124 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6855 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 2.5150\nNew best validation model, saving...\nEpoch 0/25 finished.\nTrain time: 817.53 \t Val time 62.87\nTrain loss 0.7124 \t Train acc: 0.4612\nVal loss: 0.6891 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:31, 16.24s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/25 \t Batch 49/50 \t Loss 0.7082 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 16.2377\nTime: train: 811.89 \t Train loss 0.7082 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.97s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6860 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 2.5123\nEpoch 1/25 finished.\nTrain time: 811.89 \t Val time 62.81\nTrain loss 0.7082 \t Train acc: 0.4612\nVal loss: 0.6889 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:28, 16.18s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/25 \t Batch 49/50 \t Loss 0.7053 \t Running Acc 0.461 \t Total Acc 0.461 \t Avg Batch Time 16.1784\nTime: train: 808.92 \t Train loss 0.7053 \t Train acc: 0.4612\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.89s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6869 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 2.4884\nEpoch 2/25 finished.\nTrain time: 808.92 \t Val time 62.21\nTrain loss 0.7053 \t Train acc: 0.4612\nVal loss: 0.6890 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:20, 16.02s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/25 \t Batch 49/50 \t Loss 0.7002 \t Running Acc 0.456 \t Total Acc 0.456 \t Avg Batch Time 16.0197\nTime: train: 800.98 \t Train loss 0.7002 \t Train acc: 0.4562\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.74s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6887 \t Running Acc 2.071 \t Total Acc 0.580 \t Avg Batch Time 2.4473\nEpoch 3/25 finished.\nTrain time: 800.98 \t Val time 61.18\nTrain loss 0.7002 \t Train acc: 0.4562\nVal loss: 0.6898 \t Val acc: 0.5800\nBest val acc: 0.5800 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:20, 16.02s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/25 \t Batch 49/50 \t Loss 0.6936 \t Running Acc 0.525 \t Total Acc 0.525 \t Avg Batch Time 16.0183\nTime: train: 800.91 \t Train loss 0.6936 \t Train acc: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6899 \t Running Acc 2.536 \t Total Acc 0.710 \t Avg Batch Time 2.4607\nNew best validation model, saving...\nEpoch 4/25 finished.\nTrain time: 800.91 \t Val time 61.52\nTrain loss 0.6936 \t Train acc: 0.5250\nVal loss: 0.6898 \t Val acc: 0.7100\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:22, 16.05s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/25 \t Batch 49/50 \t Loss 0.6877 \t Running Acc 0.581 \t Total Acc 0.581 \t Avg Batch Time 16.0511\nTime: train: 802.55 \t Train loss 0.6877 \t Train acc: 0.5813\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.85s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6900 \t Running Acc 1.964 \t Total Acc 0.550 \t Avg Batch Time 2.4792\nEpoch 5/25 finished.\nTrain time: 802.55 \t Val time 61.98\nTrain loss 0.6877 \t Train acc: 0.5813\nVal loss: 0.6893 \t Val acc: 0.5500\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:24, 16.09s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/25 \t Batch 49/50 \t Loss 0.6834 \t Running Acc 0.605 \t Total Acc 0.605 \t Avg Batch Time 16.0927\nTime: train: 804.63 \t Train loss 0.6834 \t Train acc: 0.6050\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.89s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6913 \t Running Acc 1.750 \t Total Acc 0.490 \t Avg Batch Time 2.4900\nEpoch 6/25 finished.\nTrain time: 804.63 \t Val time 62.25\nTrain loss 0.6834 \t Train acc: 0.6050\nVal loss: 0.6900 \t Val acc: 0.4900\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:27, 16.15s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/25 \t Batch 49/50 \t Loss 0.6817 \t Running Acc 0.608 \t Total Acc 0.608 \t Avg Batch Time 16.1469\nTime: train: 807.34 \t Train loss 0.6817 \t Train acc: 0.6075\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6910 \t Running Acc 1.750 \t Total Acc 0.490 \t Avg Batch Time 2.4616\nEpoch 7/25 finished.\nTrain time: 807.34 \t Val time 61.54\nTrain loss 0.6817 \t Train acc: 0.6075\nVal loss: 0.6896 \t Val acc: 0.4900\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:19, 15.99s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/25 \t Batch 49/50 \t Loss 0.6795 \t Running Acc 0.621 \t Total Acc 0.621 \t Avg Batch Time 15.9947\nTime: train: 799.74 \t Train loss 0.6795 \t Train acc: 0.6212\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.88s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6867 \t Running Acc 1.929 \t Total Acc 0.540 \t Avg Batch Time 2.4870\nEpoch 8/25 finished.\nTrain time: 799.74 \t Val time 62.17\nTrain loss 0.6795 \t Train acc: 0.6212\nVal loss: 0.6848 \t Val acc: 0.5400\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:25, 16.10s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/25 \t Batch 49/50 \t Loss 0.6721 \t Running Acc 0.677 \t Total Acc 0.677 \t Avg Batch Time 16.1002\nTime: train: 805.01 \t Train loss 0.6721 \t Train acc: 0.6775\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.83s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6761 \t Running Acc 2.321 \t Total Acc 0.650 \t Avg Batch Time 2.4716\nEpoch 9/25 finished.\nTrain time: 805.01 \t Val time 61.79\nTrain loss 0.6721 \t Train acc: 0.6775\nVal loss: 0.6740 \t Val acc: 0.6500\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:16, 15.93s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/25 \t Batch 49/50 \t Loss 0.6590 \t Running Acc 0.703 \t Total Acc 0.703 \t Avg Batch Time 15.9311\nTime: train: 796.55 \t Train loss 0.6590 \t Train acc: 0.7025\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.81s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6615 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 2.4672\nEpoch 10/25 finished.\nTrain time: 796.55 \t Val time 61.68\nTrain loss 0.6590 \t Train acc: 0.7025\nVal loss: 0.6589 \t Val acc: 0.6700\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:17, 15.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/25 \t Batch 49/50 \t Loss 0.6461 \t Running Acc 0.715 \t Total Acc 0.715 \t Avg Batch Time 15.9531\nTime: train: 797.66 \t Train loss 0.6461 \t Train acc: 0.7150\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.77s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6592 \t Running Acc 2.321 \t Total Acc 0.650 \t Avg Batch Time 2.4568\nEpoch 11/25 finished.\nTrain time: 797.66 \t Val time 61.42\nTrain loss 0.6461 \t Train acc: 0.7150\nVal loss: 0.6564 \t Val acc: 0.6500\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:12, 15.86s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/25 \t Batch 49/50 \t Loss 0.6362 \t Running Acc 0.711 \t Total Acc 0.711 \t Avg Batch Time 15.8581\nTime: train: 792.91 \t Train loss 0.6362 \t Train acc: 0.7113\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.74s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6528 \t Running Acc 2.286 \t Total Acc 0.640 \t Avg Batch Time 2.4465\nEpoch 12/25 finished.\nTrain time: 792.91 \t Val time 61.16\nTrain loss 0.6362 \t Train acc: 0.7113\nVal loss: 0.6497 \t Val acc: 0.6400\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:19, 15.98s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/25 \t Batch 49/50 \t Loss 0.6319 \t Running Acc 0.699 \t Total Acc 0.699 \t Avg Batch Time 15.9805\nTime: train: 799.02 \t Train loss 0.6319 \t Train acc: 0.6987\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6524 \t Running Acc 2.321 \t Total Acc 0.650 \t Avg Batch Time 2.4615\nEpoch 13/25 finished.\nTrain time: 799.02 \t Val time 61.54\nTrain loss 0.6319 \t Train acc: 0.6987\nVal loss: 0.6495 \t Val acc: 0.6500\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:17, 15.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/25 \t Batch 49/50 \t Loss 0.6265 \t Running Acc 0.721 \t Total Acc 0.721 \t Avg Batch Time 15.9564\nTime: train: 797.82 \t Train loss 0.6265 \t Train acc: 0.7212\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.75s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6451 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 2.4513\nEpoch 14/25 finished.\nTrain time: 797.82 \t Val time 61.28\nTrain loss 0.6265 \t Train acc: 0.7212\nVal loss: 0.6421 \t Val acc: 0.6700\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:18, 15.96s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/25 \t Batch 49/50 \t Loss 0.6266 \t Running Acc 0.698 \t Total Acc 0.698 \t Avg Batch Time 15.9609\nTime: train: 798.04 \t Train loss 0.6266 \t Train acc: 0.6975\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.72s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6444 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 2.4426\nEpoch 15/25 finished.\nTrain time: 798.04 \t Val time 61.07\nTrain loss 0.6266 \t Train acc: 0.6975\nVal loss: 0.6413 \t Val acc: 0.6700\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:16, 15.92s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/25 \t Batch 49/50 \t Loss 0.6161 \t Running Acc 0.731 \t Total Acc 0.731 \t Avg Batch Time 15.9238\nTime: train: 796.19 \t Train loss 0.6161 \t Train acc: 0.7312\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.77s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6519 \t Running Acc 2.250 \t Total Acc 0.630 \t Avg Batch Time 2.4554\nEpoch 16/25 finished.\nTrain time: 796.19 \t Val time 61.38\nTrain loss 0.6161 \t Train acc: 0.7312\nVal loss: 0.6481 \t Val acc: 0.6300\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:16, 15.93s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/25 \t Batch 49/50 \t Loss 0.6074 \t Running Acc 0.711 \t Total Acc 0.711 \t Avg Batch Time 15.9262\nTime: train: 796.31 \t Train loss 0.6074 \t Train acc: 0.7113\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6443 \t Running Acc 2.286 \t Total Acc 0.640 \t Avg Batch Time 2.4623\nEpoch 17/25 finished.\nTrain time: 796.31 \t Val time 61.56\nTrain loss 0.6074 \t Train acc: 0.7113\nVal loss: 0.6401 \t Val acc: 0.6400\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:17, 15.95s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/25 \t Batch 49/50 \t Loss 0.6003 \t Running Acc 0.714 \t Total Acc 0.714 \t Avg Batch Time 15.9480\nTime: train: 797.40 \t Train loss 0.6003 \t Train acc: 0.7137\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6318 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 2.4613\nEpoch 18/25 finished.\nTrain time: 797.40 \t Val time 61.53\nTrain loss 0.6003 \t Train acc: 0.7137\nVal loss: 0.6243 \t Val acc: 0.6700\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:20, 16.00s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/25 \t Batch 49/50 \t Loss 0.5858 \t Running Acc 0.734 \t Total Acc 0.734 \t Avg Batch Time 16.0005\nTime: train: 800.03 \t Train loss 0.5858 \t Train acc: 0.7338\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.83s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6280 \t Running Acc 2.393 \t Total Acc 0.670 \t Avg Batch Time 2.4718\nEpoch 19/25 finished.\nTrain time: 800.03 \t Val time 61.79\nTrain loss 0.5858 \t Train acc: 0.7338\nVal loss: 0.6196 \t Val acc: 0.6700\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:20, 16.01s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/25 \t Batch 49/50 \t Loss 0.5726 \t Running Acc 0.725 \t Total Acc 0.725 \t Avg Batch Time 16.0057\nTime: train: 800.29 \t Train loss 0.5726 \t Train acc: 0.7250\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.78s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6193 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 2.4582\nEpoch 20/25 finished.\nTrain time: 800.29 \t Val time 61.46\nTrain loss 0.5726 \t Train acc: 0.7250\nVal loss: 0.6102 \t Val acc: 0.6900\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:26, 16.13s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/25 \t Batch 49/50 \t Loss 0.5656 \t Running Acc 0.731 \t Total Acc 0.731 \t Avg Batch Time 16.1311\nTime: train: 806.55 \t Train loss 0.5656 \t Train acc: 0.7312\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.85s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6126 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 2.4790\nEpoch 21/25 finished.\nTrain time: 806.55 \t Val time 61.97\nTrain loss 0.5656 \t Train acc: 0.7312\nVal loss: 0.5983 \t Val acc: 0.6900\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:23, 16.07s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/25 \t Batch 49/50 \t Loss 0.5590 \t Running Acc 0.734 \t Total Acc 0.734 \t Avg Batch Time 16.0731\nTime: train: 803.65 \t Train loss 0.5590 \t Train acc: 0.7338\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.86s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6175 \t Running Acc 2.464 \t Total Acc 0.690 \t Avg Batch Time 2.4798\nEpoch 22/25 finished.\nTrain time: 803.65 \t Val time 62.00\nTrain loss 0.5590 \t Train acc: 0.7338\nVal loss: 0.6036 \t Val acc: 0.6900\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:26, 16.12s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/25 \t Batch 49/50 \t Loss 0.5446 \t Running Acc 0.744 \t Total Acc 0.744 \t Avg Batch Time 16.1213\nTime: train: 806.06 \t Train loss 0.5446 \t Train acc: 0.7438\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.76s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6133 \t Running Acc 2.429 \t Total Acc 0.680 \t Avg Batch Time 2.4529\nEpoch 23/25 finished.\nTrain time: 806.06 \t Val time 61.32\nTrain loss 0.5446 \t Train acc: 0.7438\nVal loss: 0.6002 \t Val acc: 0.6800\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"50it [13:20, 16.01s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/25 \t Batch 49/50 \t Loss 0.5524 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 16.0058\nTime: train: 800.29 \t Train loss 0.5524 \t Train acc: 0.7450\n","output_type":"stream"},{"name":"stderr","text":"7it [01:01,  8.82s/it]\n/tmp/ipykernel_30/586310492.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6202 \t Running Acc 2.429 \t Total Acc 0.680 \t Avg Batch Time 2.4694\nEpoch 24/25 finished.\nTrain time: 800.29 \t Val time 61.74\nTrain loss 0.5524 \t Train acc: 0.7450\nVal loss: 0.6086 \t Val acc: 0.6800\nBest val acc: 0.7100 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"7it [01:02,  8.88s/it]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.6886 \t Running Acc 2.857 \t Total Acc 0.800 \t Avg Batch Time 2.4877\nFinal  tensor([[1.0000, 0.4999, 0.5001],\n        [0.0000, 0.5056, 0.4944],\n        [1.0000, 0.5002, 0.4998],\n        [0.0000, 0.4988, 0.5012],\n        [1.0000, 0.4975, 0.5025],\n        [0.0000, 0.5008, 0.4992],\n        [0.0000, 0.5031, 0.4969],\n        [0.0000, 0.5064, 0.4936],\n        [0.0000, 0.5063, 0.4937],\n        [0.0000, 0.5031, 0.4969],\n        [0.0000, 0.5004, 0.4996],\n        [1.0000, 0.4977, 0.5023],\n        [0.0000, 0.4998, 0.5002],\n        [0.0000, 0.5019, 0.4981],\n        [0.0000, 0.5012, 0.4988],\n        [1.0000, 0.4975, 0.5025],\n        [0.0000, 0.5028, 0.4972],\n        [0.0000, 0.5000, 0.5000],\n        [0.0000, 0.5041, 0.4959],\n        [0.0000, 0.5029, 0.4971],\n        [1.0000, 0.4977, 0.5023],\n        [0.0000, 0.5048, 0.4952],\n        [0.0000, 0.5016, 0.4984],\n        [0.0000, 0.5054, 0.4946],\n        [1.0000, 0.4969, 0.5031],\n        [0.0000, 0.5062, 0.4938],\n        [0.0000, 0.4988, 0.5012],\n        [0.0000, 0.5029, 0.4971],\n        [1.0000, 0.4961, 0.5039],\n        [1.0000, 0.4975, 0.5025],\n        [0.0000, 0.5044, 0.4956],\n        [1.0000, 0.5023, 0.4977],\n        [1.0000, 0.4994, 0.5006],\n        [0.0000, 0.5024, 0.4976],\n        [1.0000, 0.5003, 0.4997],\n        [1.0000, 0.4987, 0.5013],\n        [1.0000, 0.4984, 0.5016],\n        [1.0000, 0.4984, 0.5016],\n        [0.0000, 0.5034, 0.4966],\n        [1.0000, 0.4983, 0.5017],\n        [1.0000, 0.4991, 0.5009],\n        [0.0000, 0.5044, 0.4956],\n        [1.0000, 0.5010, 0.4990],\n        [0.0000, 0.5059, 0.4941],\n        [0.0000, 0.5036, 0.4964],\n        [0.0000, 0.4999, 0.5001],\n        [1.0000, 0.4962, 0.5038],\n        [1.0000, 0.5001, 0.4999],\n        [0.0000, 0.5055, 0.4945],\n        [0.0000, 0.4997, 0.5003],\n        [1.0000, 0.5026, 0.4974],\n        [0.0000, 0.5065, 0.4935],\n        [0.0000, 0.5062, 0.4938],\n        [0.0000, 0.5045, 0.4955],\n        [0.0000, 0.4999, 0.5001],\n        [1.0000, 0.5018, 0.4982],\n        [1.0000, 0.4979, 0.5021],\n        [1.0000, 0.5013, 0.4987],\n        [0.0000, 0.5050, 0.4950],\n        [1.0000, 0.4978, 0.5022],\n        [0.0000, 0.5025, 0.4975],\n        [1.0000, 0.5008, 0.4992],\n        [0.0000, 0.5048, 0.4952],\n        [0.0000, 0.5028, 0.4972],\n        [1.0000, 0.4985, 0.5015],\n        [0.0000, 0.5028, 0.4972],\n        [0.0000, 0.5014, 0.4986],\n        [1.0000, 0.5021, 0.4979],\n        [0.0000, 0.5039, 0.4961],\n        [0.0000, 0.5066, 0.4934],\n        [1.0000, 0.4966, 0.5034],\n        [1.0000, 0.5000, 0.5000],\n        [0.0000, 0.5041, 0.4959],\n        [1.0000, 0.4983, 0.5017],\n        [0.0000, 0.5056, 0.4944],\n        [0.0000, 0.5063, 0.4937],\n        [0.0000, 0.5017, 0.4983],\n        [1.0000, 0.4980, 0.5020],\n        [0.0000, 0.5043, 0.4957],\n        [1.0000, 0.5046, 0.4954],\n        [0.0000, 0.5040, 0.4960],\n        [1.0000, 0.5009, 0.4991],\n        [0.0000, 0.5058, 0.4942],\n        [0.0000, 0.5061, 0.4939],\n        [0.0000, 0.5001, 0.4999],\n        [0.0000, 0.5036, 0.4964],\n        [0.0000, 0.5020, 0.4980],\n        [0.0000, 0.5035, 0.4965],\n        [0.0000, 0.5047, 0.4953],\n        [0.0000, 0.4984, 0.5016],\n        [0.0000, 0.5030, 0.4970],\n        [0.0000, 0.5014, 0.4986],\n        [1.0000, 0.4975, 0.5025],\n        [0.0000, 0.5003, 0.4997],\n        [0.0000, 0.5033, 0.4967],\n        [0.0000, 0.5061, 0.4939],\n        [1.0000, 0.4962, 0.5038],\n        [0.0000, 0.5041, 0.4959],\n        [1.0000, 0.4993, 0.5007],\n        [1.0000, 0.5012, 0.4988]])\nTest: Loss 0.6885 \t Acc 0.8000 \t AUC: 0.9011 \t 1/eB 0.3: inf \t 1/eB 0.5: 62.0000\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_30/586310492.py:155: RuntimeWarning: divide by zero encountered in scalar divide\n  'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n/tmp/ipykernel_30/586310492.py:158: RuntimeWarning: divide by zero encountered in scalar divide\n  % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}