{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN)","metadata":{"id":"rB_xvk_TXLpz"}},{"cell_type":"code","source":"# For Colab\n!pip install torch_geometric\n# !pip install torch_sparse\n# !pip install torch_scatter","metadata":{"id":"1qx2qWQoXLp2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bf8d074-ccc1-4c6f-f8c9-64fe682f1728","scrolled":true,"execution":{"iopub.status.busy":"2024-10-28T21:12:42.750738Z","iopub.execute_input":"2024-10-28T21:12:42.751755Z","iopub.status.idle":"2024-10-28T21:12:57.667164Z","shell.execute_reply.started":"2024-10-28T21:12:42.751679Z","shell.execute_reply":"2024-10-28T21:12:57.665556Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pennylane qiskit pennylane-qiskit pylatexenc","metadata":{"id":"_CF_l60hp0xJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5040604-e7a1-4389-8e67-a80a21e53527","scrolled":true,"execution":{"iopub.status.busy":"2024-10-28T21:12:57.669396Z","iopub.execute_input":"2024-10-28T21:12:57.669800Z","iopub.status.idle":"2024-10-28T21:13:27.945988Z","shell.execute_reply.started":"2024-10-28T21:12:57.669760Z","shell.execute_reply":"2024-10-28T21:13:27.944211Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.38.0-py3-none-any.whl.metadata (9.3 kB)\nCollecting qiskit\n  Downloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.38.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.14.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.3)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting autograd (from pennylane)\n  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nCollecting pennylane-lightning>=0.38 (from pennylane)\n  Downloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pennylane) (21.3)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.12)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit) (0.3.8)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from qiskit) (2.9.0.post0)\nCollecting stevedore>=3.0.0 (from qiskit)\n  Downloading stevedore-5.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting symengine<0.14,>=0.11 (from qiskit)\n  Downloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nCollecting qiskit-aer (from pennylane-qiskit)\n  Downloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nCollecting qiskit-ibm-runtime<=0.29 (from pennylane-qiskit)\n  Downloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl.metadata (19 kB)\nCollecting qiskit-ibm-provider (from pennylane-qiskit)\n  Downloading qiskit_ibm_provider-0.11.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\nCollecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.26.18)\nRequirement already satisfied: websocket-client>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.8.0)\nCollecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_platform_services-0.59.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.9.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.8.30)\nCollecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pennylane) (3.1.2)\nRequirement already satisfied: psutil>=5 in /opt/conda/lib/python3.10/site-packages (from qiskit-aer->pennylane-qiskit) (5.9.3)\nRequirement already satisfied: websockets>=10.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (12.0)\nCollecting ibm-cloud-sdk-core<4.0.0,>=3.22.0 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.23.4)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (42.0.8)\nCollecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading pyspnego-0.11.1-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (1.16.0)\nCollecting urllib3>=1.21.1 (from qiskit-ibm-runtime<=0.29->pennylane-qiskit)\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.22.0->ibm-platform-services>=0.22.6->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.8.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime<=0.29->pennylane-qiskit) (2.22)\nDownloading PennyLane-0.38.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit-1.2.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading PennyLane_qiskit-0.38.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_runtime-0.29.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading stevedore-5.3.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_aer-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_provider-0.11.0-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ibm_platform_services-0.59.0-py3-none-any.whl (340 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.8/340.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\nDownloading ibm_cloud_sdk_core-3.22.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspnego-0.11.1-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pylatexenc\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=739ae0ecbb4bde576c46fbb1a0c9fba5d8a7f14a3c80c4692409f6b69a935337\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\nSuccessfully built pylatexenc\nInstalling collected packages: pylatexenc, urllib3, symengine, rustworkx, pbr, autoray, autograd, stevedore, qiskit, pyspnego, ibm-cloud-sdk-core, requests-ntlm, qiskit-aer, ibm-platform-services, qiskit-ibm-runtime, qiskit-ibm-provider, pennylane-lightning, pennylane, pennylane-qiskit\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autograd-1.7.0 autoray-0.7.0 ibm-cloud-sdk-core-3.22.0 ibm-platform-services-0.59.0 pbr-6.1.0 pennylane-0.38.0 pennylane-lightning-0.38.0 pennylane-qiskit-0.38.1 pylatexenc-2.10 pyspnego-0.11.1 qiskit-1.2.4 qiskit-aer-0.15.1 qiskit-ibm-provider-0.11.0 qiskit-ibm-runtime-0.29.0 requests-ntlm-1.3.0 rustworkx-0.15.1 stevedore-5.3.0 symengine-0.13.0 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pennylane as qml\nimport qiskit\nprint(qml.__version__)\nprint(qiskit.__version__)\nimport pennylane_qiskit\nprint(pennylane_qiskit.__version__)\nimport pennylane as qml\nfrom pennylane import numpy as np\n# from pennylane_qiskit import AerDevice","metadata":{"id":"wITHoRhbp1XM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f4b7de2-cd89-42c1-db72-c0504a891eee","execution":{"iopub.status.busy":"2024-10-28T21:13:27.948111Z","iopub.execute_input":"2024-10-28T21:13:27.948684Z","iopub.status.idle":"2024-10-28T21:13:32.222633Z","shell.execute_reply.started":"2024-10-28T21:13:27.948622Z","shell.execute_reply":"2024-10-28T21:13:32.221316Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0.38.0\n1.2.4\n0.38.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"Okf-o8BkYVJF"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nfrom torchvision import datasets, transforms\nfrom scipy.sparse import coo_matrix\n\ndef save_fashion_mnist_tensors(class_indices=[0, 1], num_data_per_class=500, save_dir=\"fashion_mnist/data\"):\n    \"\"\"\n    Generate and save tensor data files for Fashion MNIST dataset in a graph-like format.\n    \"\"\"\n    assert len(class_indices) == 2, \"Please specify exactly 2 class indices\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load Fashion MNIST data\n    fashion_mnist = datasets.FashionMNIST(root='./data', train=True, download=True,\n                                        transform=transforms.ToTensor())\n\n    # Select subset of data for specified classes\n    indices = []\n    labels = []\n    for i, class_idx in enumerate(class_indices):\n        class_indices_temp = (fashion_mnist.targets == class_idx).nonzero().squeeze()\n        selected_indices = class_indices_temp[:num_data_per_class].tolist()\n        indices.extend(selected_indices)\n        # Create binary labels (0 for first class, 1 for second class)\n        labels.extend([i] * num_data_per_class)\n\n    # Convert indices and labels to tensors\n    indices = torch.tensor(indices)\n    labels = torch.tensor(labels, dtype=torch.float)\n\n    # Shuffle the data\n    shuffle_idx = torch.randperm(len(indices))\n    indices = indices[shuffle_idx]\n    labels = labels[shuffle_idx]\n\n    X = fashion_mnist.data[indices].float() / 255.0  # Normalize to [0,1]\n\n    batch_size = len(X)\n    n_nodes = 10  # Maximum number of significant points to extract per image\n\n    def image_to_nodes(image_data, max_nodes=10, threshold=0.1):\n        \"\"\"\n        Convert Fashion MNIST images to graph nodes.\n        Extracts significant points based on intensity values.\n        \"\"\"\n        batch_size = len(image_data)\n        nodes = np.zeros((batch_size, max_nodes, 1))  # Single channel feature\n        p4s = np.zeros((batch_size, max_nodes, 4))    # Position and feature information\n        atom_masks = np.zeros((batch_size, max_nodes), dtype=bool)\n\n        for b in range(batch_size):\n            # Find significant points in the image (edges and important features)\n            significant_points = np.where(image_data[b] > threshold)\n            values = image_data[b][significant_points]\n\n            # Sort points by their intensity and take top max_nodes\n            sorted_indices = np.argsort(-values)\n            n_points = min(len(sorted_indices), max_nodes)\n            selected_indices = sorted_indices[:n_points]\n\n            for idx_pos, idx in enumerate(selected_indices):\n                h, w = significant_points[0][idx], significant_points[1][idx]\n                intensity = values[idx]\n\n                # Node feature (intensity value)\n                nodes[b, idx_pos, 0] = intensity\n\n                # Create p4s (x, y, intensity, 0)\n                # Normalize coordinates to [-1, 1] range\n                x = (w - 13.5) / 13.5  # Center and normalize\n                y = (h - 13.5) / 13.5  # Center and normalize\n                p4s[b, idx_pos] = [x, y, intensity, 0]\n                atom_masks[b, idx_pos] = True\n\n        return p4s, nodes, atom_masks\n\n    # Convert image data to graph format\n    p4s, nodes, atom_mask = image_to_nodes(X.numpy())\n\n    # Convert to torch tensors\n    p4s = torch.from_numpy(p4s).float()\n    nodes = torch.from_numpy(nodes).float()\n    atom_mask = torch.from_numpy(atom_mask)\n\n    # Create edge mask (fully connected graph between valid nodes)\n    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    edge_mask = edge_mask * diag_mask\n\n    # Calculate edges\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx * n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n    edges = np.stack([rows, cols])\n\n    # Save tensors\n    torch.save(p4s, os.path.join(save_dir, \"p4s.pt\"))\n    torch.save(nodes, os.path.join(save_dir, \"nodes.pt\"))\n    torch.save(labels, os.path.join(save_dir, \"labels.pt\"))\n    torch.save(atom_mask, os.path.join(save_dir, \"atom_mask.pt\"))\n    np.save(os.path.join(save_dir, \"edge_mask.npy\"), edge_mask.numpy())\n    np.save(os.path.join(save_dir, \"edges.npy\"), edges)\n\n    print(f\"Saved tensor files to {save_dir}\")\n    print(f\"Classes used: {class_indices}\")\n    print(f\"Shapes:\")\n    print(f\"p4s: {p4s.shape}\")\n    print(f\"nodes: {nodes.shape}\")\n    print(f\"labels: {labels.shape}\")\n    print(f\"atom_mask: {atom_mask.shape}\")\n    print(f\"edge_mask: {edge_mask.shape}\")\n    print(f\"edges: {edges.shape}\")\n\n    # Print label distribution\n    unique_labels, counts = torch.unique(labels, return_counts=True)\n    print(\"\\nLabel distribution:\")\n    for label, count in zip(unique_labels, counts):\n        print(f\"Class {label.item()}: {count.item()} samples\")\n\n# Generate data for T-shirts (0) vs Trousers (1)\nsave_fashion_mnist_tensors(\n    class_indices=[0, 1],  # T-shirt/top vs Trouser\n    num_data_per_class=5000,  # 500 samples per class = 1000 total\n    save_dir=\"fashion_mnist/data\"\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDF86EgyDwkY","outputId":"024002a1-9501-4b18-c651-97e48b0dab74","execution":{"iopub.status.busy":"2024-10-28T21:52:22.647428Z","iopub.execute_input":"2024-10-28T21:52:22.647858Z","iopub.status.idle":"2024-10-28T21:52:25.016948Z","shell.execute_reply.started":"2024-10-28T21:52:22.647818Z","shell.execute_reply":"2024-10-28T21:52:25.015208Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Saved tensor files to fashion_mnist/data\nClasses used: [0, 1]\nShapes:\np4s: torch.Size([10000, 10, 4])\nnodes: torch.Size([10000, 10, 1])\nlabels: torch.Size([10000])\natom_mask: torch.Size([10000, 10])\nedge_mask: torch.Size([10000, 10, 10])\nedges: (2, 900000)\n\nLabel distribution:\nClass 0.0: 5000 samples\nClass 1.0: 5000 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"# from torch.utils.data import TensorDataset, random_split\n# import torch\n# import numpy as np\n# from torch.utils.data import TensorDataset, DataLoader\n# from scipy.sparse import coo_matrix\n# import h5py\n\ndef get_adj_matrix(n_nodes, batch_size, edge_mask):\n    rows, cols = [], []\n    for batch_idx in range(batch_size):\n        nn = batch_idx*n_nodes\n        x = coo_matrix(edge_mask[batch_idx])\n        rows.append(nn + x.row)\n        cols.append(nn + x.col)\n    rows = np.concatenate(rows)\n    cols = np.concatenate(cols)\n\n    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n    return edges\n\ndef collate_fn(data):\n    data = list(zip(*data)) # label p4s nodes atom_mask\n    data = [torch.stack(item) for item in data]\n    batch_size, n_nodes, _ = data[1].size()\n    atom_mask = data[-1]\n    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n    # edge_mask *= diag_mask\n\n    edge_mask = data[-2]\n\n    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n    return data + [edges]","metadata":{"id":"qGYOC1AdECNJ","execution":{"iopub.status.busy":"2024-10-28T21:52:27.650713Z","iopub.execute_input":"2024-10-28T21:52:27.651130Z","iopub.status.idle":"2024-10-28T21:52:27.660959Z","shell.execute_reply.started":"2024-10-28T21:52:27.651093Z","shell.execute_reply":"2024-10-28T21:52:27.659599Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\nimport torch\n\ndef create_stratified_split(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n    \"\"\"\n    Creates stratified train/val/test splits for binary classification.\n    \"\"\"\n    # Extract labels and convert to integer labels for stratification\n    labels = dataset.tensors[0].numpy()\n    labels_int = labels.astype(int)  # Convert to int for stratification\n\n    # Print initial distribution\n    unique_classes, class_counts = np.unique(labels_int, return_counts=True)\n    print(\"\\nInitial class distribution:\")\n    for class_idx, count in zip(unique_classes, class_counts):\n        print(f\"Class {class_idx}: {count} samples ({count/len(labels)*100:.2f}%)\")\n\n    # First split: separate train from temporary val+test\n    first_sss = StratifiedShuffleSplit(n_splits=1,\n                                      train_size=train_ratio,\n                                      random_state=random_state)\n\n    train_idx, temp_idx = next(first_sss.split(np.zeros(len(labels)), labels_int))\n\n    # Second split: separate val and test from temporary set\n    temp_labels = labels_int[temp_idx]\n    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n\n    second_sss = StratifiedShuffleSplit(n_splits=1,\n                                       train_size=val_ratio_adjusted,\n                                       random_state=random_state)\n\n    val_idx_temp, test_idx_temp = next(second_sss.split(np.zeros(len(temp_labels)), temp_labels))\n\n    # Convert temporary indices to original dataset indices\n    val_idx = temp_idx[val_idx_temp]\n    test_idx = temp_idx[test_idx_temp]\n\n    # Create subset datasets\n    train_dataset = Subset(dataset, train_idx)\n    val_dataset = Subset(dataset, val_idx)\n    test_dataset = Subset(dataset, test_idx)\n\n    # Verify class distribution in each split\n    def get_split_distribution(indices):\n        split_labels = labels_int[indices]\n        unique, counts = np.unique(split_labels, return_counts=True)\n        return dict(zip(unique, counts / len(indices)))\n\n    print(\"\\nClass distribution in splits:\")\n    print(\"Train:\", get_split_distribution(train_idx))\n    print(\"Val:\", get_split_distribution(val_idx))\n    print(\"Test:\", get_split_distribution(test_idx))\n\n    return {\n        \"train\": train_dataset,\n        \"val\": val_dataset,\n        \"test\": test_dataset\n    }\n\n# Let's reload the data and ensure we have binary labels\nlabels = torch.load('fashion_mnist/data/labels.pt')\np4s = torch.load('fashion_mnist/data/p4s.pt')\nnodes = torch.load('fashion_mnist/data/nodes.pt')\natom_mask = torch.load('fashion_mnist/data/atom_mask.pt')\nedge_mask = torch.from_numpy(np.load('fashion_mnist/data/edge_mask.npy'))\n\n# Print label statistics before creating dataset\nprint(\"Label Statistics:\")\nprint(\"Shape:\", labels.shape)\nprint(\"Unique values:\", torch.unique(labels))\nprint(\"Class distribution:\", torch.bincount(labels.long()) / len(labels))\n\n# Create the dataset\ndataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n\n# Create stratified splits\ndatasets = create_stratified_split(dataset_all)\n\n# Create dataloaders\ndataloaders = {\n    split: DataLoader(\n        dataset,\n        batch_size=16,\n        pin_memory=False,\n        collate_fn=collate_fn,\n        drop_last=True if (split == 'train') else False,\n        num_workers=0,\n        shuffle=(split == 'train')\n    )\n    for split, dataset in datasets.items()\n}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSghd7-HTiEU","outputId":"331388b3-f85b-4355-fbd0-3b59748f4882","execution":{"iopub.status.busy":"2024-10-28T21:52:29.607789Z","iopub.execute_input":"2024-10-28T21:52:29.608234Z","iopub.status.idle":"2024-10-28T21:52:29.642640Z","shell.execute_reply.started":"2024-10-28T21:52:29.608184Z","shell.execute_reply":"2024-10-28T21:52:29.641348Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Label Statistics:\nShape: torch.Size([10000])\nUnique values: tensor([0., 1.])\nClass distribution: tensor([0.5000, 0.5000])\n\nInitial class distribution:\nClass 0: 5000 samples (50.00%)\nClass 1: 5000 samples (50.00%)\n\nClass distribution in splits:\nTrain: {0: 0.5, 1: 0.5}\nVal: {0: 0.5, 1: 0.5}\nTest: {0: 0.5, 1: 0.5}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/930516978.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  labels = torch.load('fashion_mnist/data/labels.pt')\n/tmp/ipykernel_31/930516978.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  p4s = torch.load('fashion_mnist/data/p4s.pt')\n/tmp/ipykernel_31/930516978.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  nodes = torch.load('fashion_mnist/data/nodes.pt')\n/tmp/ipykernel_31/930516978.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  atom_mask = torch.load('fashion_mnist/data/atom_mask.pt')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set desired dimensions\nbatch_size = 1\nn_nodes = 3\ndevice = 'cpu'\ndtype = torch.float32\n\n# Print initial shapes\nprint(\"Initial shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Select subset of data\np4s = p4s[:batch_size, :n_nodes, :]\natom_mask = atom_mask[:batch_size, :n_nodes]\nedge_mask = edge_mask[:batch_size, :n_nodes, :n_nodes]\nnodes = nodes[:batch_size, :n_nodes, :]\n\nprint(\"\\nAfter selection shapes:\")\nprint(\"p4s:\", p4s.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\n\n# Reshape tensors\natom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\natom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n# Don't reshape edge_mask yet\nnodes = nodes.view(batch_size * n_nodes, -1).to(device, dtype)\n\nprint(\"\\nAfter reshape shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)  # original shape\nprint(\"nodes:\", nodes.shape)\n\n# Recalculate edges for the subset\nfrom scipy.sparse import coo_matrix\nrows, cols = [], []\nfor batch_idx in range(batch_size):\n    nn = batch_idx * n_nodes\n    # Convert edge_mask to numpy and remove any extra dimensions\n    edge_mask_np = edge_mask[batch_idx].cpu().numpy().squeeze()\n    x = coo_matrix(edge_mask_np)\n    rows.append(nn + x.row)\n    cols.append(nn + x.col)\n\nedges = [torch.LongTensor(np.concatenate(rows)).to(device),\n         torch.LongTensor(np.concatenate(cols)).to(device)]\n\n# Now reshape edge_mask after edges are calculated\nedge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n\nprint(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"TEs9qVoYXLp8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8da95105-9a5b-4097-c36d-b923e8f511e0","execution":{"iopub.status.busy":"2024-10-28T21:52:30.106767Z","iopub.execute_input":"2024-10-28T21:52:30.107168Z","iopub.status.idle":"2024-10-28T21:52:30.125264Z","shell.execute_reply.started":"2024-10-28T21:52:30.107130Z","shell.execute_reply":"2024-10-28T21:52:30.123996Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Initial shapes:\np4s: torch.Size([10000, 10, 4])\natom_mask: torch.Size([10000, 10])\nedge_mask: torch.Size([10000, 10, 10])\nnodes: torch.Size([10000, 10, 1])\n\nAfter selection shapes:\np4s: torch.Size([1, 3, 4])\natom_mask: torch.Size([1, 3])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([1, 3, 1])\n\nAfter reshape shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([1, 3, 3])\nnodes: torch.Size([3, 1])\n\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\nFinal shapes:\")\nprint(\"atom_positions:\", atom_positions.shape)\nprint(\"atom_mask:\", atom_mask.shape)\nprint(\"edge_mask:\", edge_mask.shape)\nprint(\"nodes:\", nodes.shape)\nprint(\"edges:\", [e.shape for e in edges])","metadata":{"id":"cNq2qPiZKqis","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41df1033-b47d-48cc-f428-a68419dbce22","execution":{"iopub.status.busy":"2024-10-28T21:52:31.762945Z","iopub.execute_input":"2024-10-28T21:52:31.763524Z","iopub.status.idle":"2024-10-28T21:52:31.771132Z","shell.execute_reply.started":"2024-10-28T21:52:31.763471Z","shell.execute_reply":"2024-10-28T21:52:31.769472Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nFinal shapes:\natom_positions: torch.Size([3, 4])\natom_mask: torch.Size([3, 1])\nedge_mask: torch.Size([9, 1])\nnodes: torch.Size([3, 1])\nedges: [torch.Size([6]), torch.Size([6])]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. LorentzNet","metadata":{"id":"_3cyDPxrXLp-"}},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\n\n\"\"\"Some auxiliary functions\"\"\"\n\ndef unsorted_segment_sum(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    return result\n\ndef unsorted_segment_mean(data, segment_ids, num_segments):\n    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n    Adapted from https://github.com/vgsatorras/egnn.\n    '''\n    result = data.new_zeros((num_segments, data.size(1)))\n    count = data.new_zeros((num_segments, data.size(1)))\n    result.index_add_(0, segment_ids, data)\n    count.index_add_(0, segment_ids, torch.ones_like(data))\n    return result / count.clamp(min=1)\n\ndef normsq4(p):\n    r''' Minkowski square norm\n         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n    '''\n    psq = torch.pow(p, 2)\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef dotsq4(p,q):\n    r''' Minkowski inner product\n         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n    '''\n    psq = p*q\n    return 2 * psq[..., 0] - psq.sum(dim=-1)\n\ndef normA_fn(A):\n    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n\ndef dotA_fn(A):\n    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n\ndef psi(p):\n    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n    '''\n    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n\n\n\"\"\"Lorentz Group-Equivariant Block\"\"\"\n\nclass LGEB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LGEB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2.\n        print('Input size of phi_e: ', n_input)\n\n        self.include_x = include_x\n        self.phi_e = nn.Sequential(\n            nn.Linear(n_input, n_hidden, bias=False), # n_input * 2 + n_edge_attr\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU())\n\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        # Reduce the dimension of 'out' to 4 using a linear layer\n        out = self.dimension_reducer(out)\n        out = self.phi_e(out)\n        # print(\"m_model output: \", out.shape)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.phi_e(out)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m): # norms\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # print(\"m: \", m.shape)\n        # print(\"trans: \", trans.shape)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        # print(\"trans: \", trans.shape)\n        # print(\"x.size: \", x.size(0))\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            # print(\"X: \", x)\n            x = self.x_model(x, edges, x_diff, m)\n            # print(\"phi_x(X) = \", x, '\\n---\\n')\n\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LorentzNet(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LGEB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LorentzNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n\n        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n        return pred.squeeze(1)","metadata":{"id":"49hLoUYRXLp_","execution":{"iopub.status.busy":"2024-10-28T21:52:33.063693Z","iopub.execute_input":"2024-10-28T21:52:33.064157Z","iopub.status.idle":"2024-10-28T21:52:33.101865Z","shell.execute_reply.started":"2024-10-28T21:52:33.064114Z","shell.execute_reply":"2024-10-28T21:52:33.100480Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"LGEB(self.n_hidden, self.n_hidden, self.n_hidden,\\\n                                    n_node_attr=n_scalar, dropout=dropout,\\\n                                    c_weight=c_weight, last_layer=\\(i==n_layers-1), A=A, include_x=include_x)\n                                    \nWe are using n_hidden = 4 and n_layers = 6\n\nn_input=n_hidden, n_output=n_hidden, n_hidden=n_hidden, n_node_attr=n_scalar=8","metadata":{"id":"yKQ1ZOC2U92Y"}},{"cell_type":"code","source":"# @title\nimport torch\nimport os, json, random, string\nimport torch.distributed as dist\n\ndef makedir(path):\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n\ndef args_init(args):\n    r''' Initialize seed and exp_name.\n    '''\n    if args.seed is None: # use random seed if not specified\n        args.seed = np.random.randint(100)\n    if args.exp_name == '': # use random strings if not specified\n        args.exp_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n    if (args.local_rank == 0): # master\n        print(args)\n        makedir(f\"{args.logdir}/{args.exp_name}\")\n        with open(f\"{args.logdir}/{args.exp_name}/args.json\", 'w') as f:\n            json.dump(args.__dict__, f, indent=4)\n\ndef sum_reduce(num, device):\n    r''' Sum the tensor across the devices.\n    '''\n    if not torch.is_tensor(num):\n        rt = torch.tensor(num).to(device)\n    else:\n        rt = num.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    return rt\n\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        warmup_epoch: target learning rate is reached at warmup_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    Reference:\n        https://github.com/ildoonet/pytorch-gradual-warmup-lr\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, warmup_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.warmup_epoch = warmup_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    @property\n    def _warmup_lr(self):\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch + 1) / self.warmup_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * (self.last_epoch + 1) / self.warmup_epoch + 1.) for base_lr in self.base_lrs]\n\n    def get_lr(self):\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        return self._warmup_lr\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        self.last_epoch = self.last_epoch + 1 if epoch==None else epoch\n        if self.last_epoch >= self.warmup_epoch - 1:\n            if not self.finished:\n                warmup_lr = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                    param_group['lr'] = lr\n                self.finished = True\n                return\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.warmup_epoch)\n            return\n\n        for param_group, lr in zip(self.optimizer.param_groups, self._warmup_lr):\n            param_group['lr'] = lr\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.warmup_epoch)\n                self.last_epoch = self.after_scheduler.last_epoch + self.warmup_epoch + 1\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        result = {key: value for key, value in self.__dict__.items() if key != 'optimizer' or key != \"after_scheduler\"}\n        if self.after_scheduler:\n            result.update({\"after_scheduler\": self.after_scheduler.state_dict()})\n        return result\n\n    def load_state_dict(self, state_dict):\n        after_scheduler_state = state_dict.pop(\"after_scheduler\", None)\n        self.__dict__.update(state_dict)\n        if after_scheduler_state:\n            self.after_scheduler.load_state_dict(after_scheduler_state)\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport numpy as np\n\ndef buildROC(labels, score, targetEff=[0.3,0.5]):\n    r''' ROC curve is a plot of the true positive rate (Sensitivity) in the function of the false positive rate\n    (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a\n    sensitivity/specificity pair corresponding to a particular decision threshold. The Area Under the ROC\n    curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups.\n    '''\n    if not isinstance(targetEff, list):\n        targetEff = [targetEff]\n    fpr, tpr, threshold = roc_curve(labels, score)\n    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n    eB, eS = fpr[idx], tpr[idx]\n    return fpr, tpr, threshold, eB, eS","metadata":{"id":"KRBzC37VorM9","execution":{"iopub.status.busy":"2024-10-28T21:52:34.424170Z","iopub.execute_input":"2024-10-28T21:52:34.424628Z","iopub.status.idle":"2024-10-28T21:52:34.452594Z","shell.execute_reply.started":"2024-10-28T21:52:34.424585Z","shell.execute_reply":"2024-10-28T21:52:34.451351Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\n\ndef run(model, epoch, loader, partition, N_EPOCHS=None):\n    if partition == 'train':\n        model.train()\n    else:\n        model.eval()\n\n    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n\n    tik = time.time()\n    loader_length = len(loader)\n\n    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n        if partition == 'train':\n            optimizer.zero_grad()\n\n        batch_size, n_nodes, _ = p4s.size()\n        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n        edges = [a.to(device) for a in edges]\n        label = label.to(device, dtype).long()\n\n        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n                         edge_mask=edge_mask, n_nodes=n_nodes)\n\n        predict = pred.max(1).indices\n        correct = torch.sum(predict == label).item()\n        # print(pred.shape,label.shape)\n        loss = loss_fn(pred, label)\n\n        if partition == 'train':\n            loss.backward()\n            optimizer.step()\n        elif partition == 'test':\n            # save labels and probilities for ROC / AUC\n            # print(\"Preds \", pred)\n            score = torch.nn.functional.softmax(pred, dim = -1)\n            # print(\"Score test \", score)\n            # raise\n            res['label'].append(label)\n            res['score'].append(score)\n\n        res['time'] = time.time() - tik\n        res['correct'] += correct\n        res['loss'] += loss.item() * batch_size\n        res['counter'] += batch_size\n        res['loss_arr'].append(loss.item())\n        res['correct_arr'].append(correct)\n\n        # if i != 0 and i % args.log_interval == 0:\n\n    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n    avg_time = res['time']/res['counter'] * batch_size\n    tmp_counter = res['counter']\n    tmp_loss = res['loss'] / tmp_counter\n    tmp_acc = res['correct'] / tmp_counter\n\n    if N_EPOCHS:\n        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n    else:\n        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n             (partition, running_loss, running_acc, tmp_acc, avg_time))\n\n    torch.cuda.empty_cache()\n    # ---------- reduce -----------\n    if partition == 'test':\n        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n        res['score'] = torch.cat(res['score'])\n        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n    res['counter'] = res['counter']\n    res['loss'] = res['loss'] / res['counter']\n    res['acc'] = res['correct'] / res['counter']\n    return res\n\ndef train(model, res, N_EPOCHS, model_path, log_path):\n    ### training and validation\n    os.makedirs(model_path, exist_ok=True)\n    os.makedirs(log_path, exist_ok=True)\n\n    for epoch in range(N_EPOCHS):\n        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n        # if epoch % args.val_interval == 0:\n\n        # if (args.local_rank == 0):\n        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n        with torch.no_grad():\n            val_res = run(model, epoch, dataloaders['val'], partition='val')\n\n        # if (args.local_rank == 0): # only master process save\n        res['lr'].append(optimizer.param_groups[0]['lr'])\n        res['train_time'].append(train_res['time'])\n        res['val_time'].append(val_res['time'])\n        res['train_loss'].append(train_res['loss'])\n        res['train_acc'].append(train_res['acc'])\n        res['val_loss'].append(val_res['loss'])\n        res['val_acc'].append(val_res['acc'])\n        res['epochs'].append(epoch)\n\n        ## save best model\n        if val_res['acc'] > res['best_val']:\n            print(\"New best validation model, saving...\")\n            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n            res['best_val'] = val_res['acc']\n            res['best_epoch'] = epoch\n\n        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n\n        json_object = json.dumps(res, indent=4)\n        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n            outfile.write(json_object)\n\n        ## adjust learning rate\n        if (epoch < 31):\n            lr_scheduler.step(metrics=val_res['acc'])\n        else:\n            for g in optimizer.param_groups:\n                g['lr'] = g['lr']*0.5\n\n\ndef test(model, res, model_path, log_path):\n    ### test on best model\n    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n    model.load_state_dict(best_model)\n    with torch.no_grad():\n        test_res = run(model, 0, dataloaders['test'], partition='test')\n\n    print(\"Final \", test_res['score'])\n    pred = test_res['score'].cpu()\n\n    np.save(os.path.join(log_path, \"score.npy\"), pred)\n    fpr, tpr, thres, eB, eS  = buildROC(pred[...,0], pred[...,2])\n    auc = roc_auc_score(pred[...,0], pred[...,2])\n\n    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n    res.update(metric)\n    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n    json_object = json.dumps(res, indent=4)\n    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n        outfile.write(json_object)\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 55 # 60\n\n    model_path = \"models/LorentzNet/\"\n    log_path = \"logs/LorentzNet/\"\n    # args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cuda\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cpu\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    ### create parallel model\n    model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"scrolled":true,"id":"8azxxVjtXLqI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c705f49b-fcae-4a2a-98f5-0e8db54c6c2b","execution":{"iopub.status.busy":"2024-10-28T21:52:35.618344Z","iopub.execute_input":"2024-10-28T21:52:35.618767Z","iopub.status.idle":"2024-10-28T21:56:22.954424Z","shell.execute_reply.started":"2024-10-28T21:52:35.618729Z","shell.execute_reply":"2024-10-28T21:56:22.952854Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input size of phi_e:  4\nModel Size: 199\n train samples: 8000\n val samples: 1000\n test samples: 1000\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 124.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/55 \t Batch 499/500 \t Loss 0.7082 \t Running Acc 0.503 \t Total Acc 0.503 \t Avg Batch Time 0.0080\nTime: train: 4.01 \t Train loss 0.7082 \t Train acc: 0.5031\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 193.55it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6903 \t Running Acc 0.992 \t Total Acc 0.500 \t Avg Batch Time 0.0026\nNew best validation model, saving...\nEpoch 0/55 finished.\nTrain time: 4.01 \t Val time 0.33\nTrain loss 0.7082 \t Train acc: 0.5031\nVal loss: 0.6905 \t Val acc: 0.5000\nBest val acc: 0.5000 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.14it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/55 \t Batch 499/500 \t Loss 0.6741 \t Running Acc 0.581 \t Total Acc 0.581 \t Avg Batch Time 0.0077\nTime: train: 3.87 \t Train loss 0.6741 \t Train acc: 0.5806\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 244.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6668 \t Running Acc 1.224 \t Total Acc 0.617 \t Avg Batch Time 0.0021\nNew best validation model, saving...\nEpoch 1/55 finished.\nTrain time: 3.87 \t Val time 0.26\nTrain loss 0.6741 \t Train acc: 0.5806\nVal loss: 0.6670 \t Val acc: 0.6170\nBest val acc: 0.6170 at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 131.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/55 \t Batch 499/500 \t Loss 0.6033 \t Running Acc 0.727 \t Total Acc 0.727 \t Avg Batch Time 0.0076\nTime: train: 3.80 \t Train loss 0.6033 \t Train acc: 0.7266\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 252.23it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6500 \t Running Acc 1.210 \t Total Acc 0.610 \t Avg Batch Time 0.0020\nEpoch 2/55 finished.\nTrain time: 3.80 \t Val time 0.25\nTrain loss 0.6033 \t Train acc: 0.7266\nVal loss: 0.6506 \t Val acc: 0.6100\nBest val acc: 0.6170 at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.63it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/55 \t Batch 499/500 \t Loss 0.5589 \t Running Acc 0.744 \t Total Acc 0.744 \t Avg Batch Time 0.0077\nTime: train: 3.86 \t Train loss 0.5589 \t Train acc: 0.7444\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 238.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5300 \t Running Acc 1.496 \t Total Acc 0.754 \t Avg Batch Time 0.0021\nNew best validation model, saving...\nEpoch 3/55 finished.\nTrain time: 3.86 \t Val time 0.27\nTrain loss 0.5589 \t Train acc: 0.7444\nVal loss: 0.5310 \t Val acc: 0.7540\nBest val acc: 0.7540 at epoch 3.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/55 \t Batch 499/500 \t Loss 0.5477 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 0.0075\nTime: train: 3.73 \t Train loss 0.5477 \t Train acc: 0.7452\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 262.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5755 \t Running Acc 1.417 \t Total Acc 0.714 \t Avg Batch Time 0.0019\nEpoch 4/55 finished.\nTrain time: 3.73 \t Val time 0.24\nTrain loss 0.5477 \t Train acc: 0.7452\nVal loss: 0.5776 \t Val acc: 0.7140\nBest val acc: 0.7540 at epoch 3.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 134.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/55 \t Batch 499/500 \t Loss 0.5387 \t Running Acc 0.749 \t Total Acc 0.749 \t Avg Batch Time 0.0074\nTime: train: 3.72 \t Train loss 0.5387 \t Train acc: 0.7492\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 241.81it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5221 \t Running Acc 1.502 \t Total Acc 0.757 \t Avg Batch Time 0.0021\nNew best validation model, saving...\nEpoch 5/55 finished.\nTrain time: 3.72 \t Val time 0.26\nTrain loss 0.5387 \t Train acc: 0.7492\nVal loss: 0.5229 \t Val acc: 0.7570\nBest val acc: 0.7570 at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 137.55it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/55 \t Batch 499/500 \t Loss 0.5273 \t Running Acc 0.759 \t Total Acc 0.759 \t Avg Batch Time 0.0073\nTime: train: 3.64 \t Train loss 0.5273 \t Train acc: 0.7589\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 262.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5103 \t Running Acc 1.538 \t Total Acc 0.775 \t Avg Batch Time 0.0019\nNew best validation model, saving...\nEpoch 6/55 finished.\nTrain time: 3.64 \t Val time 0.24\nTrain loss 0.5273 \t Train acc: 0.7589\nVal loss: 0.5113 \t Val acc: 0.7750\nBest val acc: 0.7750 at epoch 6.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.80it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/55 \t Batch 499/500 \t Loss 0.5210 \t Running Acc 0.760 \t Total Acc 0.760 \t Avg Batch Time 0.0075\nTime: train: 3.74 \t Train loss 0.5210 \t Train acc: 0.7600\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 252.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4882 \t Running Acc 1.560 \t Total Acc 0.786 \t Avg Batch Time 0.0020\nNew best validation model, saving...\nEpoch 7/55 finished.\nTrain time: 3.74 \t Val time 0.25\nTrain loss 0.5210 \t Train acc: 0.7600\nVal loss: 0.4890 \t Val acc: 0.7860\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/55 \t Batch 499/500 \t Loss 0.5271 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0077\nTime: train: 3.87 \t Train loss 0.5271 \t Train acc: 0.7569\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 205.15it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6399 \t Running Acc 1.238 \t Total Acc 0.624 \t Avg Batch Time 0.0025\nEpoch 8/55 finished.\nTrain time: 3.87 \t Val time 0.31\nTrain loss 0.5271 \t Train acc: 0.7569\nVal loss: 0.6417 \t Val acc: 0.6240\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.81it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/55 \t Batch 499/500 \t Loss 0.5274 \t Running Acc 0.751 \t Total Acc 0.751 \t Avg Batch Time 0.0075\nTime: train: 3.77 \t Train loss 0.5274 \t Train acc: 0.7510\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 250.40it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5092 \t Running Acc 1.542 \t Total Acc 0.777 \t Avg Batch Time 0.0020\nEpoch 9/55 finished.\nTrain time: 3.77 \t Val time 0.25\nTrain loss 0.5274 \t Train acc: 0.7510\nVal loss: 0.5100 \t Val acc: 0.7770\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 128.09it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/55 \t Batch 499/500 \t Loss 0.5206 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0078\nTime: train: 3.91 \t Train loss 0.5206 \t Train acc: 0.7572\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 261.72it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4823 \t Running Acc 1.556 \t Total Acc 0.784 \t Avg Batch Time 0.0020\nEpoch 10/55 finished.\nTrain time: 3.91 \t Val time 0.24\nTrain loss 0.5206 \t Train acc: 0.7572\nVal loss: 0.4826 \t Val acc: 0.7840\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 131.51it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/55 \t Batch 499/500 \t Loss 0.5168 \t Running Acc 0.761 \t Total Acc 0.761 \t Avg Batch Time 0.0076\nTime: train: 3.80 \t Train loss 0.5168 \t Train acc: 0.7608\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 259.20it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5306 \t Running Acc 1.452 \t Total Acc 0.732 \t Avg Batch Time 0.0020\nEpoch 11/55 finished.\nTrain time: 3.80 \t Val time 0.25\nTrain loss 0.5168 \t Train acc: 0.7608\nVal loss: 0.5321 \t Val acc: 0.7320\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.28it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/55 \t Batch 499/500 \t Loss 0.5206 \t Running Acc 0.757 \t Total Acc 0.757 \t Avg Batch Time 0.0077\nTime: train: 3.87 \t Train loss 0.5206 \t Train acc: 0.7569\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 254.13it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4804 \t Running Acc 1.548 \t Total Acc 0.780 \t Avg Batch Time 0.0020\nEpoch 12/55 finished.\nTrain time: 3.87 \t Val time 0.25\nTrain loss 0.5206 \t Train acc: 0.7569\nVal loss: 0.4808 \t Val acc: 0.7800\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 127.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/55 \t Batch 499/500 \t Loss 0.5165 \t Running Acc 0.758 \t Total Acc 0.758 \t Avg Batch Time 0.0078\nTime: train: 3.92 \t Train loss 0.5165 \t Train acc: 0.7580\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 247.71it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4737 \t Running Acc 1.556 \t Total Acc 0.784 \t Avg Batch Time 0.0021\nEpoch 13/55 finished.\nTrain time: 3.92 \t Val time 0.26\nTrain loss 0.5165 \t Train acc: 0.7580\nVal loss: 0.4747 \t Val acc: 0.7840\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 126.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/55 \t Batch 499/500 \t Loss 0.5171 \t Running Acc 0.760 \t Total Acc 0.760 \t Avg Batch Time 0.0079\nTime: train: 3.95 \t Train loss 0.5171 \t Train acc: 0.7602\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 241.00it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4781 \t Running Acc 1.550 \t Total Acc 0.781 \t Avg Batch Time 0.0021\nEpoch 14/55 finished.\nTrain time: 3.95 \t Val time 0.26\nTrain loss 0.5171 \t Train acc: 0.7602\nVal loss: 0.4789 \t Val acc: 0.7810\nBest val acc: 0.7860 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 125.22it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/55 \t Batch 499/500 \t Loss 0.5114 \t Running Acc 0.760 \t Total Acc 0.760 \t Avg Batch Time 0.0080\nTime: train: 3.99 \t Train loss 0.5114 \t Train acc: 0.7599\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 234.04it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4712 \t Running Acc 1.569 \t Total Acc 0.791 \t Avg Batch Time 0.0022\nNew best validation model, saving...\nEpoch 15/55 finished.\nTrain time: 3.99 \t Val time 0.27\nTrain loss 0.5114 \t Train acc: 0.7599\nVal loss: 0.4721 \t Val acc: 0.7910\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 123.47it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/55 \t Batch 499/500 \t Loss 0.5138 \t Running Acc 0.765 \t Total Acc 0.765 \t Avg Batch Time 0.0081\nTime: train: 4.05 \t Train loss 0.5138 \t Train acc: 0.7648\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 232.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4972 \t Running Acc 1.530 \t Total Acc 0.771 \t Avg Batch Time 0.0022\nEpoch 16/55 finished.\nTrain time: 4.05 \t Val time 0.27\nTrain loss 0.5138 \t Train acc: 0.7648\nVal loss: 0.4979 \t Val acc: 0.7710\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 128.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/55 \t Batch 499/500 \t Loss 0.5140 \t Running Acc 0.762 \t Total Acc 0.762 \t Avg Batch Time 0.0078\nTime: train: 3.88 \t Train loss 0.5140 \t Train acc: 0.7621\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 231.80it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4853 \t Running Acc 1.530 \t Total Acc 0.771 \t Avg Batch Time 0.0022\nEpoch 17/55 finished.\nTrain time: 3.88 \t Val time 0.28\nTrain loss 0.5140 \t Train acc: 0.7621\nVal loss: 0.4855 \t Val acc: 0.7710\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/55 \t Batch 499/500 \t Loss 0.5183 \t Running Acc 0.759 \t Total Acc 0.759 \t Avg Batch Time 0.0076\nTime: train: 3.78 \t Train loss 0.5183 \t Train acc: 0.7588\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 250.65it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6117 \t Running Acc 1.278 \t Total Acc 0.644 \t Avg Batch Time 0.0020\nEpoch 18/55 finished.\nTrain time: 3.78 \t Val time 0.25\nTrain loss 0.5183 \t Train acc: 0.7588\nVal loss: 0.6134 \t Val acc: 0.6440\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/55 \t Batch 499/500 \t Loss 0.5212 \t Running Acc 0.754 \t Total Acc 0.754 \t Avg Batch Time 0.0075\nTime: train: 3.76 \t Train loss 0.5212 \t Train acc: 0.7545\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 251.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4952 \t Running Acc 1.516 \t Total Acc 0.764 \t Avg Batch Time 0.0020\nEpoch 19/55 finished.\nTrain time: 3.76 \t Val time 0.25\nTrain loss 0.5212 \t Train acc: 0.7545\nVal loss: 0.4957 \t Val acc: 0.7640\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.60it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/55 \t Batch 499/500 \t Loss 0.5045 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0077\nTime: train: 3.86 \t Train loss 0.5045 \t Train acc: 0.7669\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 233.57it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5790 \t Running Acc 1.357 \t Total Acc 0.684 \t Avg Batch Time 0.0022\nEpoch 20/55 finished.\nTrain time: 3.86 \t Val time 0.27\nTrain loss 0.5045 \t Train acc: 0.7669\nVal loss: 0.5808 \t Val acc: 0.6840\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 135.07it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/55 \t Batch 499/500 \t Loss 0.5108 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0074\nTime: train: 3.70 \t Train loss 0.5108 \t Train acc: 0.7671\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 245.87it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4891 \t Running Acc 1.534 \t Total Acc 0.773 \t Avg Batch Time 0.0021\nEpoch 21/55 finished.\nTrain time: 3.70 \t Val time 0.26\nTrain loss 0.5108 \t Train acc: 0.7671\nVal loss: 0.4894 \t Val acc: 0.7730\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 123.14it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/55 \t Batch 499/500 \t Loss 0.5102 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0081\nTime: train: 4.06 \t Train loss 0.5102 \t Train acc: 0.7684\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 247.49it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4710 \t Running Acc 1.558 \t Total Acc 0.785 \t Avg Batch Time 0.0021\nEpoch 22/55 finished.\nTrain time: 4.06 \t Val time 0.26\nTrain loss 0.5102 \t Train acc: 0.7684\nVal loss: 0.4722 \t Val acc: 0.7850\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 128.49it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/55 \t Batch 499/500 \t Loss 0.5096 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0078\nTime: train: 3.89 \t Train loss 0.5096 \t Train acc: 0.7676\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 224.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4899 \t Running Acc 1.536 \t Total Acc 0.774 \t Avg Batch Time 0.0023\nEpoch 23/55 finished.\nTrain time: 3.89 \t Val time 0.28\nTrain loss 0.5096 \t Train acc: 0.7676\nVal loss: 0.4904 \t Val acc: 0.7740\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 117.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/55 \t Batch 499/500 \t Loss 0.5103 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0085\nTime: train: 4.25 \t Train loss 0.5103 \t Train acc: 0.7679\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 233.41it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4885 \t Running Acc 1.554 \t Total Acc 0.783 \t Avg Batch Time 0.0022\nEpoch 24/55 finished.\nTrain time: 4.25 \t Val time 0.27\nTrain loss 0.5103 \t Train acc: 0.7679\nVal loss: 0.4891 \t Val acc: 0.7830\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 125.33it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 26/55 \t Batch 499/500 \t Loss 0.5095 \t Running Acc 0.764 \t Total Acc 0.764 \t Avg Batch Time 0.0080\nTime: train: 3.99 \t Train loss 0.5095 \t Train acc: 0.7642\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 219.56it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4706 \t Running Acc 1.554 \t Total Acc 0.783 \t Avg Batch Time 0.0023\nEpoch 25/55 finished.\nTrain time: 3.99 \t Val time 0.29\nTrain loss 0.5095 \t Train acc: 0.7642\nVal loss: 0.4712 \t Val acc: 0.7830\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 126.44it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 27/55 \t Batch 499/500 \t Loss 0.5000 \t Running Acc 0.771 \t Total Acc 0.771 \t Avg Batch Time 0.0079\nTime: train: 3.96 \t Train loss 0.5000 \t Train acc: 0.7710\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 256.54it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4826 \t Running Acc 1.514 \t Total Acc 0.763 \t Avg Batch Time 0.0020\nEpoch 26/55 finished.\nTrain time: 3.96 \t Val time 0.25\nTrain loss 0.5000 \t Train acc: 0.7710\nVal loss: 0.4837 \t Val acc: 0.7630\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.76it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 28/55 \t Batch 499/500 \t Loss 0.5145 \t Running Acc 0.761 \t Total Acc 0.761 \t Avg Batch Time 0.0077\nTime: train: 3.85 \t Train loss 0.5145 \t Train acc: 0.7606\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 250.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4706 \t Running Acc 1.562 \t Total Acc 0.787 \t Avg Batch Time 0.0020\nEpoch 27/55 finished.\nTrain time: 3.85 \t Val time 0.25\nTrain loss 0.5145 \t Train acc: 0.7606\nVal loss: 0.4712 \t Val acc: 0.7870\nBest val acc: 0.7910 at epoch 15.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.81it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 29/55 \t Batch 499/500 \t Loss 0.5103 \t Running Acc 0.765 \t Total Acc 0.765 \t Avg Batch Time 0.0077\nTime: train: 3.85 \t Train loss 0.5103 \t Train acc: 0.7655\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 230.80it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4641 \t Running Acc 1.571 \t Total Acc 0.792 \t Avg Batch Time 0.0022\nNew best validation model, saving...\nEpoch 28/55 finished.\nTrain time: 3.85 \t Val time 0.28\nTrain loss 0.5103 \t Train acc: 0.7655\nVal loss: 0.4647 \t Val acc: 0.7920\nBest val acc: 0.7920 at epoch 28.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 121.10it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 30/55 \t Batch 499/500 \t Loss 0.5091 \t Running Acc 0.761 \t Total Acc 0.761 \t Avg Batch Time 0.0083\nTime: train: 4.13 \t Train loss 0.5091 \t Train acc: 0.7610\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 246.69it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4642 \t Running Acc 1.569 \t Total Acc 0.791 \t Avg Batch Time 0.0021\nEpoch 29/55 finished.\nTrain time: 4.13 \t Val time 0.26\nTrain loss 0.5091 \t Train acc: 0.7610\nVal loss: 0.4653 \t Val acc: 0.7910\nBest val acc: 0.7920 at epoch 28.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.21it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 31/55 \t Batch 499/500 \t Loss 0.5033 \t Running Acc 0.766 \t Total Acc 0.766 \t Avg Batch Time 0.0076\nTime: train: 3.78 \t Train loss 0.5033 \t Train acc: 0.7660\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 254.41it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4688 \t Running Acc 1.569 \t Total Acc 0.791 \t Avg Batch Time 0.0020\nEpoch 30/55 finished.\nTrain time: 3.78 \t Val time 0.25\nTrain loss 0.5033 \t Train acc: 0.7660\nVal loss: 0.4697 \t Val acc: 0.7910\nBest val acc: 0.7920 at epoch 28.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.02it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 32/55 \t Batch 499/500 \t Loss 0.5048 \t Running Acc 0.770 \t Total Acc 0.770 \t Avg Batch Time 0.0075\nTime: train: 3.76 \t Train loss 0.5048 \t Train acc: 0.7696\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 180.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4621 \t Running Acc 1.583 \t Total Acc 0.798 \t Avg Batch Time 0.0028\nNew best validation model, saving...\nEpoch 31/55 finished.\nTrain time: 3.76 \t Val time 0.35\nTrain loss 0.5048 \t Train acc: 0.7696\nVal loss: 0.4631 \t Val acc: 0.7980\nBest val acc: 0.7980 at epoch 31.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 130.24it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 33/55 \t Batch 499/500 \t Loss 0.5070 \t Running Acc 0.765 \t Total Acc 0.765 \t Avg Batch Time 0.0077\nTime: train: 3.84 \t Train loss 0.5070 \t Train acc: 0.7654\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 250.83it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4631 \t Running Acc 1.579 \t Total Acc 0.796 \t Avg Batch Time 0.0020\nEpoch 32/55 finished.\nTrain time: 3.84 \t Val time 0.25\nTrain loss 0.5070 \t Train acc: 0.7654\nVal loss: 0.4640 \t Val acc: 0.7960\nBest val acc: 0.7980 at epoch 31.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.13it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 34/55 \t Batch 499/500 \t Loss 0.5071 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0075\nTime: train: 3.76 \t Train loss 0.5071 \t Train acc: 0.7681\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 237.24it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4779 \t Running Acc 1.550 \t Total Acc 0.781 \t Avg Batch Time 0.0022\nEpoch 33/55 finished.\nTrain time: 3.76 \t Val time 0.27\nTrain loss 0.5071 \t Train acc: 0.7681\nVal loss: 0.4783 \t Val acc: 0.7810\nBest val acc: 0.7980 at epoch 31.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.65it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 35/55 \t Batch 499/500 \t Loss 0.5063 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0077\nTime: train: 3.86 \t Train loss 0.5063 \t Train acc: 0.7666\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 249.19it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4676 \t Running Acc 1.585 \t Total Acc 0.799 \t Avg Batch Time 0.0020\nNew best validation model, saving...\nEpoch 34/55 finished.\nTrain time: 3.86 \t Val time 0.26\nTrain loss 0.5063 \t Train acc: 0.7666\nVal loss: 0.4685 \t Val acc: 0.7990\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 134.20it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 36/55 \t Batch 499/500 \t Loss 0.5041 \t Running Acc 0.774 \t Total Acc 0.774 \t Avg Batch Time 0.0075\nTime: train: 3.73 \t Train loss 0.5041 \t Train acc: 0.7740\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 265.50it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4691 \t Running Acc 1.575 \t Total Acc 0.794 \t Avg Batch Time 0.0019\nEpoch 35/55 finished.\nTrain time: 3.73 \t Val time 0.24\nTrain loss 0.5041 \t Train acc: 0.7740\nVal loss: 0.4699 \t Val acc: 0.7940\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.32it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 37/55 \t Batch 499/500 \t Loss 0.5084 \t Running Acc 0.771 \t Total Acc 0.771 \t Avg Batch Time 0.0076\nTime: train: 3.78 \t Train loss 0.5084 \t Train acc: 0.7711\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 253.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4658 \t Running Acc 1.583 \t Total Acc 0.798 \t Avg Batch Time 0.0020\nEpoch 36/55 finished.\nTrain time: 3.78 \t Val time 0.25\nTrain loss 0.5084 \t Train acc: 0.7711\nVal loss: 0.4667 \t Val acc: 0.7980\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 134.19it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 38/55 \t Batch 499/500 \t Loss 0.5087 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0075\nTime: train: 3.73 \t Train loss 0.5087 \t Train acc: 0.7666\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 257.24it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4710 \t Running Acc 1.563 \t Total Acc 0.788 \t Avg Batch Time 0.0020\nEpoch 37/55 finished.\nTrain time: 3.73 \t Val time 0.25\nTrain loss 0.5087 \t Train acc: 0.7666\nVal loss: 0.4717 \t Val acc: 0.7880\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 135.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 39/55 \t Batch 499/500 \t Loss 0.5128 \t Running Acc 0.764 \t Total Acc 0.764 \t Avg Batch Time 0.0074\nTime: train: 3.69 \t Train loss 0.5128 \t Train acc: 0.7641\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 256.11it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4674 \t Running Acc 1.565 \t Total Acc 0.789 \t Avg Batch Time 0.0020\nEpoch 38/55 finished.\nTrain time: 3.69 \t Val time 0.25\nTrain loss 0.5128 \t Train acc: 0.7641\nVal loss: 0.4681 \t Val acc: 0.7890\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 130.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 40/55 \t Batch 499/500 \t Loss 0.5089 \t Running Acc 0.766 \t Total Acc 0.766 \t Avg Batch Time 0.0076\nTime: train: 3.82 \t Train loss 0.5089 \t Train acc: 0.7665\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 180.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4690 \t Running Acc 1.573 \t Total Acc 0.793 \t Avg Batch Time 0.0028\nEpoch 39/55 finished.\nTrain time: 3.82 \t Val time 0.35\nTrain loss 0.5089 \t Train acc: 0.7665\nVal loss: 0.4698 \t Val acc: 0.7930\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 128.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 41/55 \t Batch 499/500 \t Loss 0.5060 \t Running Acc 0.770 \t Total Acc 0.770 \t Avg Batch Time 0.0078\nTime: train: 3.88 \t Train loss 0.5060 \t Train acc: 0.7704\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 238.34it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4619 \t Running Acc 1.581 \t Total Acc 0.797 \t Avg Batch Time 0.0021\nEpoch 40/55 finished.\nTrain time: 3.88 \t Val time 0.27\nTrain loss 0.5060 \t Train acc: 0.7704\nVal loss: 0.4629 \t Val acc: 0.7970\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.50it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 42/55 \t Batch 499/500 \t Loss 0.5089 \t Running Acc 0.763 \t Total Acc 0.763 \t Avg Batch Time 0.0077\nTime: train: 3.86 \t Train loss 0.5089 \t Train acc: 0.7635\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 237.85it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4652 \t Running Acc 1.569 \t Total Acc 0.791 \t Avg Batch Time 0.0021\nEpoch 41/55 finished.\nTrain time: 3.86 \t Val time 0.27\nTrain loss 0.5089 \t Train acc: 0.7635\nVal loss: 0.4662 \t Val acc: 0.7910\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.57it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 43/55 \t Batch 499/500 \t Loss 0.5097 \t Running Acc 0.763 \t Total Acc 0.763 \t Avg Batch Time 0.0075\nTime: train: 3.74 \t Train loss 0.5097 \t Train acc: 0.7629\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 258.17it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4637 \t Running Acc 1.573 \t Total Acc 0.793 \t Avg Batch Time 0.0020\nEpoch 42/55 finished.\nTrain time: 3.74 \t Val time 0.25\nTrain loss 0.5097 \t Train acc: 0.7629\nVal loss: 0.4646 \t Val acc: 0.7930\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.61it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 44/55 \t Batch 499/500 \t Loss 0.5076 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0077\nTime: train: 3.86 \t Train loss 0.5076 \t Train acc: 0.7666\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 254.59it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4680 \t Running Acc 1.569 \t Total Acc 0.791 \t Avg Batch Time 0.0020\nEpoch 43/55 finished.\nTrain time: 3.86 \t Val time 0.25\nTrain loss 0.5076 \t Train acc: 0.7666\nVal loss: 0.4686 \t Val acc: 0.7910\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 131.46it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 45/55 \t Batch 499/500 \t Loss 0.5074 \t Running Acc 0.766 \t Total Acc 0.766 \t Avg Batch Time 0.0076\nTime: train: 3.81 \t Train loss 0.5074 \t Train acc: 0.7660\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 259.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4630 \t Running Acc 1.579 \t Total Acc 0.796 \t Avg Batch Time 0.0020\nEpoch 44/55 finished.\nTrain time: 3.81 \t Val time 0.25\nTrain loss 0.5074 \t Train acc: 0.7660\nVal loss: 0.4638 \t Val acc: 0.7960\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 134.29it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 46/55 \t Batch 499/500 \t Loss 0.5037 \t Running Acc 0.770 \t Total Acc 0.770 \t Avg Batch Time 0.0075\nTime: train: 3.73 \t Train loss 0.5037 \t Train acc: 0.7698\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 253.78it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4637 \t Running Acc 1.583 \t Total Acc 0.798 \t Avg Batch Time 0.0020\nEpoch 45/55 finished.\nTrain time: 3.73 \t Val time 0.25\nTrain loss 0.5037 \t Train acc: 0.7698\nVal loss: 0.4645 \t Val acc: 0.7980\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 133.80it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 47/55 \t Batch 499/500 \t Loss 0.5035 \t Running Acc 0.774 \t Total Acc 0.774 \t Avg Batch Time 0.0075\nTime: train: 3.74 \t Train loss 0.5035 \t Train acc: 0.7739\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 235.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4710 \t Running Acc 1.562 \t Total Acc 0.787 \t Avg Batch Time 0.0022\nEpoch 46/55 finished.\nTrain time: 3.74 \t Val time 0.27\nTrain loss 0.5035 \t Train acc: 0.7739\nVal loss: 0.4717 \t Val acc: 0.7870\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.88it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 48/55 \t Batch 499/500 \t Loss 0.5048 \t Running Acc 0.767 \t Total Acc 0.767 \t Avg Batch Time 0.0077\nTime: train: 3.85 \t Train loss 0.5048 \t Train acc: 0.7674\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 237.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4634 \t Running Acc 1.581 \t Total Acc 0.797 \t Avg Batch Time 0.0021\nEpoch 47/55 finished.\nTrain time: 3.85 \t Val time 0.27\nTrain loss 0.5048 \t Train acc: 0.7674\nVal loss: 0.4643 \t Val acc: 0.7970\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 131.77it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 49/55 \t Batch 499/500 \t Loss 0.5048 \t Running Acc 0.770 \t Total Acc 0.770 \t Avg Batch Time 0.0076\nTime: train: 3.80 \t Train loss 0.5048 \t Train acc: 0.7698\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 267.64it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4654 \t Running Acc 1.577 \t Total Acc 0.795 \t Avg Batch Time 0.0019\nEpoch 48/55 finished.\nTrain time: 3.80 \t Val time 0.24\nTrain loss 0.5048 \t Train acc: 0.7698\nVal loss: 0.4663 \t Val acc: 0.7950\nBest val acc: 0.7990 at epoch 34.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 126.21it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 50/55 \t Batch 499/500 \t Loss 0.5039 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0079\nTime: train: 3.96 \t Train loss 0.5039 \t Train acc: 0.7676\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 233.69it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4650 \t Running Acc 1.599 \t Total Acc 0.806 \t Avg Batch Time 0.0022\nNew best validation model, saving...\nEpoch 49/55 finished.\nTrain time: 3.96 \t Val time 0.27\nTrain loss 0.5039 \t Train acc: 0.7676\nVal loss: 0.4660 \t Val acc: 0.8060\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.07it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 51/55 \t Batch 499/500 \t Loss 0.5048 \t Running Acc 0.770 \t Total Acc 0.770 \t Avg Batch Time 0.0076\nTime: train: 3.79 \t Train loss 0.5048 \t Train acc: 0.7704\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 256.98it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4628 \t Running Acc 1.577 \t Total Acc 0.795 \t Avg Batch Time 0.0020\nEpoch 50/55 finished.\nTrain time: 3.79 \t Val time 0.25\nTrain loss 0.5048 \t Train acc: 0.7704\nVal loss: 0.4636 \t Val acc: 0.7950\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 128.53it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 52/55 \t Batch 499/500 \t Loss 0.5008 \t Running Acc 0.773 \t Total Acc 0.773 \t Avg Batch Time 0.0078\nTime: train: 3.89 \t Train loss 0.5008 \t Train acc: 0.7729\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 248.85it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4665 \t Running Acc 1.575 \t Total Acc 0.794 \t Avg Batch Time 0.0020\nEpoch 51/55 finished.\nTrain time: 3.89 \t Val time 0.26\nTrain loss 0.5008 \t Train acc: 0.7729\nVal loss: 0.4674 \t Val acc: 0.7940\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 132.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 53/55 \t Batch 499/500 \t Loss 0.5038 \t Running Acc 0.769 \t Total Acc 0.769 \t Avg Batch Time 0.0076\nTime: train: 3.78 \t Train loss 0.5038 \t Train acc: 0.7686\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 266.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4655 \t Running Acc 1.575 \t Total Acc 0.794 \t Avg Batch Time 0.0019\nEpoch 52/55 finished.\nTrain time: 3.78 \t Val time 0.24\nTrain loss 0.5038 \t Train acc: 0.7686\nVal loss: 0.4661 \t Val acc: 0.7940\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:03, 129.14it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 54/55 \t Batch 499/500 \t Loss 0.5077 \t Running Acc 0.768 \t Total Acc 0.768 \t Avg Batch Time 0.0077\nTime: train: 3.87 \t Train loss 0.5077 \t Train acc: 0.7684\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 235.28it/s]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4717 \t Running Acc 1.558 \t Total Acc 0.785 \t Avg Batch Time 0.0022\nEpoch 53/55 finished.\nTrain time: 3.87 \t Val time 0.27\nTrain loss 0.5077 \t Train acc: 0.7684\nVal loss: 0.4723 \t Val acc: 0.7850\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"500it [00:04, 120.84it/s]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 55/55 \t Batch 499/500 \t Loss 0.5030 \t Running Acc 0.769 \t Total Acc 0.769 \t Avg Batch Time 0.0083\nTime: train: 4.14 \t Train loss 0.5030 \t Train acc: 0.7688\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 233.89it/s]\n/tmp/ipykernel_31/861624552.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.4626 \t Running Acc 1.581 \t Total Acc 0.797 \t Avg Batch Time 0.0022\nEpoch 54/55 finished.\nTrain time: 4.14 \t Val time 0.27\nTrain loss 0.5030 \t Train acc: 0.7688\nVal loss: 0.4633 \t Val acc: 0.7970\nBest val acc: 0.8060 at epoch 49.\n","output_type":"stream"},{"name":"stderr","text":"63it [00:00, 244.95it/s]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.4781 \t Running Acc 1.581 \t Total Acc 0.797 \t Avg Batch Time 0.0021\nFinal  tensor([[1.0000, 0.8694, 0.1306],\n        [0.0000, 0.6214, 0.3786],\n        [1.0000, 0.1845, 0.8155],\n        ...,\n        [1.0000, 0.2325, 0.7675],\n        [0.0000, 0.5240, 0.4760],\n        [1.0000, 0.1203, 0.8797]])\nTest: Loss 0.4771 \t Acc 0.7970 \t AUC: 0.8621 \t 1/eB 0.3: 55.5556 \t 1/eB 0.5: 15.1515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Proposed\n","metadata":{"id":"gZwcNRHBXLqI"}},{"cell_type":"code","source":"import torch\nimport pennylane as qml\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch_geometric.utils import to_dense_adj\n\nn_qubits = 4\n\ndev = qml.device('default.qubit', wires=n_qubits)\n# dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n\n\ndef H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\ndef RY_RX_layer(weights):\n    \"\"\"Applies a layer of parametrized RY and RX rotations.\"\"\"\n    for i, w in enumerate(weights):\n        qml.RY(w, wires=i)\n        qml.RX(w, wires=i)\n\ndef full_entangling_layer(n_qubits):\n    \"\"\"Applies CNOT gates between all pairs of qubits.\"\"\"\n    for i in range(n_qubits):\n        for j in range(i+1, n_qubits):\n            qml.CNOT(wires=[i, j])\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(nqubits - 1):\n        qml.CRZ(np.pi / 2, wires=[i, i + 1])\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.SWAP(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.SWAP(wires=[i, i + 1])\n\n\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    # RY_layer(q_input_features)\n    qml.AngleEmbedding(features=q_input_features, wires=range(n_qubits), rotation='Z')\n\n    # Sequence of trainable variational layers\n    # for k in range(q_depth):\n    #     entangling_layer(n_qubits)\n    #     RY_RX_layer(q_weights[k])\n    #     # RY_layer(q_weights[k])\n    for k in range(q_depth):\n        if k % 2 == 0:\n            entangling_layer(n_qubits)\n            RY_layer(q_weights[k])\n        else:\n            full_entangling_layer(n_qubits)\n            RY_RX_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)\n\n\nclass DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n        print('n_qubits: ', n_qubits)\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.q_depth = q_depth\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n\n    def forward(self, input_features):\n        \"\"\"\n        Optimized forward pass to reduce runtime.\n        \"\"\"\n\n        # Quantum Embedding (U(X))\n        q_in = torch.tanh(input_features) * np.pi / 2.0\n\n        # Preallocate output tensor\n        batch_size = q_in.shape[0]\n        q_out = torch.zeros(batch_size, self.n_qubits, device=q_in.device)\n\n        # Vectorized execution\n        for i, elem in enumerate(q_in):\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float()\n            q_out[i] = q_out_elem\n\n        return q_out","metadata":{"id":"3T8yKHYCZbPk","execution":{"iopub.status.busy":"2024-10-28T21:56:22.957280Z","iopub.execute_input":"2024-10-28T21:56:22.957794Z","iopub.status.idle":"2024-10-28T21:56:22.979294Z","shell.execute_reply.started":"2024-10-28T21:56:22.957738Z","shell.execute_reply":"2024-10-28T21:56:22.977955Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# @title\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pennylane as qml\n\n\"\"\"\n    Lie-Equivariant Quantum Block (LEQB).\n\n        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n          that we develop further), once the metric tensor J is found via the equation:\n\n                          L.J + J.(L^T) = 0,\n\n          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group.\n          In the cells below, we can see how the model preserves symmetries (starting with the default Lorentz group),\n          and when we change J to some other metric (Euclidean, for example), Lorentz boosts **break** equivariance, while other\n          transformations preserve it (rotations, for the example shown in the cells below)\n\"\"\"\nclass LEQB(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n        super(LEQB, self).__init__()\n        self.c_weight = c_weight\n        self.dimension_reducer = nn.Linear(10, 4) # New linear layer for dimension reduction\n        self.dimension_reducer2 = nn.Linear(9, 4) # New linear layer for dimension reduction for phi_h\n        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n        # With include_X = False, not include_x becomes True, so the value of n_edge_attr is 2. n_input = n_hidden = 4\n        print('Input size of phi_e: ', n_input)\n        self.include_x = include_x\n\n        \"\"\"\n            phi_e: input size: n_qubits -> output size: n_qubits\n            n_hidden has to be equal to n_input,\n            but this is just considering that this is a simple working example.\n        \"\"\"\n        self.phi_e = DressedQuantumNet(n_input)\n#         self.phi_e = nn.Sequential(\n#             nn.Linear(n_input, n_hidden, bias=False),  # n_input * 2 + n_edge_attr\n#             nn.BatchNorm1d(n_hidden),\n#             nn.ReLU(),\n#             nn.Linear(n_hidden, n_hidden),\n#             nn.ReLU())\n\n        n_hidden = n_input # n_input * 2 + n_edge_attr\n        self.phi_h = nn.Sequential(\n            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n            nn.BatchNorm1d(n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output))\n\n#         self.phi_h = DressedQuantumNet(n_hidden)\n\n        layer = nn.Linear(n_hidden, 1, bias=False)\n        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n\n        self.phi_x = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            layer)\n\n#         self.phi_x = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             layer)\n\n#         self.phi_m = nn.Sequential(\n#             DressedQuantumNet(n_hidden),\n#             nn.Linear(n_hidden, 1),\n#             nn.Sigmoid())\n\n        self.phi_m = nn.Sequential(\n            nn.Linear(n_hidden, 1),\n            nn.Sigmoid())\n\n        # self.phi_e = nn.Sequential(\n        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n        #     nn.BatchNorm1d(n_hidden),\n        #     nn.ReLU(),\n        #     nn.Linear(n_hidden, n_hidden),\n        #     nn.ReLU())\n\n        self.last_layer = last_layer\n        if last_layer:\n            del self.phi_x\n\n        self.A = A\n        self.norm_fn = normA_fn(A) if A is not None else normsq4\n        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n\n    def m_model(self, hi, hj, norms, dots):\n        out = torch.cat([hi, hj, norms, dots], dim=1)\n        out = self.dimension_reducer(out) # extra\n        # print(\"Before embedding to |psi> : \", out)\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n        out = self.dimension_reducer(out) # extra\n        out = self.phi_e(out).squeeze(0)\n        w = self.phi_m(out)\n        out = out * w\n        return out\n\n    def h_model(self, h, edges, m, node_attr):\n        i, j = edges\n        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n        agg = torch.cat([h, agg, node_attr], dim=1)\n        #agg = self.dimension_reducer2(agg) # extra for phi_h\n        out = h + self.phi_h(agg)\n        return out\n\n    def x_model(self, x, edges, x_diff, m):\n        i, j = edges\n        trans = x_diff * self.phi_x(m)\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        # From https://github.com/vgsatorras/egnn\n        # This is never activated but just in case it explosed it may save the train\n        trans = torch.clamp(trans, min=-100, max=100)\n        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n        x = x + agg * self.c_weight\n        return x\n\n    def minkowski_feats(self, edges, x):\n        i, j = edges\n        x_diff = x[i] - x[j]\n        norms = self.norm_fn(x_diff).unsqueeze(1)\n        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n        norms, dots = psi(norms), psi(dots)\n        return norms, dots, x_diff\n\n    def forward(self, h, x, edges, node_attr=None):\n        i, j = edges\n        norms, dots, x_diff = self.minkowski_feats(edges, x)\n\n        if self.include_x:\n            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n        else:\n            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n        if not self.last_layer:\n            x = self.x_model(x, edges, x_diff, m)\n        h = self.h_model(h, edges, m, node_attr)\n        return h, x, m\n\nclass LieEQGNN(nn.Module):\n    r''' Implementation of LorentzNet.\n\n    Args:\n        - `n_scalar` (int): number of input scalars.\n        - `n_hidden` (int): dimension of latent space.\n        - `n_class`  (int): number of output classes.\n        - `n_layers` (int): number of LEQB layers.\n        - `c_weight` (float): weight c in the x_model.\n        - `dropout`  (float): dropout rate.\n    '''\n    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n        super(LieEQGNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.embedding = nn.Linear(n_scalar, n_hidden)\n        self.LEQBs = nn.ModuleList([LEQB(self.n_hidden, self.n_hidden, self.n_hidden,\n                                    n_node_attr=n_scalar, dropout=dropout,\n                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n                                    for i in range(n_layers)])\n        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n                                       nn.ReLU(),\n                                       nn.Dropout(dropout),\n                                       nn.Linear(self.n_hidden, n_class)) # classification\n\n    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n        h = self.embedding(scalars)\n\n        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n        for i in range(self.n_layers):\n            h, x, _ = self.LEQBs[i](h, x, edges, node_attr=scalars)\n\n        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n\n        h = h * node_mask\n        h = h.view(-1, n_nodes, self.n_hidden)\n        h = torch.mean(h, dim=1)\n        pred = self.graph_dec(h)\n        return pred.squeeze(1)","metadata":{"id":"9sBE05_9XLqJ","execution":{"iopub.status.busy":"2024-10-28T21:56:22.981584Z","iopub.execute_input":"2024-10-28T21:56:22.982029Z","iopub.status.idle":"2024-10-28T21:56:23.013413Z","shell.execute_reply.started":"2024-10-28T21:56:22.981965Z","shell.execute_reply":"2024-10-28T21:56:23.011965Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn, optim\nimport json, time\n# import utils_lorentz\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nif __name__ == \"__main__\":\n\n    N_EPOCHS = 25 # 60\n\n    model_path = \"models/LieEQGNN/\"\n    log_path = \"logs/LieEQGNN/\"\n    # utils_lorentz.args_init(args)\n\n    ### set random seed\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    ### initialize cpu\n    # dist.init_process_group(backend='nccl')\n    device = 'cpu' #torch.device(\"cuda\")\n    dtype = torch.float32\n\n    ### load data\n    # dataloaders = retrieve_dataloaders( batch_size,\n    #                                     num_data=100000, # use all data\n    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n    #                                     num_workers=0,\n    #                                     use_one_hot=True)\n\n    model = LieEQGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n                       dropout = 0.2, n_layers = 1,\\\n                       c_weight = 1e-3)\n\n    model = model.to(device)\n\n    ### print model and dataset information\n    # if (args.local_rank == 0):\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(\"Model Size:\", pytorch_total_params)\n    for (split, dataloader) in dataloaders.items():\n        print(f\" {split} samples: {len(dataloader.dataset)}\")\n\n    ### optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n\n    ### lr scheduler\n    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=1,\\\n                                                warmup_epoch=5,\\\n                                                after_scheduler=base_scheduler) ## warmup\n\n    ### loss function\n    loss_fn = nn.CrossEntropyLoss()\n\n    ### initialize logs\n    res = {'epochs': [], 'lr' : [],\\\n           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n\n    ### training and testing\n    print(\"Training...\")\n    train(model, res, N_EPOCHS, model_path, log_path)\n    test(model, res, model_path, log_path)","metadata":{"id":"sCLi_VJSZiEE","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c9b2dbc-a5ad-4d04-edd1-865274255ee0","execution":{"iopub.status.busy":"2024-10-28T21:56:23.016197Z","iopub.execute_input":"2024-10-28T21:56:23.017488Z","iopub.status.idle":"2024-10-29T07:40:41.834235Z","shell.execute_reply.started":"2024-10-28T21:56:23.017438Z","shell.execute_reply":"2024-10-29T07:40:41.832948Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input size of phi_e:  4\nn_qubits:  4\nModel Size: 199\n train samples: 8000\n val samples: 1000\n test samples: 1000\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"500it [21:43,  2.61s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 1/25 \t Batch 499/500 \t Loss 0.7276 \t Running Acc 0.500 \t Total Acc 0.500 \t Avg Batch Time 2.6068\nTime: train: 1303.39 \t Train loss 0.7276 \t Train acc: 0.5000\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6951 \t Running Acc 0.992 \t Total Acc 0.500 \t Avg Batch Time 0.8158\nNew best validation model, saving...\nEpoch 0/25 finished.\nTrain time: 1303.39 \t Val time 101.97\nTrain loss 0.7276 \t Train acc: 0.5000\nVal loss: 0.6955 \t Val acc: 0.5000\nBest val acc: 0.5000 at epoch 0.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:19,  2.56s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 2/25 \t Batch 499/500 \t Loss 0.6927 \t Running Acc 0.533 \t Total Acc 0.533 \t Avg Batch Time 2.5589\nTime: train: 1279.45 \t Train loss 0.6927 \t Train acc: 0.5331\n","output_type":"stream"},{"name":"stderr","text":"63it [01:48,  1.72s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6792 \t Running Acc 1.272 \t Total Acc 0.641 \t Avg Batch Time 0.8668\nNew best validation model, saving...\nEpoch 1/25 finished.\nTrain time: 1279.45 \t Val time 108.35\nTrain loss 0.6927 \t Train acc: 0.5331\nVal loss: 0.6794 \t Val acc: 0.6410\nBest val acc: 0.6410 at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:39,  2.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 3/25 \t Batch 499/500 \t Loss 0.6785 \t Running Acc 0.567 \t Total Acc 0.567 \t Avg Batch Time 2.5988\nTime: train: 1299.41 \t Train loss 0.6785 \t Train acc: 0.5670\n","output_type":"stream"},{"name":"stderr","text":"63it [01:42,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.7483 \t Running Acc 0.992 \t Total Acc 0.500 \t Avg Batch Time 0.8235\nEpoch 2/25 finished.\nTrain time: 1299.41 \t Val time 102.94\nTrain loss 0.6785 \t Train acc: 0.5670\nVal loss: 0.7500 \t Val acc: 0.5000\nBest val acc: 0.6410 at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:28,  2.58s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 4/25 \t Batch 499/500 \t Loss 0.6439 \t Running Acc 0.618 \t Total Acc 0.618 \t Avg Batch Time 2.5765\nTime: train: 1288.26 \t Train loss 0.6439 \t Train acc: 0.6179\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5918 \t Running Acc 1.395 \t Total Acc 0.703 \t Avg Batch Time 0.8107\nNew best validation model, saving...\nEpoch 3/25 finished.\nTrain time: 1288.26 \t Val time 101.33\nTrain loss 0.6439 \t Train acc: 0.6179\nVal loss: 0.5927 \t Val acc: 0.7030\nBest val acc: 0.7030 at epoch 3.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:49,  2.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 5/25 \t Batch 499/500 \t Loss 0.5849 \t Running Acc 0.681 \t Total Acc 0.681 \t Avg Batch Time 2.6185\nTime: train: 1309.23 \t Train loss 0.5849 \t Train acc: 0.6809\n","output_type":"stream"},{"name":"stderr","text":"63it [01:42,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5663 \t Running Acc 1.435 \t Total Acc 0.723 \t Avg Batch Time 0.8208\nNew best validation model, saving...\nEpoch 4/25 finished.\nTrain time: 1309.23 \t Val time 102.60\nTrain loss 0.5849 \t Train acc: 0.6809\nVal loss: 0.5668 \t Val acc: 0.7230\nBest val acc: 0.7230 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:23,  2.57s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 6/25 \t Batch 499/500 \t Loss 0.5692 \t Running Acc 0.711 \t Total Acc 0.711 \t Avg Batch Time 2.5666\nTime: train: 1283.31 \t Train loss 0.5692 \t Train acc: 0.7114\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5519 \t Running Acc 1.421 \t Total Acc 0.716 \t Avg Batch Time 0.8119\nEpoch 5/25 finished.\nTrain time: 1283.31 \t Val time 101.49\nTrain loss 0.5692 \t Train acc: 0.7114\nVal loss: 0.5525 \t Val acc: 0.7160\nBest val acc: 0.7230 at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:50,  2.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 7/25 \t Batch 499/500 \t Loss 0.5601 \t Running Acc 0.727 \t Total Acc 0.727 \t Avg Batch Time 2.6205\nTime: train: 1310.26 \t Train loss 0.5601 \t Train acc: 0.7266\n","output_type":"stream"},{"name":"stderr","text":"63it [01:46,  1.70s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5522 \t Running Acc 1.444 \t Total Acc 0.728 \t Avg Batch Time 0.8543\nNew best validation model, saving...\nEpoch 6/25 finished.\nTrain time: 1310.26 \t Val time 106.79\nTrain loss 0.5601 \t Train acc: 0.7266\nVal loss: 0.5525 \t Val acc: 0.7280\nBest val acc: 0.7280 at epoch 6.\n","output_type":"stream"},{"name":"stderr","text":"500it [22:01,  2.64s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 8/25 \t Batch 499/500 \t Loss 0.5554 \t Running Acc 0.732 \t Total Acc 0.732 \t Avg Batch Time 2.6434\nTime: train: 1321.70 \t Train loss 0.5554 \t Train acc: 0.7319\n","output_type":"stream"},{"name":"stderr","text":"63it [01:45,  1.67s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5399 \t Running Acc 1.452 \t Total Acc 0.732 \t Avg Batch Time 0.8435\nNew best validation model, saving...\nEpoch 7/25 finished.\nTrain time: 1321.70 \t Val time 105.44\nTrain loss 0.5554 \t Train acc: 0.7319\nVal loss: 0.5404 \t Val acc: 0.7320\nBest val acc: 0.7320 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:40,  2.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 9/25 \t Batch 499/500 \t Loss 0.5607 \t Running Acc 0.732 \t Total Acc 0.732 \t Avg Batch Time 2.6006\nTime: train: 1300.28 \t Train loss 0.5607 \t Train acc: 0.7315\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.9873 \t Running Acc 1.111 \t Total Acc 0.560 \t Avg Batch Time 0.8116\nEpoch 8/25 finished.\nTrain time: 1300.28 \t Val time 101.45\nTrain loss 0.5607 \t Train acc: 0.7315\nVal loss: 0.9849 \t Val acc: 0.5600\nBest val acc: 0.7320 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:27,  2.57s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 10/25 \t Batch 499/500 \t Loss 0.5551 \t Running Acc 0.734 \t Total Acc 0.734 \t Avg Batch Time 2.5745\nTime: train: 1287.25 \t Train loss 0.5551 \t Train acc: 0.7342\n","output_type":"stream"},{"name":"stderr","text":"63it [01:42,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5870 \t Running Acc 1.421 \t Total Acc 0.716 \t Avg Batch Time 0.8175\nEpoch 9/25 finished.\nTrain time: 1287.25 \t Val time 102.19\nTrain loss 0.5551 \t Train acc: 0.7342\nVal loss: 0.5880 \t Val acc: 0.7160\nBest val acc: 0.7320 at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:08,  2.54s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 11/25 \t Batch 499/500 \t Loss 0.5573 \t Running Acc 0.738 \t Total Acc 0.738 \t Avg Batch Time 2.5362\nTime: train: 1268.08 \t Train loss 0.5573 \t Train acc: 0.7376\n","output_type":"stream"},{"name":"stderr","text":"63it [01:43,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5384 \t Running Acc 1.472 \t Total Acc 0.742 \t Avg Batch Time 0.8250\nNew best validation model, saving...\nEpoch 10/25 finished.\nTrain time: 1268.08 \t Val time 103.13\nTrain loss 0.5573 \t Train acc: 0.7376\nVal loss: 0.5390 \t Val acc: 0.7420\nBest val acc: 0.7420 at epoch 10.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:39,  2.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 12/25 \t Batch 499/500 \t Loss 0.5438 \t Running Acc 0.742 \t Total Acc 0.742 \t Avg Batch Time 2.5989\nTime: train: 1299.47 \t Train loss 0.5438 \t Train acc: 0.7419\n","output_type":"stream"},{"name":"stderr","text":"63it [01:49,  1.74s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5453 \t Running Acc 1.437 \t Total Acc 0.724 \t Avg Batch Time 0.8794\nEpoch 11/25 finished.\nTrain time: 1299.47 \t Val time 109.92\nTrain loss 0.5438 \t Train acc: 0.7419\nVal loss: 0.5453 \t Val acc: 0.7240\nBest val acc: 0.7420 at epoch 10.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:22,  2.57s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 13/25 \t Batch 499/500 \t Loss 0.5466 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 2.5656\nTime: train: 1282.80 \t Train loss 0.5466 \t Train acc: 0.7450\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5245 \t Running Acc 1.482 \t Total Acc 0.747 \t Avg Batch Time 0.8112\nNew best validation model, saving...\nEpoch 12/25 finished.\nTrain time: 1282.80 \t Val time 101.40\nTrain loss 0.5466 \t Train acc: 0.7450\nVal loss: 0.5246 \t Val acc: 0.7470\nBest val acc: 0.7470 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:52,  2.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 14/25 \t Batch 499/500 \t Loss 0.5431 \t Running Acc 0.747 \t Total Acc 0.747 \t Avg Batch Time 2.6247\nTime: train: 1312.33 \t Train loss 0.5431 \t Train acc: 0.7472\n","output_type":"stream"},{"name":"stderr","text":"63it [01:44,  1.66s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5312 \t Running Acc 1.460 \t Total Acc 0.736 \t Avg Batch Time 0.8343\nEpoch 13/25 finished.\nTrain time: 1312.33 \t Val time 104.29\nTrain loss 0.5431 \t Train acc: 0.7472\nVal loss: 0.5314 \t Val acc: 0.7360\nBest val acc: 0.7470 at epoch 12.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:50,  2.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 15/25 \t Batch 499/500 \t Loss 0.5455 \t Running Acc 0.744 \t Total Acc 0.744 \t Avg Batch Time 2.6207\nTime: train: 1310.37 \t Train loss 0.5455 \t Train acc: 0.7435\n","output_type":"stream"},{"name":"stderr","text":"63it [01:47,  1.71s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5261 \t Running Acc 1.484 \t Total Acc 0.748 \t Avg Batch Time 0.8603\nNew best validation model, saving...\nEpoch 14/25 finished.\nTrain time: 1310.37 \t Val time 107.53\nTrain loss 0.5455 \t Train acc: 0.7435\nVal loss: 0.5263 \t Val acc: 0.7480\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:30,  2.58s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 16/25 \t Batch 499/500 \t Loss 0.5440 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 2.5800\nTime: train: 1290.02 \t Train loss 0.5440 \t Train acc: 0.7448\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5253 \t Running Acc 1.464 \t Total Acc 0.738 \t Avg Batch Time 0.8082\nEpoch 15/25 finished.\nTrain time: 1290.02 \t Val time 101.03\nTrain loss 0.5440 \t Train acc: 0.7448\nVal loss: 0.5255 \t Val acc: 0.7380\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [22:11,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 17/25 \t Batch 499/500 \t Loss 0.5543 \t Running Acc 0.737 \t Total Acc 0.737 \t Avg Batch Time 2.6622\nTime: train: 1331.08 \t Train loss 0.5543 \t Train acc: 0.7365\n","output_type":"stream"},{"name":"stderr","text":"63it [01:52,  1.79s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5311 \t Running Acc 1.474 \t Total Acc 0.743 \t Avg Batch Time 0.9013\nEpoch 16/25 finished.\nTrain time: 1331.08 \t Val time 112.66\nTrain loss 0.5543 \t Train acc: 0.7365\nVal loss: 0.5313 \t Val acc: 0.7430\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:38,  2.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 18/25 \t Batch 499/500 \t Loss 0.5497 \t Running Acc 0.744 \t Total Acc 0.744 \t Avg Batch Time 2.5979\nTime: train: 1298.93 \t Train loss 0.5497 \t Train acc: 0.7442\n","output_type":"stream"},{"name":"stderr","text":"63it [01:47,  1.71s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.6295 \t Running Acc 1.319 \t Total Acc 0.665 \t Avg Batch Time 0.8610\nEpoch 17/25 finished.\nTrain time: 1298.93 \t Val time 107.63\nTrain loss 0.5497 \t Train acc: 0.7442\nVal loss: 0.6293 \t Val acc: 0.6650\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:36,  2.59s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 19/25 \t Batch 499/500 \t Loss 0.5465 \t Running Acc 0.741 \t Total Acc 0.741 \t Avg Batch Time 2.5921\nTime: train: 1296.05 \t Train loss 0.5465 \t Train acc: 0.7406\n","output_type":"stream"},{"name":"stderr","text":"63it [01:44,  1.66s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5395 \t Running Acc 1.452 \t Total Acc 0.732 \t Avg Batch Time 0.8367\nEpoch 18/25 finished.\nTrain time: 1296.05 \t Val time 104.59\nTrain loss 0.5465 \t Train acc: 0.7406\nVal loss: 0.5402 \t Val acc: 0.7320\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:29,  2.58s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 20/25 \t Batch 499/500 \t Loss 0.5463 \t Running Acc 0.739 \t Total Acc 0.739 \t Avg Batch Time 2.5782\nTime: train: 1289.08 \t Train loss 0.5463 \t Train acc: 0.7390\n","output_type":"stream"},{"name":"stderr","text":"63it [01:40,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5492 \t Running Acc 1.450 \t Total Acc 0.731 \t Avg Batch Time 0.8018\nEpoch 19/25 finished.\nTrain time: 1289.08 \t Val time 100.22\nTrain loss 0.5463 \t Train acc: 0.7390\nVal loss: 0.5499 \t Val acc: 0.7310\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:20,  2.56s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 21/25 \t Batch 499/500 \t Loss 0.5468 \t Running Acc 0.741 \t Total Acc 0.741 \t Avg Batch Time 2.5614\nTime: train: 1280.71 \t Train loss 0.5468 \t Train acc: 0.7409\n","output_type":"stream"},{"name":"stderr","text":"63it [01:40,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5423 \t Running Acc 1.458 \t Total Acc 0.735 \t Avg Batch Time 0.8035\nEpoch 20/25 finished.\nTrain time: 1280.71 \t Val time 100.44\nTrain loss 0.5468 \t Train acc: 0.7409\nVal loss: 0.5430 \t Val acc: 0.7350\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:23,  2.57s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 22/25 \t Batch 499/500 \t Loss 0.5394 \t Running Acc 0.744 \t Total Acc 0.744 \t Avg Batch Time 2.5663\nTime: train: 1283.13 \t Train loss 0.5394 \t Train acc: 0.7440\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5236 \t Running Acc 1.476 \t Total Acc 0.744 \t Avg Batch Time 0.8152\nEpoch 21/25 finished.\nTrain time: 1283.13 \t Val time 101.90\nTrain loss 0.5394 \t Train acc: 0.7440\nVal loss: 0.5240 \t Val acc: 0.7440\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:35,  2.59s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 23/25 \t Batch 499/500 \t Loss 0.5442 \t Running Acc 0.746 \t Total Acc 0.746 \t Avg Batch Time 2.5918\nTime: train: 1295.89 \t Train loss 0.5442 \t Train acc: 0.7462\n","output_type":"stream"},{"name":"stderr","text":"63it [01:40,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5212 \t Running Acc 1.470 \t Total Acc 0.741 \t Avg Batch Time 0.8045\nEpoch 22/25 finished.\nTrain time: 1295.89 \t Val time 100.57\nTrain loss 0.5442 \t Train acc: 0.7462\nVal loss: 0.5214 \t Val acc: 0.7410\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:04,  2.53s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 24/25 \t Batch 499/500 \t Loss 0.5432 \t Running Acc 0.745 \t Total Acc 0.745 \t Avg Batch Time 2.5290\nTime: train: 1264.52 \t Train loss 0.5432 \t Train acc: 0.7451\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5313 \t Running Acc 1.460 \t Total Acc 0.736 \t Avg Batch Time 0.8140\nEpoch 23/25 finished.\nTrain time: 1264.52 \t Val time 101.75\nTrain loss 0.5432 \t Train acc: 0.7451\nVal loss: 0.5312 \t Val acc: 0.7360\nBest val acc: 0.7480 at epoch 14.\n","output_type":"stream"},{"name":"stderr","text":"500it [21:18,  2.56s/it]\n","output_type":"stream"},{"name":"stdout","text":">> train \t Epoch 25/25 \t Batch 499/500 \t Loss 0.5415 \t Running Acc 0.750 \t Total Acc 0.750 \t Avg Batch Time 2.5566\nTime: train: 1278.31 \t Train loss 0.5415 \t Train acc: 0.7496\n","output_type":"stream"},{"name":"stderr","text":"63it [01:40,  1.59s/it]\n/tmp/ipykernel_31/861624552.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":">> val \t Loss 0.5145 \t Running Acc 1.500 \t Total Acc 0.756 \t Avg Batch Time 0.8023\nNew best validation model, saving...\nEpoch 24/25 finished.\nTrain time: 1278.31 \t Val time 100.29\nTrain loss 0.5415 \t Train acc: 0.7496\nVal loss: 0.5147 \t Val acc: 0.7560\nBest val acc: 0.7560 at epoch 24.\n","output_type":"stream"},{"name":"stderr","text":"63it [01:41,  1.60s/it]","output_type":"stream"},{"name":"stdout","text":">> test \t Loss 0.5116 \t Running Acc 1.484 \t Total Acc 0.748 \t Avg Batch Time 0.8087\nFinal  tensor([[1.0000, 0.8497, 0.1503],\n        [0.0000, 0.5746, 0.4254],\n        [1.0000, 0.2029, 0.7971],\n        ...,\n        [1.0000, 0.3306, 0.6694],\n        [0.0000, 0.2611, 0.7389],\n        [1.0000, 0.1844, 0.8156]])\nTest: Loss 0.5108 \t Acc 0.7480 \t AUC: 0.8320 \t 1/eB 0.3: 33.3333 \t 1/eB 0.5: 12.5000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"LGWyv5SIYVJQ"},"execution_count":null,"outputs":[]}]}